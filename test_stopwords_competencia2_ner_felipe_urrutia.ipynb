{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "test_stopwords_competencia2_ner_felipe_urrutia.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": true,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": true
    },
    "varInspector": {
      "cols": {
        "lenName": 16,
        "lenType": 16,
        "lenVar": 40
      },
      "kernels_config": {
        "python": {
          "delete_cmd_postfix": "",
          "delete_cmd_prefix": "del ",
          "library": "var_list.py",
          "varRefreshCmd": "print(var_dic_list())"
        },
        "r": {
          "delete_cmd_postfix": ") ",
          "delete_cmd_prefix": "rm(",
          "library": "var_list.r",
          "varRefreshCmd": "cat(var_dic_list()) "
        }
      },
      "types_to_exclude": [
        "module",
        "function",
        "builtin_function_or_method",
        "instance",
        "_Feature"
      ],
      "window_display": false
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/furrutiav/NLP-competition-2/blob/main/test_stopwords_competencia2_ner_felipe_urrutia.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "27csY87GaSFO",
        "scrolled": false,
        "outputId": "68ad0532-f794-4db5-8ce0-50753835a5dd"
      },
      "source": [
        "# Instalamos torchtext que nos facilitará la vida en el pre-procesamiento del formato ConLL.\n",
        "!pip install -U torchtext==0.10.0"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33mWARNING: Ignoring invalid distribution -orch (/usr/local/lib/python3.7/dist-packages)\u001b[0m\n",
            "\u001b[33mWARNING: Ignoring invalid distribution -orch (/usr/local/lib/python3.7/dist-packages)\u001b[0m\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting torchtext==0.10.0\n",
            "  Using cached torchtext-0.10.0-cp37-cp37m-manylinux1_x86_64.whl (7.6 MB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchtext==0.10.0) (1.21.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from torchtext==0.10.0) (4.64.0)\n",
            "Collecting torch==1.9.0\n",
            "  Using cached torch-1.9.0-cp37-cp37m-manylinux1_x86_64.whl (831.4 MB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torchtext==0.10.0) (2.23.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.9.0->torchtext==0.10.0) (4.1.1)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.10.0) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.10.0) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.10.0) (2022.6.15)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.10.0) (2.10)\n",
            "\u001b[33mWARNING: Ignoring invalid distribution -orch (/usr/local/lib/python3.7/dist-packages)\u001b[0m\n",
            "Installing collected packages: torch, torchtext\n",
            "  Attempting uninstall: torchtext\n",
            "\u001b[33m    WARNING: Ignoring invalid distribution -orch (/usr/local/lib/python3.7/dist-packages)\u001b[0m\n",
            "    Found existing installation: torchtext 0.12.0\n",
            "    Uninstalling torchtext-0.12.0:\n",
            "      Successfully uninstalled torchtext-0.12.0\n",
            "\u001b[33mWARNING: Ignoring invalid distribution -orch (/usr/local/lib/python3.7/dist-packages)\u001b[0m\n",
            "\u001b[33mWARNING: Ignoring invalid distribution -orch (/usr/local/lib/python3.7/dist-packages)\u001b[0m\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchvision 0.12.0+cu113 requires torch==1.11.0, but you have torch 1.9.0 which is incompatible.\n",
            "torchaudio 0.11.0+cu113 requires torch==1.11.0, but you have torch 1.9.0 which is incompatible.\u001b[0m\n",
            "Successfully installed torch-1.9.0 torchtext-0.10.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ng7wRGEyawjM"
      },
      "source": [
        "import torch\n",
        "from torchtext import data, datasets, legacy\n",
        "\n",
        "\n",
        "# Garantizar reproducibilidad de los experimentos\n",
        "SEED = 1234\n",
        "torch.manual_seed(SEED)\n",
        "torch.backends.cudnn.deterministic = True"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BehSou6rCvwg"
      },
      "source": [
        "Descargamos los datos de entrenamiento, validación y prueba en nuestro directorio de trabajo"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lbT0g_kC18Jb",
        "outputId": "d9d238bb-8be7-40c7-a915-c1ddf3e28b61"
      },
      "source": [
        "#%%capture\n",
        "\n",
        "!wget https://github.com/dccuchile/CC6205/releases/download/v1.0/train.txt -nc # Dataset de Entrenamiento\n",
        "!wget https://github.com/dccuchile/CC6205/releases/download/v1.0/dev.txt -nc    # Dataset de Validación (Para probar y ajustar el modelo)\n",
        "!wget https://github.com/dccuchile/CC6205/releases/download/v1.0/test.txt -nc  # Dataset de la Competencia. Estos datos solo contienen los tokens. ¡¡SON LOS QUE DEBEN SER PREDICHOS!!"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-06-28 16:54:36--  https://github.com/dccuchile/CC6205/releases/download/v1.0/train.txt\n",
            "Resolving github.com (github.com)... 192.30.255.113\n",
            "Connecting to github.com (github.com)|192.30.255.113|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://objects.githubusercontent.com/github-production-release-asset-2e65be/196273020/77198f00-c145-11eb-83d1-11e647241ab6?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20220628%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20220628T165437Z&X-Amz-Expires=300&X-Amz-Signature=e071b9b0a41866d87f98fbdfa63882e2d3ccfe1dc954569a69d0cfeb788c16db&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=196273020&response-content-disposition=attachment%3B%20filename%3Dtrain.txt&response-content-type=application%2Foctet-stream [following]\n",
            "--2022-06-28 16:54:37--  https://objects.githubusercontent.com/github-production-release-asset-2e65be/196273020/77198f00-c145-11eb-83d1-11e647241ab6?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20220628%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20220628T165437Z&X-Amz-Expires=300&X-Amz-Signature=e071b9b0a41866d87f98fbdfa63882e2d3ccfe1dc954569a69d0cfeb788c16db&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=196273020&response-content-disposition=attachment%3B%20filename%3Dtrain.txt&response-content-type=application%2Foctet-stream\n",
            "Resolving objects.githubusercontent.com (objects.githubusercontent.com)... 185.199.111.133, 185.199.110.133, 185.199.108.133, ...\n",
            "Connecting to objects.githubusercontent.com (objects.githubusercontent.com)|185.199.111.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1607913 (1.5M) [application/octet-stream]\n",
            "Saving to: ‘train.txt’\n",
            "\n",
            "train.txt           100%[===================>]   1.53M  --.-KB/s    in 0.07s   \n",
            "\n",
            "2022-06-28 16:54:37 (21.6 MB/s) - ‘train.txt’ saved [1607913/1607913]\n",
            "\n",
            "--2022-06-28 16:54:37--  https://github.com/dccuchile/CC6205/releases/download/v1.0/dev.txt\n",
            "Resolving github.com (github.com)... 192.30.255.112\n",
            "Connecting to github.com (github.com)|192.30.255.112|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://objects.githubusercontent.com/github-production-release-asset-2e65be/196273020/741e9e80-c145-11eb-813a-b9abac0d674c?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20220628%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20220628T165437Z&X-Amz-Expires=300&X-Amz-Signature=d68a83ed2a4fccdfc2596346af86dd3db68354069d03e3695ff6986e2dcd62df&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=196273020&response-content-disposition=attachment%3B%20filename%3Ddev.txt&response-content-type=application%2Foctet-stream [following]\n",
            "--2022-06-28 16:54:37--  https://objects.githubusercontent.com/github-production-release-asset-2e65be/196273020/741e9e80-c145-11eb-813a-b9abac0d674c?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20220628%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20220628T165437Z&X-Amz-Expires=300&X-Amz-Signature=d68a83ed2a4fccdfc2596346af86dd3db68354069d03e3695ff6986e2dcd62df&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=196273020&response-content-disposition=attachment%3B%20filename%3Ddev.txt&response-content-type=application%2Foctet-stream\n",
            "Resolving objects.githubusercontent.com (objects.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to objects.githubusercontent.com (objects.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 177166 (173K) [application/octet-stream]\n",
            "Saving to: ‘dev.txt’\n",
            "\n",
            "dev.txt             100%[===================>] 173.01K  --.-KB/s    in 0.02s   \n",
            "\n",
            "2022-06-28 16:54:37 (9.62 MB/s) - ‘dev.txt’ saved [177166/177166]\n",
            "\n",
            "--2022-06-28 16:54:37--  https://github.com/dccuchile/CC6205/releases/download/v1.0/test.txt\n",
            "Resolving github.com (github.com)... 192.30.255.113\n",
            "Connecting to github.com (github.com)|192.30.255.113|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://objects.githubusercontent.com/github-production-release-asset-2e65be/196273020/75e86200-c145-11eb-94f8-49517311d768?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20220628%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20220628T165438Z&X-Amz-Expires=300&X-Amz-Signature=ecfacd1e3e9e0e554a27ac3ada1f97312491dccedb0ef28ade1fe39af04c00ae&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=196273020&response-content-disposition=attachment%3B%20filename%3Dtest.txt&response-content-type=application%2Foctet-stream [following]\n",
            "--2022-06-28 16:54:38--  https://objects.githubusercontent.com/github-production-release-asset-2e65be/196273020/75e86200-c145-11eb-94f8-49517311d768?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20220628%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20220628T165438Z&X-Amz-Expires=300&X-Amz-Signature=ecfacd1e3e9e0e554a27ac3ada1f97312491dccedb0ef28ade1fe39af04c00ae&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=196273020&response-content-disposition=attachment%3B%20filename%3Dtest.txt&response-content-type=application%2Foctet-stream\n",
            "Resolving objects.githubusercontent.com (objects.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to objects.githubusercontent.com (objects.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 147052 (144K) [application/octet-stream]\n",
            "Saving to: ‘test.txt’\n",
            "\n",
            "test.txt            100%[===================>] 143.61K  --.-KB/s    in 0.02s   \n",
            "\n",
            "2022-06-28 16:54:38 (8.97 MB/s) - ‘test.txt’ saved [147052/147052]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torchtext import data, datasets, legacy\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "import string\n",
        "import time"
      ],
      "metadata": {
        "id": "8GwTj5eWp1Sm"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U spacy\n",
        "import spacy\n",
        "!python -m spacy download es_core_news_md\n",
        "nlp = spacy.load('es_core_news_md')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IycMWwu-Hhek",
        "outputId": "c0921018-b6f4-492c-8dce-a498aedb731c"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33mWARNING: Ignoring invalid distribution -orch (/usr/local/lib/python3.7/dist-packages)\u001b[0m\n",
            "\u001b[33mWARNING: Ignoring invalid distribution -orch (/usr/local/lib/python3.7/dist-packages)\u001b[0m\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.7/dist-packages (3.3.1)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.4.3)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.23.0)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.0.2)\n",
            "Requirement already satisfied: thinc<8.1.0,>=8.0.14 in /usr/local/lib/python3.7/dist-packages (from spacy) (8.0.17)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.0.7)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (4.64.0)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.21.6)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.9 in /usr/local/lib/python3.7/dist-packages (from spacy) (3.0.9)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.7.7)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (21.3)\n",
            "Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.6.1)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.0.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy) (57.4.0)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (3.0.6)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (3.3.0)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.0.7)\n",
            "Requirement already satisfied: typer<0.5.0,>=0.3.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.11.3)\n",
            "Requirement already satisfied: typing-extensions<4.2.0,>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy) (4.1.1)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.9.1)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.8.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from catalogue<2.1.0,>=2.0.6->spacy) (3.8.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->spacy) (3.0.9)\n",
            "Requirement already satisfied: smart-open<6.0.0,>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from pathy>=0.3.5->spacy) (5.2.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2022.6.15)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (1.24.3)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.7/dist-packages (from typer<0.5.0,>=0.3.0->spacy) (7.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->spacy) (2.0.1)\n",
            "\u001b[33mWARNING: Ignoring invalid distribution -orch (/usr/local/lib/python3.7/dist-packages)\u001b[0m\n",
            "\u001b[33mWARNING: Ignoring invalid distribution -orch (/usr/local/lib/python3.7/dist-packages)\u001b[0m\n",
            "\u001b[33mWARNING: Ignoring invalid distribution -orch (/usr/local/lib/python3.7/dist-packages)\u001b[0m\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting es-core-news-md==3.3.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/es_core_news_md-3.3.0/es_core_news_md-3.3.0-py3-none-any.whl (42.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 42.3 MB 1.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: spacy<3.4.0,>=3.3.0.dev0 in /usr/local/lib/python3.7/dist-packages (from es-core-news-md==3.3.0) (3.3.1)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->es-core-news-md==3.3.0) (3.0.6)\n",
            "Requirement already satisfied: typer<0.5.0,>=0.3.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->es-core-news-md==3.3.0) (0.4.1)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->es-core-news-md==3.3.0) (1.8.2)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->es-core-news-md==3.3.0) (2.4.3)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->es-core-news-md==3.3.0) (1.21.6)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->es-core-news-md==3.3.0) (2.11.3)\n",
            "Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->es-core-news-md==3.3.0) (0.6.1)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->es-core-news-md==3.3.0) (4.64.0)\n",
            "Requirement already satisfied: thinc<8.1.0,>=8.0.14 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->es-core-news-md==3.3.0) (8.0.17)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->es-core-news-md==3.3.0) (2.23.0)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->es-core-news-md==3.3.0) (2.0.7)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->es-core-news-md==3.3.0) (21.3)\n",
            "Requirement already satisfied: typing-extensions<4.2.0,>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->es-core-news-md==3.3.0) (4.1.1)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.9 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->es-core-news-md==3.3.0) (3.0.9)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->es-core-news-md==3.3.0) (1.0.7)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->es-core-news-md==3.3.0) (0.7.7)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->es-core-news-md==3.3.0) (0.9.1)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->es-core-news-md==3.3.0) (2.0.6)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->es-core-news-md==3.3.0) (3.3.0)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->es-core-news-md==3.3.0) (1.0.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->es-core-news-md==3.3.0) (57.4.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from catalogue<2.1.0,>=2.0.6->spacy<3.4.0,>=3.3.0.dev0->es-core-news-md==3.3.0) (3.8.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->spacy<3.4.0,>=3.3.0.dev0->es-core-news-md==3.3.0) (3.0.9)\n",
            "Requirement already satisfied: smart-open<6.0.0,>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from pathy>=0.3.5->spacy<3.4.0,>=3.3.0.dev0->es-core-news-md==3.3.0) (5.2.1)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.4.0,>=3.3.0.dev0->es-core-news-md==3.3.0) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.4.0,>=3.3.0.dev0->es-core-news-md==3.3.0) (2022.6.15)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.4.0,>=3.3.0.dev0->es-core-news-md==3.3.0) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.4.0,>=3.3.0.dev0->es-core-news-md==3.3.0) (2.10)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.7/dist-packages (from typer<0.5.0,>=0.3.0->spacy<3.4.0,>=3.3.0.dev0->es-core-news-md==3.3.0) (7.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->spacy<3.4.0,>=3.3.0.dev0->es-core-news-md==3.3.0) (2.0.1)\n",
            "\u001b[33mWARNING: Ignoring invalid distribution -orch (/usr/local/lib/python3.7/dist-packages)\u001b[0m\n",
            "Installing collected packages: es-core-news-md\n",
            "\u001b[33mWARNING: Ignoring invalid distribution -orch (/usr/local/lib/python3.7/dist-packages)\u001b[0m\n",
            "Successfully installed es-core-news-md-3.3.0\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('es_core_news_md')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open('train.txt') as f:\n",
        "    lines_train = f.readlines()\n",
        "\n",
        "with open('dev.txt') as f:\n",
        "    lines_dev = f.readlines()"
      ],
      "metadata": {
        "id": "6O9kt2WHp2T4"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocessing(lines, method=\"masked\", verbose=True):\n",
        "  _times = []\n",
        "  lines_stopwords = []\n",
        "  N = len(lines)\n",
        "  for i, line in enumerate(lines):\n",
        "    start = time.time()\n",
        "    new_line = line\n",
        "    stop = False\n",
        "    if len(line.split())==2:\n",
        "      w, tag = line.split()\n",
        "      doc = nlp(w)\n",
        "      if doc[0].is_stop:\n",
        "        stop = True\n",
        "        w = \"<stopword>\"\n",
        "        new_line = f\"{w} {tag}\\n\"\n",
        "    if method == \"masked\": lines_stopwords.append(new_line)\n",
        "    else:\n",
        "      if not stop: lines_stopwords.append(new_line)\n",
        "    \n",
        "    _times.append(time.time()-start)\n",
        "    if ((i+1)%1000==0 or (i+1)==len(lines)) and verbose:\n",
        "      end_expected = (N-(i+1))*np.mean(_times)\n",
        "      end_expected_hrs = end_expected//(60*60)\n",
        "      end_expected_min = (end_expected-end_expected_hrs*(60*60))//60\n",
        "      end_expected_sec = end_expected - end_expected_hrs*(60*60) - end_expected_min*60\n",
        "          \n",
        "      trans_expected = np.sum(_times)\n",
        "      trans_expected_hrs = trans_expected//(60*60)\n",
        "      trans_expected_min = (trans_expected-trans_expected_hrs*(60*60))//60\n",
        "      trans_expected_sec = trans_expected - trans_expected_hrs*(60*60) - trans_expected_min*60\n",
        "\n",
        "      print(f\"\"\"{(i+1)}/{N}, progress: {100*(i+1)/N: .1f} %, dt: {_times[-1]: .2f}, exp. dt: {np.mean(_times): .2f} p/m {np.std(_times): .2f} s, t. trans: {trans_expected_hrs: .1f} hrs {trans_expected_min: .1f} min {trans_expected_sec: .1f} s, exp. t. end: {end_expected_hrs: .1f} hrs {end_expected_min: .1f} min {end_expected_sec: .1f} s\"\"\")\n",
        "\n",
        "  return lines_stopwords"
      ],
      "metadata": {
        "id": "DG6Qlq91p5tt"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "lines_stopwords_masked = preprocessing(lines_train, method=\"masked\", verbose=True)\n",
        "lines_stopwords_masked      "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HxOvWNfRI_Yd",
        "outputId": "ad2280be-0fa2-4ea7-f86a-4185f40cea70"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1000/157246, progress:  0.6 %, dt:  0.00, exp. dt:  0.00 p/m  0.00 s, t. trans:  0.0 hrs  0.0 min  4.7 s, exp. t. end:  0.0 hrs  12.0 min  13.1 s\n",
            "2000/157246, progress:  1.3 %, dt:  0.01, exp. dt:  0.01 p/m  0.00 s, t. trans:  0.0 hrs  0.0 min  12.5 s, exp. t. end:  0.0 hrs  16.0 min  13.6 s\n",
            "3000/157246, progress:  1.9 %, dt:  0.00, exp. dt:  0.01 p/m  0.00 s, t. trans:  0.0 hrs  0.0 min  16.9 s, exp. t. end:  0.0 hrs  14.0 min  29.2 s\n",
            "4000/157246, progress:  2.5 %, dt:  0.00, exp. dt:  0.01 p/m  0.00 s, t. trans:  0.0 hrs  0.0 min  21.2 s, exp. t. end:  0.0 hrs  13.0 min  32.9 s\n",
            "5000/157246, progress:  3.2 %, dt:  0.00, exp. dt:  0.01 p/m  0.00 s, t. trans:  0.0 hrs  0.0 min  25.5 s, exp. t. end:  0.0 hrs  12.0 min  56.3 s\n",
            "6000/157246, progress:  3.8 %, dt:  0.00, exp. dt:  0.00 p/m  0.00 s, t. trans:  0.0 hrs  0.0 min  29.7 s, exp. t. end:  0.0 hrs  12.0 min  29.7 s\n",
            "7000/157246, progress:  4.5 %, dt:  0.00, exp. dt:  0.00 p/m  0.00 s, t. trans:  0.0 hrs  0.0 min  34.0 s, exp. t. end:  0.0 hrs  12.0 min  9.6 s\n",
            "8000/157246, progress:  5.1 %, dt:  0.00, exp. dt:  0.00 p/m  0.00 s, t. trans:  0.0 hrs  0.0 min  38.2 s, exp. t. end:  0.0 hrs  11.0 min  52.7 s\n",
            "9000/157246, progress:  5.7 %, dt:  0.00, exp. dt:  0.00 p/m  0.00 s, t. trans:  0.0 hrs  0.0 min  42.4 s, exp. t. end:  0.0 hrs  11.0 min  39.2 s\n",
            "10000/157246, progress:  6.4 %, dt:  0.00, exp. dt:  0.00 p/m  0.00 s, t. trans:  0.0 hrs  0.0 min  46.7 s, exp. t. end:  0.0 hrs  11.0 min  27.3 s\n",
            "11000/157246, progress:  7.0 %, dt:  0.00, exp. dt:  0.00 p/m  0.00 s, t. trans:  0.0 hrs  0.0 min  50.8 s, exp. t. end:  0.0 hrs  11.0 min  15.6 s\n",
            "12000/157246, progress:  7.6 %, dt:  0.00, exp. dt:  0.00 p/m  0.00 s, t. trans:  0.0 hrs  0.0 min  55.0 s, exp. t. end:  0.0 hrs  11.0 min  5.3 s\n",
            "13000/157246, progress:  8.3 %, dt:  0.00, exp. dt:  0.00 p/m  0.00 s, t. trans:  0.0 hrs  0.0 min  59.2 s, exp. t. end:  0.0 hrs  10.0 min  57.3 s\n",
            "14000/157246, progress:  8.9 %, dt:  0.00, exp. dt:  0.00 p/m  0.00 s, t. trans:  0.0 hrs  1.0 min  3.5 s, exp. t. end:  0.0 hrs  10.0 min  49.6 s\n",
            "15000/157246, progress:  9.5 %, dt:  0.00, exp. dt:  0.00 p/m  0.00 s, t. trans:  0.0 hrs  1.0 min  7.7 s, exp. t. end:  0.0 hrs  10.0 min  42.4 s\n",
            "16000/157246, progress:  10.2 %, dt:  0.00, exp. dt:  0.00 p/m  0.00 s, t. trans:  0.0 hrs  1.0 min  12.1 s, exp. t. end:  0.0 hrs  10.0 min  36.1 s\n",
            "17000/157246, progress:  10.8 %, dt:  0.00, exp. dt:  0.00 p/m  0.00 s, t. trans:  0.0 hrs  1.0 min  16.2 s, exp. t. end:  0.0 hrs  10.0 min  29.0 s\n",
            "18000/157246, progress:  11.4 %, dt:  0.00, exp. dt:  0.00 p/m  0.00 s, t. trans:  0.0 hrs  1.0 min  21.2 s, exp. t. end:  0.0 hrs  10.0 min  28.0 s\n",
            "19000/157246, progress:  12.1 %, dt:  0.00, exp. dt:  0.00 p/m  0.00 s, t. trans:  0.0 hrs  1.0 min  25.4 s, exp. t. end:  0.0 hrs  10.0 min  21.3 s\n",
            "20000/157246, progress:  12.7 %, dt:  0.00, exp. dt:  0.00 p/m  0.00 s, t. trans:  0.0 hrs  1.0 min  29.6 s, exp. t. end:  0.0 hrs  10.0 min  15.1 s\n",
            "21000/157246, progress:  13.4 %, dt:  0.00, exp. dt:  0.00 p/m  0.00 s, t. trans:  0.0 hrs  1.0 min  33.8 s, exp. t. end:  0.0 hrs  10.0 min  8.7 s\n",
            "22000/157246, progress:  14.0 %, dt:  0.00, exp. dt:  0.00 p/m  0.00 s, t. trans:  0.0 hrs  1.0 min  38.1 s, exp. t. end:  0.0 hrs  10.0 min  3.0 s\n",
            "23000/157246, progress:  14.6 %, dt:  0.00, exp. dt:  0.00 p/m  0.00 s, t. trans:  0.0 hrs  1.0 min  42.4 s, exp. t. end:  0.0 hrs  9.0 min  57.6 s\n",
            "24000/157246, progress:  15.3 %, dt:  0.01, exp. dt:  0.00 p/m  0.00 s, t. trans:  0.0 hrs  1.0 min  47.3 s, exp. t. end:  0.0 hrs  9.0 min  55.9 s\n",
            "25000/157246, progress:  15.9 %, dt:  0.00, exp. dt:  0.00 p/m  0.00 s, t. trans:  0.0 hrs  1.0 min  51.6 s, exp. t. end:  0.0 hrs  9.0 min  50.3 s\n",
            "26000/157246, progress:  16.5 %, dt:  0.00, exp. dt:  0.00 p/m  0.00 s, t. trans:  0.0 hrs  1.0 min  55.8 s, exp. t. end:  0.0 hrs  9.0 min  44.7 s\n",
            "27000/157246, progress:  17.2 %, dt:  0.00, exp. dt:  0.00 p/m  0.00 s, t. trans:  0.0 hrs  2.0 min  0.0 s, exp. t. end:  0.0 hrs  9.0 min  39.1 s\n",
            "28000/157246, progress:  17.8 %, dt:  0.00, exp. dt:  0.00 p/m  0.00 s, t. trans:  0.0 hrs  2.0 min  4.3 s, exp. t. end:  0.0 hrs  9.0 min  33.9 s\n",
            "29000/157246, progress:  18.4 %, dt:  0.00, exp. dt:  0.00 p/m  0.00 s, t. trans:  0.0 hrs  2.0 min  8.5 s, exp. t. end:  0.0 hrs  9.0 min  28.4 s\n",
            "30000/157246, progress:  19.1 %, dt:  0.01, exp. dt:  0.00 p/m  0.00 s, t. trans:  0.0 hrs  2.0 min  12.7 s, exp. t. end:  0.0 hrs  9.0 min  23.0 s\n",
            "31000/157246, progress:  19.7 %, dt:  0.00, exp. dt:  0.00 p/m  0.00 s, t. trans:  0.0 hrs  2.0 min  17.0 s, exp. t. end:  0.0 hrs  9.0 min  17.9 s\n",
            "32000/157246, progress:  20.4 %, dt:  0.00, exp. dt:  0.00 p/m  0.00 s, t. trans:  0.0 hrs  2.0 min  21.2 s, exp. t. end:  0.0 hrs  9.0 min  12.7 s\n",
            "33000/157246, progress:  21.0 %, dt:  0.00, exp. dt:  0.00 p/m  0.00 s, t. trans:  0.0 hrs  2.0 min  25.8 s, exp. t. end:  0.0 hrs  9.0 min  9.1 s\n",
            "34000/157246, progress:  21.6 %, dt:  0.00, exp. dt:  0.00 p/m  0.00 s, t. trans:  0.0 hrs  2.0 min  30.1 s, exp. t. end:  0.0 hrs  9.0 min  4.0 s\n",
            "35000/157246, progress:  22.3 %, dt:  0.00, exp. dt:  0.00 p/m  0.00 s, t. trans:  0.0 hrs  2.0 min  34.3 s, exp. t. end:  0.0 hrs  8.0 min  59.0 s\n",
            "36000/157246, progress:  22.9 %, dt:  0.00, exp. dt:  0.00 p/m  0.00 s, t. trans:  0.0 hrs  2.0 min  38.5 s, exp. t. end:  0.0 hrs  8.0 min  54.0 s\n",
            "37000/157246, progress:  23.5 %, dt:  0.00, exp. dt:  0.00 p/m  0.00 s, t. trans:  0.0 hrs  2.0 min  42.7 s, exp. t. end:  0.0 hrs  8.0 min  48.8 s\n",
            "38000/157246, progress:  24.2 %, dt:  0.00, exp. dt:  0.00 p/m  0.00 s, t. trans:  0.0 hrs  2.0 min  46.9 s, exp. t. end:  0.0 hrs  8.0 min  43.7 s\n",
            "39000/157246, progress:  24.8 %, dt:  0.00, exp. dt:  0.00 p/m  0.00 s, t. trans:  0.0 hrs  2.0 min  51.2 s, exp. t. end:  0.0 hrs  8.0 min  39.0 s\n",
            "40000/157246, progress:  25.4 %, dt:  0.00, exp. dt:  0.00 p/m  0.00 s, t. trans:  0.0 hrs  2.0 min  55.3 s, exp. t. end:  0.0 hrs  8.0 min  33.9 s\n",
            "41000/157246, progress:  26.1 %, dt:  0.00, exp. dt:  0.00 p/m  0.00 s, t. trans:  0.0 hrs  2.0 min  59.6 s, exp. t. end:  0.0 hrs  8.0 min  29.1 s\n",
            "42000/157246, progress:  26.7 %, dt:  0.00, exp. dt:  0.00 p/m  0.00 s, t. trans:  0.0 hrs  3.0 min  3.8 s, exp. t. end:  0.0 hrs  8.0 min  24.3 s\n",
            "43000/157246, progress:  27.3 %, dt:  0.00, exp. dt:  0.00 p/m  0.00 s, t. trans:  0.0 hrs  3.0 min  8.0 s, exp. t. end:  0.0 hrs  8.0 min  19.4 s\n",
            "44000/157246, progress:  28.0 %, dt:  0.00, exp. dt:  0.00 p/m  0.00 s, t. trans:  0.0 hrs  3.0 min  12.2 s, exp. t. end:  0.0 hrs  8.0 min  14.6 s\n",
            "45000/157246, progress:  28.6 %, dt:  0.00, exp. dt:  0.00 p/m  0.00 s, t. trans:  0.0 hrs  3.0 min  16.3 s, exp. t. end:  0.0 hrs  8.0 min  9.7 s\n",
            "46000/157246, progress:  29.3 %, dt:  0.00, exp. dt:  0.00 p/m  0.00 s, t. trans:  0.0 hrs  3.0 min  20.6 s, exp. t. end:  0.0 hrs  8.0 min  5.0 s\n",
            "47000/157246, progress:  29.9 %, dt:  0.00, exp. dt:  0.00 p/m  0.00 s, t. trans:  0.0 hrs  3.0 min  24.9 s, exp. t. end:  0.0 hrs  8.0 min  0.5 s\n",
            "48000/157246, progress:  30.5 %, dt:  0.00, exp. dt:  0.00 p/m  0.00 s, t. trans:  0.0 hrs  3.0 min  29.1 s, exp. t. end:  0.0 hrs  7.0 min  55.9 s\n",
            "49000/157246, progress:  31.2 %, dt:  0.00, exp. dt:  0.00 p/m  0.00 s, t. trans:  0.0 hrs  3.0 min  33.4 s, exp. t. end:  0.0 hrs  7.0 min  51.3 s\n",
            "50000/157246, progress:  31.8 %, dt:  0.00, exp. dt:  0.00 p/m  0.00 s, t. trans:  0.0 hrs  3.0 min  37.6 s, exp. t. end:  0.0 hrs  7.0 min  46.8 s\n",
            "51000/157246, progress:  32.4 %, dt:  0.00, exp. dt:  0.00 p/m  0.00 s, t. trans:  0.0 hrs  3.0 min  41.9 s, exp. t. end:  0.0 hrs  7.0 min  42.3 s\n",
            "52000/157246, progress:  33.1 %, dt:  0.01, exp. dt:  0.00 p/m  0.00 s, t. trans:  0.0 hrs  3.0 min  46.2 s, exp. t. end:  0.0 hrs  7.0 min  37.8 s\n",
            "53000/157246, progress:  33.7 %, dt:  0.00, exp. dt:  0.00 p/m  0.00 s, t. trans:  0.0 hrs  3.0 min  50.6 s, exp. t. end:  0.0 hrs  7.0 min  33.6 s\n",
            "54000/157246, progress:  34.3 %, dt:  0.00, exp. dt:  0.00 p/m  0.00 s, t. trans:  0.0 hrs  3.0 min  54.8 s, exp. t. end:  0.0 hrs  7.0 min  29.0 s\n",
            "55000/157246, progress:  35.0 %, dt:  0.00, exp. dt:  0.00 p/m  0.00 s, t. trans:  0.0 hrs  3.0 min  58.9 s, exp. t. end:  0.0 hrs  7.0 min  24.2 s\n",
            "56000/157246, progress:  35.6 %, dt:  0.00, exp. dt:  0.00 p/m  0.00 s, t. trans:  0.0 hrs  4.0 min  3.2 s, exp. t. end:  0.0 hrs  7.0 min  19.7 s\n",
            "57000/157246, progress:  36.2 %, dt:  0.00, exp. dt:  0.00 p/m  0.00 s, t. trans:  0.0 hrs  4.0 min  7.5 s, exp. t. end:  0.0 hrs  7.0 min  15.2 s\n",
            "58000/157246, progress:  36.9 %, dt:  0.00, exp. dt:  0.00 p/m  0.00 s, t. trans:  0.0 hrs  4.0 min  11.7 s, exp. t. end:  0.0 hrs  7.0 min  10.7 s\n",
            "59000/157246, progress:  37.5 %, dt:  0.00, exp. dt:  0.00 p/m  0.00 s, t. trans:  0.0 hrs  4.0 min  16.0 s, exp. t. end:  0.0 hrs  7.0 min  6.2 s\n",
            "60000/157246, progress:  38.2 %, dt:  0.00, exp. dt:  0.00 p/m  0.00 s, t. trans:  0.0 hrs  4.0 min  20.1 s, exp. t. end:  0.0 hrs  7.0 min  1.6 s\n",
            "61000/157246, progress:  38.8 %, dt:  0.00, exp. dt:  0.00 p/m  0.00 s, t. trans:  0.0 hrs  4.0 min  24.4 s, exp. t. end:  0.0 hrs  6.0 min  57.2 s\n",
            "62000/157246, progress:  39.4 %, dt:  0.00, exp. dt:  0.00 p/m  0.00 s, t. trans:  0.0 hrs  4.0 min  28.6 s, exp. t. end:  0.0 hrs  6.0 min  52.7 s\n",
            "63000/157246, progress:  40.1 %, dt:  0.00, exp. dt:  0.00 p/m  0.00 s, t. trans:  0.0 hrs  4.0 min  32.8 s, exp. t. end:  0.0 hrs  6.0 min  48.2 s\n",
            "64000/157246, progress:  40.7 %, dt:  0.00, exp. dt:  0.00 p/m  0.00 s, t. trans:  0.0 hrs  4.0 min  37.1 s, exp. t. end:  0.0 hrs  6.0 min  43.7 s\n",
            "65000/157246, progress:  41.3 %, dt:  0.00, exp. dt:  0.00 p/m  0.00 s, t. trans:  0.0 hrs  4.0 min  41.4 s, exp. t. end:  0.0 hrs  6.0 min  39.4 s\n",
            "66000/157246, progress:  42.0 %, dt:  0.00, exp. dt:  0.00 p/m  0.00 s, t. trans:  0.0 hrs  4.0 min  45.7 s, exp. t. end:  0.0 hrs  6.0 min  34.9 s\n",
            "67000/157246, progress:  42.6 %, dt:  0.00, exp. dt:  0.00 p/m  0.00 s, t. trans:  0.0 hrs  4.0 min  49.9 s, exp. t. end:  0.0 hrs  6.0 min  30.5 s\n",
            "68000/157246, progress:  43.2 %, dt:  0.00, exp. dt:  0.00 p/m  0.00 s, t. trans:  0.0 hrs  4.0 min  54.1 s, exp. t. end:  0.0 hrs  6.0 min  26.0 s\n",
            "69000/157246, progress:  43.9 %, dt:  0.00, exp. dt:  0.00 p/m  0.00 s, t. trans:  0.0 hrs  4.0 min  58.4 s, exp. t. end:  0.0 hrs  6.0 min  21.6 s\n",
            "70000/157246, progress:  44.5 %, dt:  0.00, exp. dt:  0.00 p/m  0.00 s, t. trans:  0.0 hrs  5.0 min  2.6 s, exp. t. end:  0.0 hrs  6.0 min  17.2 s\n",
            "71000/157246, progress:  45.2 %, dt:  0.00, exp. dt:  0.00 p/m  0.00 s, t. trans:  0.0 hrs  5.0 min  6.8 s, exp. t. end:  0.0 hrs  6.0 min  12.7 s\n",
            "72000/157246, progress:  45.8 %, dt:  0.01, exp. dt:  0.00 p/m  0.00 s, t. trans:  0.0 hrs  5.0 min  11.0 s, exp. t. end:  0.0 hrs  6.0 min  8.3 s\n",
            "73000/157246, progress:  46.4 %, dt:  0.00, exp. dt:  0.00 p/m  0.00 s, t. trans:  0.0 hrs  5.0 min  15.5 s, exp. t. end:  0.0 hrs  6.0 min  4.1 s\n",
            "74000/157246, progress:  47.1 %, dt:  0.00, exp. dt:  0.00 p/m  0.00 s, t. trans:  0.0 hrs  5.0 min  19.8 s, exp. t. end:  0.0 hrs  5.0 min  59.8 s\n",
            "75000/157246, progress:  47.7 %, dt:  0.00, exp. dt:  0.00 p/m  0.00 s, t. trans:  0.0 hrs  5.0 min  23.9 s, exp. t. end:  0.0 hrs  5.0 min  55.2 s\n",
            "76000/157246, progress:  48.3 %, dt:  0.00, exp. dt:  0.00 p/m  0.00 s, t. trans:  0.0 hrs  5.0 min  28.1 s, exp. t. end:  0.0 hrs  5.0 min  50.8 s\n",
            "77000/157246, progress:  49.0 %, dt:  0.00, exp. dt:  0.00 p/m  0.00 s, t. trans:  0.0 hrs  5.0 min  32.3 s, exp. t. end:  0.0 hrs  5.0 min  46.3 s\n",
            "78000/157246, progress:  49.6 %, dt:  0.00, exp. dt:  0.00 p/m  0.00 s, t. trans:  0.0 hrs  5.0 min  36.5 s, exp. t. end:  0.0 hrs  5.0 min  41.9 s\n",
            "79000/157246, progress:  50.2 %, dt:  0.01, exp. dt:  0.00 p/m  0.00 s, t. trans:  0.0 hrs  5.0 min  41.2 s, exp. t. end:  0.0 hrs  5.0 min  37.9 s\n",
            "80000/157246, progress:  50.9 %, dt:  0.00, exp. dt:  0.00 p/m  0.00 s, t. trans:  0.0 hrs  5.0 min  45.7 s, exp. t. end:  0.0 hrs  5.0 min  33.8 s\n",
            "81000/157246, progress:  51.5 %, dt:  0.00, exp. dt:  0.00 p/m  0.00 s, t. trans:  0.0 hrs  5.0 min  49.9 s, exp. t. end:  0.0 hrs  5.0 min  29.4 s\n",
            "82000/157246, progress:  52.1 %, dt:  0.00, exp. dt:  0.00 p/m  0.00 s, t. trans:  0.0 hrs  5.0 min  54.1 s, exp. t. end:  0.0 hrs  5.0 min  24.9 s\n",
            "83000/157246, progress:  52.8 %, dt:  0.00, exp. dt:  0.00 p/m  0.00 s, t. trans:  0.0 hrs  5.0 min  58.3 s, exp. t. end:  0.0 hrs  5.0 min  20.5 s\n",
            "84000/157246, progress:  53.4 %, dt:  0.00, exp. dt:  0.00 p/m  0.00 s, t. trans:  0.0 hrs  6.0 min  2.5 s, exp. t. end:  0.0 hrs  5.0 min  16.1 s\n",
            "85000/157246, progress:  54.1 %, dt:  0.00, exp. dt:  0.00 p/m  0.00 s, t. trans:  0.0 hrs  6.0 min  6.9 s, exp. t. end:  0.0 hrs  5.0 min  11.8 s\n",
            "86000/157246, progress:  54.7 %, dt:  0.00, exp. dt:  0.00 p/m  0.00 s, t. trans:  0.0 hrs  6.0 min  11.1 s, exp. t. end:  0.0 hrs  5.0 min  7.4 s\n",
            "87000/157246, progress:  55.3 %, dt:  0.00, exp. dt:  0.00 p/m  0.00 s, t. trans:  0.0 hrs  6.0 min  15.3 s, exp. t. end:  0.0 hrs  5.0 min  3.0 s\n",
            "88000/157246, progress:  56.0 %, dt:  0.00, exp. dt:  0.00 p/m  0.00 s, t. trans:  0.0 hrs  6.0 min  19.6 s, exp. t. end:  0.0 hrs  4.0 min  58.7 s\n",
            "89000/157246, progress:  56.6 %, dt:  0.01, exp. dt:  0.00 p/m  0.00 s, t. trans:  0.0 hrs  6.0 min  24.2 s, exp. t. end:  0.0 hrs  4.0 min  54.6 s\n",
            "90000/157246, progress:  57.2 %, dt:  0.00, exp. dt:  0.00 p/m  0.00 s, t. trans:  0.0 hrs  6.0 min  28.4 s, exp. t. end:  0.0 hrs  4.0 min  50.2 s\n",
            "91000/157246, progress:  57.9 %, dt:  0.00, exp. dt:  0.00 p/m  0.00 s, t. trans:  0.0 hrs  6.0 min  32.7 s, exp. t. end:  0.0 hrs  4.0 min  45.9 s\n",
            "92000/157246, progress:  58.5 %, dt:  0.00, exp. dt:  0.00 p/m  0.00 s, t. trans:  0.0 hrs  6.0 min  36.9 s, exp. t. end:  0.0 hrs  4.0 min  41.5 s\n",
            "93000/157246, progress:  59.1 %, dt:  0.00, exp. dt:  0.00 p/m  0.00 s, t. trans:  0.0 hrs  6.0 min  41.2 s, exp. t. end:  0.0 hrs  4.0 min  37.1 s\n",
            "94000/157246, progress:  59.8 %, dt:  0.00, exp. dt:  0.00 p/m  0.00 s, t. trans:  0.0 hrs  6.0 min  45.4 s, exp. t. end:  0.0 hrs  4.0 min  32.8 s\n",
            "95000/157246, progress:  60.4 %, dt:  0.00, exp. dt:  0.00 p/m  0.00 s, t. trans:  0.0 hrs  6.0 min  49.7 s, exp. t. end:  0.0 hrs  4.0 min  28.4 s\n",
            "96000/157246, progress:  61.1 %, dt:  0.00, exp. dt:  0.00 p/m  0.00 s, t. trans:  0.0 hrs  6.0 min  53.9 s, exp. t. end:  0.0 hrs  4.0 min  24.1 s\n",
            "97000/157246, progress:  61.7 %, dt:  0.00, exp. dt:  0.00 p/m  0.00 s, t. trans:  0.0 hrs  6.0 min  58.1 s, exp. t. end:  0.0 hrs  4.0 min  19.7 s\n",
            "98000/157246, progress:  62.3 %, dt:  0.00, exp. dt:  0.00 p/m  0.00 s, t. trans:  0.0 hrs  7.0 min  2.3 s, exp. t. end:  0.0 hrs  4.0 min  15.3 s\n",
            "99000/157246, progress:  63.0 %, dt:  0.00, exp. dt:  0.00 p/m  0.00 s, t. trans:  0.0 hrs  7.0 min  6.6 s, exp. t. end:  0.0 hrs  4.0 min  11.0 s\n",
            "100000/157246, progress:  63.6 %, dt:  0.00, exp. dt:  0.00 p/m  0.00 s, t. trans:  0.0 hrs  7.0 min  10.9 s, exp. t. end:  0.0 hrs  4.0 min  6.7 s\n",
            "101000/157246, progress:  64.2 %, dt:  0.00, exp. dt:  0.00 p/m  0.00 s, t. trans:  0.0 hrs  7.0 min  15.1 s, exp. t. end:  0.0 hrs  4.0 min  2.3 s\n",
            "102000/157246, progress:  64.9 %, dt:  0.00, exp. dt:  0.00 p/m  0.00 s, t. trans:  0.0 hrs  7.0 min  19.3 s, exp. t. end:  0.0 hrs  3.0 min  58.0 s\n",
            "103000/157246, progress:  65.5 %, dt:  0.00, exp. dt:  0.00 p/m  0.00 s, t. trans:  0.0 hrs  7.0 min  23.6 s, exp. t. end:  0.0 hrs  3.0 min  53.6 s\n",
            "104000/157246, progress:  66.1 %, dt:  0.00, exp. dt:  0.00 p/m  0.00 s, t. trans:  0.0 hrs  7.0 min  27.9 s, exp. t. end:  0.0 hrs  3.0 min  49.3 s\n",
            "105000/157246, progress:  66.8 %, dt:  0.00, exp. dt:  0.00 p/m  0.00 s, t. trans:  0.0 hrs  7.0 min  32.2 s, exp. t. end:  0.0 hrs  3.0 min  45.0 s\n",
            "106000/157246, progress:  67.4 %, dt:  0.00, exp. dt:  0.00 p/m  0.00 s, t. trans:  0.0 hrs  7.0 min  36.4 s, exp. t. end:  0.0 hrs  3.0 min  40.7 s\n",
            "107000/157246, progress:  68.0 %, dt:  0.00, exp. dt:  0.00 p/m  0.00 s, t. trans:  0.0 hrs  7.0 min  40.7 s, exp. t. end:  0.0 hrs  3.0 min  36.3 s\n",
            "108000/157246, progress:  68.7 %, dt:  0.00, exp. dt:  0.00 p/m  0.00 s, t. trans:  0.0 hrs  7.0 min  44.9 s, exp. t. end:  0.0 hrs  3.0 min  32.0 s\n",
            "109000/157246, progress:  69.3 %, dt:  0.00, exp. dt:  0.00 p/m  0.00 s, t. trans:  0.0 hrs  7.0 min  49.0 s, exp. t. end:  0.0 hrs  3.0 min  27.6 s\n",
            "110000/157246, progress:  70.0 %, dt:  0.00, exp. dt:  0.00 p/m  0.00 s, t. trans:  0.0 hrs  7.0 min  53.3 s, exp. t. end:  0.0 hrs  3.0 min  23.3 s\n",
            "111000/157246, progress:  70.6 %, dt:  0.00, exp. dt:  0.00 p/m  0.00 s, t. trans:  0.0 hrs  7.0 min  57.5 s, exp. t. end:  0.0 hrs  3.0 min  18.9 s\n",
            "112000/157246, progress:  71.2 %, dt:  0.00, exp. dt:  0.00 p/m  0.00 s, t. trans:  0.0 hrs  8.0 min  1.7 s, exp. t. end:  0.0 hrs  3.0 min  14.6 s\n",
            "113000/157246, progress:  71.9 %, dt:  0.00, exp. dt:  0.00 p/m  0.00 s, t. trans:  0.0 hrs  8.0 min  5.9 s, exp. t. end:  0.0 hrs  3.0 min  10.3 s\n",
            "114000/157246, progress:  72.5 %, dt:  0.00, exp. dt:  0.00 p/m  0.00 s, t. trans:  0.0 hrs  8.0 min  10.2 s, exp. t. end:  0.0 hrs  3.0 min  6.0 s\n",
            "115000/157246, progress:  73.1 %, dt:  0.00, exp. dt:  0.00 p/m  0.00 s, t. trans:  0.0 hrs  8.0 min  14.4 s, exp. t. end:  0.0 hrs  3.0 min  1.6 s\n",
            "116000/157246, progress:  73.8 %, dt:  0.00, exp. dt:  0.00 p/m  0.00 s, t. trans:  0.0 hrs  8.0 min  18.6 s, exp. t. end:  0.0 hrs  2.0 min  57.3 s\n",
            "117000/157246, progress:  74.4 %, dt:  0.00, exp. dt:  0.00 p/m  0.00 s, t. trans:  0.0 hrs  8.0 min  22.9 s, exp. t. end:  0.0 hrs  2.0 min  53.0 s\n",
            "118000/157246, progress:  75.0 %, dt:  0.00, exp. dt:  0.00 p/m  0.00 s, t. trans:  0.0 hrs  8.0 min  27.2 s, exp. t. end:  0.0 hrs  2.0 min  48.7 s\n",
            "119000/157246, progress:  75.7 %, dt:  0.00, exp. dt:  0.00 p/m  0.00 s, t. trans:  0.0 hrs  8.0 min  31.4 s, exp. t. end:  0.0 hrs  2.0 min  44.4 s\n",
            "120000/157246, progress:  76.3 %, dt:  0.00, exp. dt:  0.00 p/m  0.00 s, t. trans:  0.0 hrs  8.0 min  35.8 s, exp. t. end:  0.0 hrs  2.0 min  40.1 s\n",
            "121000/157246, progress:  76.9 %, dt:  0.00, exp. dt:  0.00 p/m  0.00 s, t. trans:  0.0 hrs  8.0 min  40.1 s, exp. t. end:  0.0 hrs  2.0 min  35.8 s\n",
            "122000/157246, progress:  77.6 %, dt:  0.00, exp. dt:  0.00 p/m  0.00 s, t. trans:  0.0 hrs  8.0 min  44.2 s, exp. t. end:  0.0 hrs  2.0 min  31.5 s\n",
            "123000/157246, progress:  78.2 %, dt:  0.00, exp. dt:  0.00 p/m  0.00 s, t. trans:  0.0 hrs  8.0 min  48.5 s, exp. t. end:  0.0 hrs  2.0 min  27.1 s\n",
            "124000/157246, progress:  78.9 %, dt:  0.00, exp. dt:  0.00 p/m  0.00 s, t. trans:  0.0 hrs  8.0 min  52.7 s, exp. t. end:  0.0 hrs  2.0 min  22.8 s\n",
            "125000/157246, progress:  79.5 %, dt:  0.01, exp. dt:  0.00 p/m  0.00 s, t. trans:  0.0 hrs  8.0 min  57.2 s, exp. t. end:  0.0 hrs  2.0 min  18.6 s\n",
            "126000/157246, progress:  80.1 %, dt:  0.00, exp. dt:  0.00 p/m  0.00 s, t. trans:  0.0 hrs  9.0 min  1.5 s, exp. t. end:  0.0 hrs  2.0 min  14.3 s\n",
            "127000/157246, progress:  80.8 %, dt:  0.00, exp. dt:  0.00 p/m  0.00 s, t. trans:  0.0 hrs  9.0 min  5.7 s, exp. t. end:  0.0 hrs  2.0 min  10.0 s\n",
            "128000/157246, progress:  81.4 %, dt:  0.00, exp. dt:  0.00 p/m  0.00 s, t. trans:  0.0 hrs  9.0 min  9.9 s, exp. t. end:  0.0 hrs  2.0 min  5.6 s\n",
            "129000/157246, progress:  82.0 %, dt:  0.00, exp. dt:  0.00 p/m  0.00 s, t. trans:  0.0 hrs  9.0 min  14.2 s, exp. t. end:  0.0 hrs  2.0 min  1.4 s\n",
            "130000/157246, progress:  82.7 %, dt:  0.00, exp. dt:  0.00 p/m  0.00 s, t. trans:  0.0 hrs  9.0 min  18.5 s, exp. t. end:  0.0 hrs  1.0 min  57.0 s\n",
            "131000/157246, progress:  83.3 %, dt:  0.00, exp. dt:  0.00 p/m  0.00 s, t. trans:  0.0 hrs  9.0 min  22.8 s, exp. t. end:  0.0 hrs  1.0 min  52.8 s\n",
            "132000/157246, progress:  83.9 %, dt:  0.00, exp. dt:  0.00 p/m  0.00 s, t. trans:  0.0 hrs  9.0 min  27.1 s, exp. t. end:  0.0 hrs  1.0 min  48.5 s\n",
            "133000/157246, progress:  84.6 %, dt:  0.00, exp. dt:  0.00 p/m  0.00 s, t. trans:  0.0 hrs  9.0 min  31.4 s, exp. t. end:  0.0 hrs  1.0 min  44.2 s\n",
            "134000/157246, progress:  85.2 %, dt:  0.00, exp. dt:  0.00 p/m  0.00 s, t. trans:  0.0 hrs  9.0 min  35.8 s, exp. t. end:  0.0 hrs  1.0 min  39.9 s\n",
            "135000/157246, progress:  85.9 %, dt:  0.00, exp. dt:  0.00 p/m  0.00 s, t. trans:  0.0 hrs  9.0 min  40.1 s, exp. t. end:  0.0 hrs  1.0 min  35.6 s\n",
            "136000/157246, progress:  86.5 %, dt:  0.00, exp. dt:  0.00 p/m  0.00 s, t. trans:  0.0 hrs  9.0 min  44.5 s, exp. t. end:  0.0 hrs  1.0 min  31.3 s\n",
            "137000/157246, progress:  87.1 %, dt:  0.00, exp. dt:  0.00 p/m  0.00 s, t. trans:  0.0 hrs  9.0 min  48.8 s, exp. t. end:  0.0 hrs  1.0 min  27.0 s\n",
            "138000/157246, progress:  87.8 %, dt:  0.00, exp. dt:  0.00 p/m  0.00 s, t. trans:  0.0 hrs  9.0 min  53.1 s, exp. t. end:  0.0 hrs  1.0 min  22.7 s\n",
            "139000/157246, progress:  88.4 %, dt:  0.00, exp. dt:  0.00 p/m  0.00 s, t. trans:  0.0 hrs  9.0 min  57.5 s, exp. t. end:  0.0 hrs  1.0 min  18.4 s\n",
            "140000/157246, progress:  89.0 %, dt:  0.01, exp. dt:  0.00 p/m  0.00 s, t. trans:  0.0 hrs  10.0 min  1.8 s, exp. t. end:  0.0 hrs  1.0 min  14.1 s\n",
            "141000/157246, progress:  89.7 %, dt:  0.00, exp. dt:  0.00 p/m  0.00 s, t. trans:  0.0 hrs  10.0 min  6.2 s, exp. t. end:  0.0 hrs  1.0 min  9.8 s\n",
            "142000/157246, progress:  90.3 %, dt:  0.00, exp. dt:  0.00 p/m  0.00 s, t. trans:  0.0 hrs  10.0 min  10.6 s, exp. t. end:  0.0 hrs  1.0 min  5.6 s\n",
            "143000/157246, progress:  90.9 %, dt:  0.00, exp. dt:  0.00 p/m  0.00 s, t. trans:  0.0 hrs  10.0 min  15.0 s, exp. t. end:  0.0 hrs  1.0 min  1.3 s\n",
            "144000/157246, progress:  91.6 %, dt:  0.00, exp. dt:  0.00 p/m  0.00 s, t. trans:  0.0 hrs  10.0 min  19.4 s, exp. t. end:  0.0 hrs  0.0 min  57.0 s\n",
            "145000/157246, progress:  92.2 %, dt:  0.00, exp. dt:  0.00 p/m  0.00 s, t. trans:  0.0 hrs  10.0 min  23.8 s, exp. t. end:  0.0 hrs  0.0 min  52.7 s\n",
            "146000/157246, progress:  92.8 %, dt:  0.00, exp. dt:  0.00 p/m  0.00 s, t. trans:  0.0 hrs  10.0 min  28.2 s, exp. t. end:  0.0 hrs  0.0 min  48.4 s\n",
            "147000/157246, progress:  93.5 %, dt:  0.00, exp. dt:  0.00 p/m  0.00 s, t. trans:  0.0 hrs  10.0 min  32.5 s, exp. t. end:  0.0 hrs  0.0 min  44.1 s\n",
            "148000/157246, progress:  94.1 %, dt:  0.00, exp. dt:  0.00 p/m  0.00 s, t. trans:  0.0 hrs  10.0 min  36.9 s, exp. t. end:  0.0 hrs  0.0 min  39.8 s\n",
            "149000/157246, progress:  94.8 %, dt:  0.00, exp. dt:  0.00 p/m  0.00 s, t. trans:  0.0 hrs  10.0 min  41.2 s, exp. t. end:  0.0 hrs  0.0 min  35.5 s\n",
            "150000/157246, progress:  95.4 %, dt:  0.00, exp. dt:  0.00 p/m  0.00 s, t. trans:  0.0 hrs  10.0 min  45.7 s, exp. t. end:  0.0 hrs  0.0 min  31.2 s\n",
            "151000/157246, progress:  96.0 %, dt:  0.00, exp. dt:  0.00 p/m  0.00 s, t. trans:  0.0 hrs  10.0 min  50.0 s, exp. t. end:  0.0 hrs  0.0 min  26.9 s\n",
            "152000/157246, progress:  96.7 %, dt:  0.00, exp. dt:  0.00 p/m  0.00 s, t. trans:  0.0 hrs  10.0 min  54.5 s, exp. t. end:  0.0 hrs  0.0 min  22.6 s\n",
            "153000/157246, progress:  97.3 %, dt:  0.00, exp. dt:  0.00 p/m  0.00 s, t. trans:  0.0 hrs  10.0 min  58.8 s, exp. t. end:  0.0 hrs  0.0 min  18.3 s\n",
            "154000/157246, progress:  97.9 %, dt:  0.00, exp. dt:  0.00 p/m  0.00 s, t. trans:  0.0 hrs  11.0 min  3.2 s, exp. t. end:  0.0 hrs  0.0 min  14.0 s\n",
            "155000/157246, progress:  98.6 %, dt:  0.00, exp. dt:  0.00 p/m  0.00 s, t. trans:  0.0 hrs  11.0 min  7.6 s, exp. t. end:  0.0 hrs  0.0 min  9.7 s\n",
            "156000/157246, progress:  99.2 %, dt:  0.00, exp. dt:  0.00 p/m  0.00 s, t. trans:  0.0 hrs  11.0 min  11.9 s, exp. t. end:  0.0 hrs  0.0 min  5.4 s\n",
            "157000/157246, progress:  99.8 %, dt:  0.00, exp. dt:  0.00 p/m  0.00 s, t. trans:  0.0 hrs  11.0 min  17.1 s, exp. t. end:  0.0 hrs  0.0 min  1.1 s\n",
            "157246/157246, progress:  100.0 %, dt:  0.01, exp. dt:  0.00 p/m  0.00 s, t. trans:  0.0 hrs  11.0 min  18.2 s, exp. t. end:  0.0 hrs  0.0 min  0.0 s\n",
            "CPU times: user 11min 20s, sys: 2.3 s, total: 11min 22s\n",
            "Wall time: 11min 22s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "lines_stopwords_wo = [line for line in lines_stopwords_masked if len(line.split())==2 and line.split()[0] != \"<stopword>\"]\n",
        "lines_stopwords_wo"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D5qo3QG_Oxtl",
        "outputId": "85101173-6f16-45a7-a237-a5c5839f8bfb"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 71.1 ms, sys: 1.01 ms, total: 72.2 ms\n",
            "Wall time: 71.7 ms\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "lines_dev_stopwords_masked = preprocessing(lines_dev, method=\"masked\", verbose=True)\n",
        "lines_dev_stopwords_masked "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3lAw1h3XPWEC",
        "outputId": "04942bd0-84ab-4b12-9667-7fcdf64bdb90"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1000/17467, progress:  5.7 %, dt:  0.01, exp. dt:  0.01 p/m  0.00 s, t. trans:  0.0 hrs  0.0 min  5.3 s, exp. t. end:  0.0 hrs  1.0 min  26.8 s\n",
            "2000/17467, progress:  11.5 %, dt:  0.00, exp. dt:  0.01 p/m  0.00 s, t. trans:  0.0 hrs  0.0 min  13.0 s, exp. t. end:  0.0 hrs  1.0 min  40.7 s\n",
            "3000/17467, progress:  17.2 %, dt:  0.00, exp. dt:  0.01 p/m  0.00 s, t. trans:  0.0 hrs  0.0 min  17.5 s, exp. t. end:  0.0 hrs  1.0 min  24.3 s\n",
            "4000/17467, progress:  22.9 %, dt:  0.00, exp. dt:  0.01 p/m  0.00 s, t. trans:  0.0 hrs  0.0 min  22.0 s, exp. t. end:  0.0 hrs  1.0 min  14.1 s\n",
            "5000/17467, progress:  28.6 %, dt:  0.00, exp. dt:  0.01 p/m  0.00 s, t. trans:  0.0 hrs  0.0 min  26.3 s, exp. t. end:  0.0 hrs  1.0 min  5.6 s\n",
            "6000/17467, progress:  34.4 %, dt:  0.01, exp. dt:  0.01 p/m  0.00 s, t. trans:  0.0 hrs  0.0 min  30.6 s, exp. t. end:  0.0 hrs  0.0 min  58.5 s\n",
            "7000/17467, progress:  40.1 %, dt:  0.00, exp. dt:  0.00 p/m  0.00 s, t. trans:  0.0 hrs  0.0 min  34.9 s, exp. t. end:  0.0 hrs  0.0 min  52.1 s\n",
            "8000/17467, progress:  45.8 %, dt:  0.00, exp. dt:  0.00 p/m  0.00 s, t. trans:  0.0 hrs  0.0 min  39.2 s, exp. t. end:  0.0 hrs  0.0 min  46.4 s\n",
            "9000/17467, progress:  51.5 %, dt:  0.00, exp. dt:  0.00 p/m  0.00 s, t. trans:  0.0 hrs  0.0 min  43.6 s, exp. t. end:  0.0 hrs  0.0 min  41.0 s\n",
            "10000/17467, progress:  57.3 %, dt:  0.00, exp. dt:  0.00 p/m  0.00 s, t. trans:  0.0 hrs  0.0 min  47.9 s, exp. t. end:  0.0 hrs  0.0 min  35.8 s\n",
            "11000/17467, progress:  63.0 %, dt:  0.00, exp. dt:  0.00 p/m  0.00 s, t. trans:  0.0 hrs  0.0 min  52.3 s, exp. t. end:  0.0 hrs  0.0 min  30.7 s\n",
            "12000/17467, progress:  68.7 %, dt:  0.00, exp. dt:  0.00 p/m  0.00 s, t. trans:  0.0 hrs  0.0 min  56.6 s, exp. t. end:  0.0 hrs  0.0 min  25.8 s\n",
            "13000/17467, progress:  74.4 %, dt:  0.00, exp. dt:  0.00 p/m  0.00 s, t. trans:  0.0 hrs  1.0 min  1.0 s, exp. t. end:  0.0 hrs  0.0 min  20.9 s\n",
            "14000/17467, progress:  80.2 %, dt:  0.00, exp. dt:  0.00 p/m  0.00 s, t. trans:  0.0 hrs  1.0 min  5.3 s, exp. t. end:  0.0 hrs  0.0 min  16.2 s\n",
            "15000/17467, progress:  85.9 %, dt:  0.00, exp. dt:  0.00 p/m  0.00 s, t. trans:  0.0 hrs  1.0 min  9.6 s, exp. t. end:  0.0 hrs  0.0 min  11.5 s\n",
            "16000/17467, progress:  91.6 %, dt:  0.00, exp. dt:  0.00 p/m  0.00 s, t. trans:  0.0 hrs  1.0 min  14.0 s, exp. t. end:  0.0 hrs  0.0 min  6.8 s\n",
            "17000/17467, progress:  97.3 %, dt:  0.01, exp. dt:  0.00 p/m  0.00 s, t. trans:  0.0 hrs  1.0 min  18.3 s, exp. t. end:  0.0 hrs  0.0 min  2.2 s\n",
            "17467/17467, progress:  100.0 %, dt:  0.00, exp. dt:  0.00 p/m  0.00 s, t. trans:  0.0 hrs  1.0 min  20.4 s, exp. t. end:  0.0 hrs  0.0 min  0.0 s\n",
            "CPU times: user 1min 18s, sys: 299 ms, total: 1min 19s\n",
            "Wall time: 1min 20s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open('train_wo.txt', 'a') as f:\n",
        "    f.writelines(lines_stopwords_wo)\n",
        "with open('train_masked.txt', 'a') as f:\n",
        "    f.writelines(lines_stopwords_masked)\n",
        "with open('dev_masked.txt', 'a') as f:\n",
        "    f.writelines(lines_dev_stopwords_masked)"
      ],
      "metadata": {
        "id": "IlMV0nJAr4FS"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3DcM_IjgCdzz"
      },
      "source": [
        "# Primer Field: TEXT. Representan los tokens de la secuencia\n",
        "TEXT = legacy.data.Field(lower=False) \n",
        "TEXT_masked = legacy.data.Field(lower=False) \n",
        "TEXT_wo = legacy.data.Field(lower=False) \n",
        "\n",
        "# Segundo Field: NER_TAGS. Representan los Tags asociados a cada palabra.\n",
        "NER_TAGS = legacy.data.Field(unk_token=None)\n",
        "fields = ((\"text\", TEXT), (\"nertags\", NER_TAGS))\n",
        "\n",
        "NER_TAGS_masked = legacy.data.Field(unk_token=None)\n",
        "fields_masked = ((\"text\", TEXT_masked), (\"nertags\", NER_TAGS_masked))\n",
        "\n",
        "NER_TAGS_wo = legacy.data.Field(unk_token=None)\n",
        "fields_wo = ((\"text\", TEXT_wo), (\"nertags\", NER_TAGS_wo))"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xCKTJOdgC5eC"
      },
      "source": [
        "\n",
        "La idea es que usando los fields que definimos antes, le indiquemos a la clase cómo cargar los datasets de prueba, validación y test."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HsHdGml62J21"
      },
      "source": [
        "train_data, valid_data, test_data = legacy.datasets.SequenceTaggingDataset.splits(\n",
        "    path=\"./\",\n",
        "    train=\"train.txt\",\n",
        "    validation=\"dev.txt\",\n",
        "    test=\"test.txt\",\n",
        "    fields=fields,\n",
        "    encoding=\"utf-8\",\n",
        "    separator=\" \"\n",
        ")\n",
        "\n",
        "train_data_masked, valid_data_masked, test_data_masked = legacy.datasets.SequenceTaggingDataset.splits(\n",
        "    path=\"./\",\n",
        "    train=\"train_masked.txt\",\n",
        "    validation=\"dev_masked.txt\",\n",
        "    test=\"test.txt\",\n",
        "    fields=fields_masked,\n",
        "    encoding=\"utf-8\",\n",
        "    separator=\" \"\n",
        ")\n",
        "\n",
        "train_data_wo, valid_data_wo, test_data_wo = legacy.datasets.SequenceTaggingDataset.splits(\n",
        "    path=\"./\",\n",
        "    train=\"train_wo.txt\",\n",
        "    validation=\"dev.txt\",\n",
        "    test=\"test.txt\",\n",
        "    fields=fields_wo,\n",
        "    encoding=\"utf-8\",\n",
        "    separator=\" \"\n",
        ")"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PBhp7WICiibL"
      },
      "source": [
        "TEXT.build_vocab(train_data)\n",
        "NER_TAGS.build_vocab(train_data)\n",
        "\n",
        "TEXT_masked.build_vocab(train_data_masked)\n",
        "NER_TAGS_masked.build_vocab(train_data_masked)\n",
        "\n",
        "TEXT_wo.build_vocab(train_data_wo)\n",
        "NER_TAGS_wo.build_vocab(train_data_wo)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M4OgUKM_iibO",
        "scrolled": true,
        "outputId": "c73d28bb-5025-4114-d49e-628c30e15e1e"
      },
      "source": [
        "print(f\"Tokens únicos en TEXT: {len(TEXT.vocab)}\")\n",
        "print(f\"Tokens únicos en NER_TAGS: {len(NER_TAGS.vocab)}\")\n",
        "\n",
        "print(f\"Tokens únicos en TEXT_masked: {len(TEXT_masked.vocab)}\")\n",
        "print(f\"Tokens únicos en NER_TAGS_masked: {len(NER_TAGS_masked.vocab)}\")\n",
        "\n",
        "print(f\"Tokens únicos en TEXT_wo: {len(TEXT_wo.vocab)}\")\n",
        "print(f\"Tokens únicos en NER_TAGS_wo: {len(NER_TAGS_wo.vocab)}\")"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokens únicos en TEXT: 17591\n",
            "Tokens únicos en NER_TAGS: 12\n",
            "Tokens únicos en TEXT_masked: 16968\n",
            "Tokens únicos en NER_TAGS_masked: 12\n",
            "Tokens únicos en TEXT_wo: 16967\n",
            "Tokens únicos en NER_TAGS_wo: 12\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d4FeyL9nFnId",
        "outputId": "0500feb6-33c7-4fe7-f9ce-f693ef212457"
      },
      "source": [
        "#Veamos las posibles etiquetas que hemos cargado:\n",
        "print(NER_TAGS.vocab.itos)\n",
        "\n",
        "print(NER_TAGS_masked.vocab.itos)\n",
        "\n",
        "print(NER_TAGS_wo.vocab.itos)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['<pad>', 'O', 'I-Disease', 'B-Disease', 'I-Body_Part', 'B-Body_Part', 'B-Procedure', 'I-Procedure', 'B-Medication', 'B-Family_Member', 'I-Medication', 'I-Family_Member']\n",
            "['<pad>', 'O', 'I-Disease', 'B-Disease', 'I-Body_Part', 'B-Body_Part', 'B-Procedure', 'I-Procedure', 'B-Medication', 'B-Family_Member', 'I-Medication', 'I-Family_Member']\n",
            "['<pad>', 'O', 'I-Disease', 'B-Disease', 'I-Body_Part', 'B-Body_Part', 'B-Procedure', 'I-Procedure', 'B-Medication', 'B-Family_Member', 'I-Medication', 'I-Family_Member']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HYQDoUqSHFKj"
      },
      "source": [
        "Observen que ademas de los tags NER, tenemos \\<pad\\>, el cual es generado por el dataloader para cumplir con el padding de cada oración.\n",
        "\n",
        "Veamos ahora los tokens mas frecuentes y especiales:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WDyNLMPz9duD"
      },
      "source": [
        "# Seteamos algunas variables que nos serán de utilidad mas adelante...\n",
        "UNK_IDX = TEXT.vocab.stoi[TEXT.unk_token]\n",
        "PAD_IDX = TEXT.vocab.stoi[TEXT.pad_token]\n",
        "\n",
        "PAD_TAG_IDX = NER_TAGS.vocab.stoi[NER_TAGS.pad_token]\n",
        "O_TAG_IDX = NER_TAGS.vocab.stoi['O']"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uB7cwLWpaSGs",
        "outputId": "3ac1fc03-4a8a-4826-e694-12c5e185b121"
      },
      "source": [
        "BATCH_SIZE = 16  # disminuir si hay problemas de ram.\n",
        "\n",
        "# Usar cuda si es que está disponible.\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print('Using', device)\n",
        "\n",
        "# Dividir datos entre entrenamiento y test. Si van a hacer algún sort no puede ser sobre\n",
        "# el conjunto de testing ya que al hacer sus predicciones sobre el conjunto de test sin etiquetas\n",
        "# debe conservar el orden original para ser comparado con los golden_labels. \n",
        "\n",
        "train_iterator, valid_iterator, test_iterator = legacy.data.BucketIterator.splits(\n",
        "    (train_data, valid_data, test_data),\n",
        "    batch_size=BATCH_SIZE,\n",
        "    device=device,\n",
        "    sort=False,\n",
        ")\n",
        "\n",
        "train_iterator_masked, valid_iterator_masked, test_iterator_masked = legacy.data.BucketIterator.splits(\n",
        "    (train_data_masked, valid_data_masked, test_data_masked),\n",
        "    batch_size=BATCH_SIZE,\n",
        "    device=device,\n",
        "    sort=False,\n",
        ")\n",
        "\n",
        "train_iterator_wo, valid_iterator_wo, test_iterator_wo = legacy.data.BucketIterator.splits(\n",
        "    (train_data_wo, valid_data_wo, test_data_wo),\n",
        "    batch_size=BATCH_SIZE,\n",
        "    device=device,\n",
        "    sort=False,\n",
        ")"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o63ov69_rX2T",
        "outputId": "fd976f72-a6d3-4e88-d813-1594deef1363"
      },
      "source": [
        "!pip install seqeval"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33mWARNING: Ignoring invalid distribution -orch (/usr/local/lib/python3.7/dist-packages)\u001b[0m\n",
            "\u001b[33mWARNING: Ignoring invalid distribution -orch (/usr/local/lib/python3.7/dist-packages)\u001b[0m\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting seqeval\n",
            "  Downloading seqeval-1.2.2.tar.gz (43 kB)\n",
            "\u001b[K     |████████████████████████████████| 43 kB 1.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.7/dist-packages (from seqeval) (1.21.6)\n",
            "Requirement already satisfied: scikit-learn>=0.21.3 in /usr/local/lib/python3.7/dist-packages (from seqeval) (1.0.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.21.3->seqeval) (3.1.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.21.3->seqeval) (1.1.0)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.21.3->seqeval) (1.4.1)\n",
            "Building wheels for collected packages: seqeval\n",
            "  Building wheel for seqeval (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for seqeval: filename=seqeval-1.2.2-py3-none-any.whl size=16180 sha256=62712fc2d1c38198a5dfac89d1576b8b219b0f664fad312cfdad423678ffb692\n",
            "  Stored in directory: /root/.cache/pip/wheels/05/96/ee/7cac4e74f3b19e3158dce26a20a1c86b3533c43ec72a549fd7\n",
            "Successfully built seqeval\n",
            "\u001b[33mWARNING: Ignoring invalid distribution -orch (/usr/local/lib/python3.7/dist-packages)\u001b[0m\n",
            "Installing collected packages: seqeval\n",
            "\u001b[33mWARNING: Ignoring invalid distribution -orch (/usr/local/lib/python3.7/dist-packages)\u001b[0m\n",
            "Successfully installed seqeval-1.2.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9mUOOLEWiicU"
      },
      "source": [
        "# Definimos las métricas\n",
        "\n",
        "from seqeval.metrics import f1_score, precision_score, recall_score\n",
        "\n",
        "def calculate_metrics(preds, y_true, pad_idx=0):\n",
        "    \"\"\"\n",
        "    Calcula precision, recall y f1 de cada batch.\n",
        "    \"\"\"\n",
        "\n",
        "    # Obtener el indice de la clase con probabilidad mayor. (clases)\n",
        "    y_pred = preds.argmax(dim=1, keepdim=True)\n",
        "\n",
        "    # filtramos <pad> para calcular los scores.\n",
        "    mask = [(y_true != pad_idx)]\n",
        "    y_pred = y_pred[mask]\n",
        "    y_true = y_true[mask]\n",
        "\n",
        "    # traemos a la cpu\n",
        "    y_pred = y_pred.view(-1).to('cpu').numpy()\n",
        "    y_true = y_true.to('cpu').numpy()\n",
        "    y_pred = [[NER_TAGS.vocab.itos[v] for v in y_pred]]\n",
        "    y_true = [[NER_TAGS.vocab.itos[v] for v in y_true]]\n",
        "    \n",
        "    # calcular scores\n",
        "    f1 = f1_score(y_true, y_pred, mode='strict')\n",
        "    precision = precision_score(y_true, y_pred, mode='strict')\n",
        "    recall = recall_score(y_true, y_pred, mode='strict')\n",
        "\n",
        "    return precision, recall, f1"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rMPL08XqaSG3"
      },
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "\n",
        "# Definir la red\n",
        "class NER_RNN(nn.Module):\n",
        "    def __init__(self, \n",
        "                 input_dim, \n",
        "                 embedding_dim, \n",
        "                 hidden_dim, \n",
        "                 output_dim,\n",
        "                 n_layers, \n",
        "                 bidirectional, \n",
        "                 dropout, \n",
        "                 pad_idx):\n",
        "\n",
        "        super().__init__()\n",
        "\n",
        "        # Capa de embedding\n",
        "        self.embedding = nn.Embedding(input_dim,\n",
        "                                      embedding_dim,\n",
        "                                      padding_idx=pad_idx,\n",
        "                                      )\n",
        "\n",
        "        # Capa LSTM\n",
        "        self.lstm = nn.LSTM(embedding_dim,\n",
        "                           hidden_dim,\n",
        "                           num_layers=n_layers,\n",
        "                           bidirectional=bidirectional, \n",
        "                           dropout = dropout if n_layers > 1 else 0)\n",
        "\n",
        "        # Capa de salida\n",
        "        self.fc = nn.Linear(hidden_dim * 2 if bidirectional else hidden_dim,\n",
        "                            output_dim)\n",
        "\n",
        "        # Dropout\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, text):\n",
        "\n",
        "        #text = [sent len, batch size]\n",
        "\n",
        "        # Convertir lo enviado a embedding\n",
        "        embedded = self.dropout(self.embedding(text))\n",
        "        \n",
        "        outputs, (hidden, cell) = self.lstm(embedded)\n",
        "        #embedded = [sent len, batch size, emb dim]\n",
        "\n",
        "        # Pasar los embeddings por la rnn (LSTM)\n",
        "\n",
        "        #output = [sent len, batch size, hid dim * n directions]\n",
        "        #hidden/cell = [n layers * n directions, batch size, hid dim]\n",
        "\n",
        "        # Predecir usando la capa de salida.\n",
        "        predictions = self.fc(self.dropout(outputs))\n",
        "        #predictions = [sent len, batch size, output dim]\n",
        "\n",
        "        return predictions"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cCl3530VaSG7"
      },
      "source": [
        "#### **Hiperparámetros de la red**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EHdi3QdOaSG8"
      },
      "source": [
        "# tamaño del vocabulario. recuerden que la entrada son vectores bag of word(one-hot).\n",
        "EMBEDDING_DIM = 200  # dimensión de los embeddings.\n",
        "HIDDEN_DIM = 128  # dimensión de la capas LSTM\n",
        "OUTPUT_DIM = len(NER_TAGS.vocab)  # número de clases\n",
        "\n",
        "N_LAYERS = 3  # número de capas.\n",
        "DROPOUT = 0.5\n",
        "BIDIRECTIONAL = False\n",
        "\n",
        "# Creamos nuestro modelo.\n",
        "baseline_model = NER_RNN(len(TEXT.vocab), EMBEDDING_DIM, HIDDEN_DIM, OUTPUT_DIM,\n",
        "                         N_LAYERS, BIDIRECTIONAL, DROPOUT, PAD_IDX)\n",
        "\n",
        "baseline_model_name = 'baseline'  \n",
        "\n",
        "# Creamos nuestro modelo.\n",
        "masked_model = NER_RNN(len(TEXT_masked.vocab), EMBEDDING_DIM, HIDDEN_DIM, OUTPUT_DIM,\n",
        "                         N_LAYERS, BIDIRECTIONAL, DROPOUT, PAD_IDX)\n",
        "\n",
        "masked_model_name = 'masked'  \n",
        "\n",
        "# Creamos nuestro modelo.\n",
        "wo_model = NER_RNN(len(TEXT_wo.vocab), EMBEDDING_DIM, HIDDEN_DIM, OUTPUT_DIM,\n",
        "                         N_LAYERS, BIDIRECTIONAL, DROPOUT, PAD_IDX)\n",
        "\n",
        "wo_model_name = 'wo'  "
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jlF1DhJeaSHA"
      },
      "source": [
        "baseline_n_epochs = 10"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s3u4imJGaSHE"
      },
      "source": [
        "#### Definimos la función de loss"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6G_4k99_aSHG"
      },
      "source": [
        "# Loss: Cross Entropy\n",
        "TAG_PAD_IDX = NER_TAGS.vocab.stoi[NER_TAGS.pad_token]\n",
        "baseline_criterion = nn.CrossEntropyLoss(ignore_index = TAG_PAD_IDX)\n",
        "masked_criterion = nn.CrossEntropyLoss(ignore_index = TAG_PAD_IDX)\n",
        "wo_criterion = nn.CrossEntropyLoss(ignore_index = TAG_PAD_IDX)"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c81f8ki5aSHL"
      },
      "source": [
        "model_1 = baseline_model\n",
        "model_name_1 = baseline_model_name\n",
        "n_epochs_1 = baseline_n_epochs\n",
        "criterion_1 = baseline_criterion"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rV9oLkN1aSHO"
      },
      "source": [
        "---------------\n",
        "\n",
        "### Modelo 2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KWPzETaNaSHP"
      },
      "source": [
        "model_2 = masked_model\n",
        "model_name_2 = masked_model_name\n",
        "n_epochs_2 = baseline_n_epochs\n",
        "criterion_2 = masked_criterion"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zpy3p7YaaSHT"
      },
      "source": [
        "---------------\n",
        "\n",
        "\n",
        "### Modelo 3"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_w0CFjA8aSHU"
      },
      "source": [
        "model_3 = wo_model\n",
        "model_name_3 = wo_model_name\n",
        "n_epochs_3 = baseline_n_epochs\n",
        "criterion_3= wo_criterion"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xs-n9Y5yiica"
      },
      "source": [
        "import time\n",
        "\n",
        "def epoch_time(start_time, end_time):\n",
        "    elapsed_time = end_time - start_time\n",
        "    elapsed_mins = int(elapsed_time / 60)\n",
        "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
        "    return elapsed_mins, elapsed_secs\n",
        "\n",
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "def train(model, iterator, optimizer, criterion):\n",
        "\n",
        "    epoch_loss = 0\n",
        "    epoch_precision = 0\n",
        "    epoch_recall = 0\n",
        "    epoch_f1 = 0\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    # Por cada batch del iterador de la época:\n",
        "    for batch in iterator:\n",
        "\n",
        "        # Extraemos el texto y los tags del batch que estamos procesado\n",
        "        text = batch.text\n",
        "        tags = batch.nertags\n",
        "\n",
        "        # Reiniciamos los gradientes calculados en la iteración anterior\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        #text = [sent len, batch size]\n",
        "\n",
        "        # Predecimos los tags del texto del batch.\n",
        "        predictions = model(text)\n",
        "\n",
        "        #predictions = [sent len, batch size, output dim]\n",
        "        #tags = [sent len, batch size]\n",
        "\n",
        "        # Reordenamos los datos para calcular la loss\n",
        "        predictions = predictions.view(-1, predictions.shape[-1])\n",
        "        tags = tags.view(-1)\n",
        "\n",
        "        #predictions = [sent len * batch size, output dim]\n",
        "\n",
        "\n",
        "\n",
        "        # Calculamos el Cross Entropy de las predicciones con respecto a las etiquetas reales\n",
        "        loss = criterion(predictions, tags)\n",
        "        \n",
        "        # Calculamos el accuracy\n",
        "        precision, recall, f1 = calculate_metrics(predictions, tags)\n",
        "\n",
        "        # Calculamos los gradientes\n",
        "        loss.backward()\n",
        "\n",
        "        # Actualizamos los parámetros de la red\n",
        "        optimizer.step()\n",
        "\n",
        "        # Actualizamos el loss y las métricas\n",
        "        epoch_loss += loss.item()\n",
        "        epoch_precision += precision\n",
        "        epoch_recall += recall\n",
        "        epoch_f1 += f1\n",
        "\n",
        "    return epoch_loss / len(iterator), epoch_precision / len(\n",
        "        iterator), epoch_recall / len(iterator), epoch_f1 / len(iterator)\n",
        "\n",
        "def evaluate(model, iterator, criterion):\n",
        "\n",
        "    epoch_loss = 0\n",
        "    epoch_precision = 0\n",
        "    epoch_recall = 0\n",
        "    epoch_f1 = 0\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    # Indicamos que ahora no guardaremos los gradientes\n",
        "    with torch.no_grad():\n",
        "        # Por cada batch\n",
        "        for batch in iterator:\n",
        "\n",
        "            text = batch.text\n",
        "            tags = batch.nertags\n",
        "\n",
        "            # Predecimos\n",
        "            predictions = model(text)\n",
        "\n",
        "            predictions = predictions.view(-1, predictions.shape[-1])\n",
        "            tags = tags.view(-1)\n",
        "\n",
        "            # Calculamos el Cross Entropy de las predicciones con respecto a las etiquetas reales\n",
        "            loss = criterion(predictions, tags)\n",
        "\n",
        "            # Calculamos las métricas\n",
        "            precision, recall, f1 = calculate_metrics(predictions, tags)\n",
        "\n",
        "            # Actualizamos el loss y las métricas\n",
        "            epoch_loss += loss.item()\n",
        "            epoch_precision += precision\n",
        "            epoch_recall += recall\n",
        "            epoch_f1 += f1\n",
        "\n",
        "    return epoch_loss / len(iterator), epoch_precision / len(\n",
        "        iterator), epoch_recall / len(iterator), epoch_f1 / len(iterator)"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Modelo seleccionado"
      ],
      "metadata": {
        "id": "Gj5LrMQYQXNP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "results_exp = {}\n",
        "for index_model in range(3):\n",
        "  model = [model_1, model_2, model_3][index_model]\n",
        "  model_name = [model_name_1, model_name_2, model_name_3][index_model]\n",
        "  criterion = [criterion_1, criterion_2, criterion_3][index_model]\n",
        "  n_epochs = [n_epochs_1, n_epochs_2, n_epochs_3][index_model]\n",
        "\n",
        "  train_iterator_ = [train_iterator, train_iterator_masked, train_iterator_wo][index_model]\n",
        "  valid_iterator_ = [valid_iterator, valid_iterator_masked, valid_iterator_wo][index_model]\n",
        "\n",
        "\n",
        "  def init_weights(m):\n",
        "      # Inicializamos los pesos como aleatorios\n",
        "      for name, param in m.named_parameters():\n",
        "          nn.init.normal_(param.data, mean=0, std=0.1) \n",
        "          \n",
        "      # Seteamos como 0 los embeddings de UNK y PAD.\n",
        "      model.embedding.weight.data[UNK_IDX] = torch.zeros(EMBEDDING_DIM)\n",
        "      model.embedding.weight.data[PAD_IDX] = torch.zeros(EMBEDDING_DIM)\n",
        "\n",
        "  model.apply(init_weights)\n",
        "\n",
        "  print(f'[{model_name}] El modelo actual tiene {count_parameters(model):,} parámetros entrenables.')\n",
        "\n",
        "  # Optimizador\n",
        "  optimizer = optim.Adam(model.parameters())\n",
        "\n",
        "  # Enviamos el modelo y la loss a cuda (en el caso en que esté disponible)\n",
        "  model = model.to(device)\n",
        "  criterion = criterion.to(device)\n",
        "\n",
        "  best_valid_loss = float('inf')\n",
        "\n",
        "  for epoch in range(n_epochs):\n",
        "\n",
        "      start_time = time.time()\n",
        "\n",
        "      # Recuerdo: train_iterator y valid_iterator contienen el dataset dividido en batches.\n",
        "\n",
        "      # Entrenar\n",
        "      train_loss, train_precision, train_recall, train_f1 = train(\n",
        "          model, train_iterator_, optimizer, criterion)\n",
        "\n",
        "      # Evaluar (valid = validación)\n",
        "      valid_loss, valid_precision, valid_recall, valid_f1 = evaluate(\n",
        "          model, valid_iterator_, criterion)\n",
        "\n",
        "      end_time = time.time()\n",
        "\n",
        "      epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
        "\n",
        "      # Si obtuvimos mejores resultados, guardamos este modelo en el almacenamiento (para poder cargarlo luego)\n",
        "      # Si detienen el entrenamiento prematuramente, pueden cargar el modelo en el siguiente recuadro de código.\n",
        "      if valid_loss < best_valid_loss:\n",
        "          best_valid_loss = valid_loss\n",
        "          torch.save(model.state_dict(), '{}.pt'.format(model_name))\n",
        "      # Si ya no mejoramos el loss de validación, terminamos de entrenar.\n",
        "\n",
        "      print(f'[{model_name}] Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
        "      print(\n",
        "          f'[{model_name}] \\tTrain Loss: {train_loss:.3f} | Train f1: {train_f1:.2f} | Train precision: {train_precision:.2f} | Train recall: {train_recall:.2f}'\n",
        "      )\n",
        "      print(\n",
        "          f'[{model_name}] \\t Val. Loss: {valid_loss:.3f} |  Val. f1: {valid_f1:.2f} |  Val. precision: {valid_precision:.2f} | Val. recall: {valid_recall:.2f}'\n",
        "      )\n",
        "\n",
        "  # cargar el mejor modelo entrenado.\n",
        "  model.load_state_dict(torch.load('{}.pt'.format(model_name)))\n",
        "\n",
        "  # Limpiar ram de cuda\n",
        "  torch.cuda.empty_cache()\n",
        "\n",
        "  valid_loss, valid_precision, valid_recall, valid_f1 = evaluate(\n",
        "      model, valid_iterator_, criterion)\n",
        "\n",
        "  print(\n",
        "      f'[{model_name}] Val. Loss: {valid_loss:.3f} |  Val. f1: {valid_f1:.2f} | Val. precision: {valid_precision:.2f} | Val. recall: {valid_recall:.2f}'\n",
        "  )\n",
        "  results_exp[model_name] = [valid_loss, valid_f1, valid_precision, valid_recall]"
      ],
      "metadata": {
        "id": "zhQrgONayRId",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "094a591f-ee48-4128-85fa-fadba66ecac0"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[baseline] El modelo actual tiene 3,952,900 parámetros entrenables.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: <pad> seems not to be NE tag.\n",
            "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
            "/usr/local/lib/python3.7/dist-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[baseline] Epoch: 01 | Epoch Time: 0m 9s\n",
            "[baseline] \tTrain Loss: 0.934 | Train f1: 0.30 | Train precision: 0.46 | Train recall: 0.24\n",
            "[baseline] \t Val. Loss: 0.594 |  Val. f1: 0.56 |  Val. precision: 0.73 | Val. recall: 0.46\n",
            "[baseline] Epoch: 02 | Epoch Time: 0m 6s\n",
            "[baseline] \tTrain Loss: 0.522 | Train f1: 0.63 | Train precision: 0.70 | Train recall: 0.57\n",
            "[baseline] \t Val. Loss: 0.494 |  Val. f1: 0.64 |  Val. precision: 0.70 | Val. recall: 0.60\n",
            "[baseline] Epoch: 03 | Epoch Time: 0m 6s\n",
            "[baseline] \tTrain Loss: 0.391 | Train f1: 0.72 | Train precision: 0.76 | Train recall: 0.69\n",
            "[baseline] \t Val. Loss: 0.420 |  Val. f1: 0.70 |  Val. precision: 0.72 | Val. recall: 0.70\n",
            "[baseline] Epoch: 04 | Epoch Time: 0m 6s\n",
            "[baseline] \tTrain Loss: 0.316 | Train f1: 0.77 | Train precision: 0.79 | Train recall: 0.76\n",
            "[baseline] \t Val. Loss: 0.393 |  Val. f1: 0.72 |  Val. precision: 0.75 | Val. recall: 0.70\n",
            "[baseline] Epoch: 05 | Epoch Time: 0m 6s\n",
            "[baseline] \tTrain Loss: 0.266 | Train f1: 0.81 | Train precision: 0.81 | Train recall: 0.80\n",
            "[baseline] \t Val. Loss: 0.404 |  Val. f1: 0.73 |  Val. precision: 0.76 | Val. recall: 0.71\n",
            "[baseline] Epoch: 06 | Epoch Time: 0m 6s\n",
            "[baseline] \tTrain Loss: 0.234 | Train f1: 0.83 | Train precision: 0.84 | Train recall: 0.83\n",
            "[baseline] \t Val. Loss: 0.402 |  Val. f1: 0.74 |  Val. precision: 0.76 | Val. recall: 0.73\n",
            "[baseline] Epoch: 07 | Epoch Time: 0m 6s\n",
            "[baseline] \tTrain Loss: 0.207 | Train f1: 0.85 | Train precision: 0.85 | Train recall: 0.85\n",
            "[baseline] \t Val. Loss: 0.414 |  Val. f1: 0.74 |  Val. precision: 0.76 | Val. recall: 0.72\n",
            "[baseline] Epoch: 08 | Epoch Time: 0m 6s\n",
            "[baseline] \tTrain Loss: 0.188 | Train f1: 0.87 | Train precision: 0.87 | Train recall: 0.87\n",
            "[baseline] \t Val. Loss: 0.410 |  Val. f1: 0.74 |  Val. precision: 0.75 | Val. recall: 0.73\n",
            "[baseline] Epoch: 09 | Epoch Time: 0m 7s\n",
            "[baseline] \tTrain Loss: 0.171 | Train f1: 0.88 | Train precision: 0.88 | Train recall: 0.88\n",
            "[baseline] \t Val. Loss: 0.444 |  Val. f1: 0.75 |  Val. precision: 0.76 | Val. recall: 0.74\n",
            "[baseline] Epoch: 10 | Epoch Time: 0m 6s\n",
            "[baseline] \tTrain Loss: 0.157 | Train f1: 0.89 | Train precision: 0.89 | Train recall: 0.89\n",
            "[baseline] \t Val. Loss: 0.468 |  Val. f1: 0.74 |  Val. precision: 0.72 | Val. recall: 0.75\n",
            "[baseline] Val. Loss: 0.393 |  Val. f1: 0.72 | Val. precision: 0.75 | Val. recall: 0.70\n",
            "[masked] El modelo actual tiene 3,828,300 parámetros entrenables.\n",
            "[masked] Epoch: 01 | Epoch Time: 0m 6s\n",
            "[masked] \tTrain Loss: 0.982 | Train f1: 0.21 | Train precision: 0.41 | Train recall: 0.16\n",
            "[masked] \t Val. Loss: 0.618 |  Val. f1: 0.50 |  Val. precision: 0.70 | Val. recall: 0.39\n",
            "[masked] Epoch: 02 | Epoch Time: 0m 6s\n",
            "[masked] \tTrain Loss: 0.566 | Train f1: 0.59 | Train precision: 0.68 | Train recall: 0.53\n",
            "[masked] \t Val. Loss: 0.516 |  Val. f1: 0.62 |  Val. precision: 0.70 | Val. recall: 0.57\n",
            "[masked] Epoch: 03 | Epoch Time: 0m 6s\n",
            "[masked] \tTrain Loss: 0.436 | Train f1: 0.68 | Train precision: 0.73 | Train recall: 0.64\n",
            "[masked] \t Val. Loss: 0.468 |  Val. f1: 0.67 |  Val. precision: 0.69 | Val. recall: 0.65\n",
            "[masked] Epoch: 04 | Epoch Time: 0m 6s\n",
            "[masked] \tTrain Loss: 0.367 | Train f1: 0.73 | Train precision: 0.76 | Train recall: 0.71\n",
            "[masked] \t Val. Loss: 0.432 |  Val. f1: 0.70 |  Val. precision: 0.74 | Val. recall: 0.66\n",
            "[masked] Epoch: 05 | Epoch Time: 0m 6s\n",
            "[masked] \tTrain Loss: 0.317 | Train f1: 0.77 | Train precision: 0.79 | Train recall: 0.76\n",
            "[masked] \t Val. Loss: 0.430 |  Val. f1: 0.72 |  Val. precision: 0.74 | Val. recall: 0.70\n",
            "[masked] Epoch: 06 | Epoch Time: 0m 6s\n",
            "[masked] \tTrain Loss: 0.281 | Train f1: 0.80 | Train precision: 0.81 | Train recall: 0.79\n",
            "[masked] \t Val. Loss: 0.422 |  Val. f1: 0.72 |  Val. precision: 0.72 | Val. recall: 0.72\n",
            "[masked] Epoch: 07 | Epoch Time: 0m 6s\n",
            "[masked] \tTrain Loss: 0.249 | Train f1: 0.82 | Train precision: 0.83 | Train recall: 0.81\n",
            "[masked] \t Val. Loss: 0.429 |  Val. f1: 0.73 |  Val. precision: 0.73 | Val. recall: 0.72\n",
            "[masked] Epoch: 08 | Epoch Time: 0m 6s\n",
            "[masked] \tTrain Loss: 0.228 | Train f1: 0.84 | Train precision: 0.85 | Train recall: 0.83\n",
            "[masked] \t Val. Loss: 0.425 |  Val. f1: 0.72 |  Val. precision: 0.72 | Val. recall: 0.73\n",
            "[masked] Epoch: 09 | Epoch Time: 0m 6s\n",
            "[masked] \tTrain Loss: 0.208 | Train f1: 0.85 | Train precision: 0.86 | Train recall: 0.85\n",
            "[masked] \t Val. Loss: 0.423 |  Val. f1: 0.73 |  Val. precision: 0.76 | Val. recall: 0.72\n",
            "[masked] Epoch: 10 | Epoch Time: 0m 7s\n",
            "[masked] \tTrain Loss: 0.193 | Train f1: 0.86 | Train precision: 0.87 | Train recall: 0.86\n",
            "[masked] \t Val. Loss: 0.453 |  Val. f1: 0.73 |  Val. precision: 0.73 | Val. recall: 0.74\n",
            "[masked] Val. Loss: 0.422 |  Val. f1: 0.72 | Val. precision: 0.72 | Val. recall: 0.72\n",
            "[wo] El modelo actual tiene 3,828,100 parámetros entrenables.\n",
            "[wo] Epoch: 01 | Epoch Time: 0m 12s\n",
            "[wo] \tTrain Loss: 2.616 | Train f1: 0.02 | Train precision: 0.01 | Train recall: 0.07\n",
            "[wo] \t Val. Loss: 2.491 |  Val. f1: 0.14 |  Val. precision: 0.10 | Val. recall: 0.25\n",
            "[wo] Epoch: 02 | Epoch Time: 0m 11s\n",
            "[wo] \tTrain Loss: 2.472 | Train f1: 0.02 | Train precision: 0.01 | Train recall: 0.08\n",
            "[wo] \t Val. Loss: 2.371 |  Val. f1: 0.14 |  Val. precision: 0.14 | Val. recall: 0.14\n",
            "[wo] Epoch: 03 | Epoch Time: 0m 11s\n",
            "[wo] \tTrain Loss: 2.338 | Train f1: 0.02 | Train precision: 0.02 | Train recall: 0.06\n",
            "[wo] \t Val. Loss: 2.244 |  Val. f1: 0.04 |  Val. precision: 0.11 | Val. recall: 0.02\n",
            "[wo] Epoch: 04 | Epoch Time: 0m 10s\n",
            "[wo] \tTrain Loss: 2.197 | Train f1: 0.02 | Train precision: 0.02 | Train recall: 0.03\n",
            "[wo] \t Val. Loss: 2.103 |  Val. f1: 0.01 |  Val. precision: 0.10 | Val. recall: 0.00\n",
            "[wo] Epoch: 05 | Epoch Time: 0m 10s\n",
            "[wo] \tTrain Loss: 2.043 | Train f1: 0.01 | Train precision: 0.02 | Train recall: 0.01\n",
            "[wo] \t Val. Loss: 1.944 |  Val. f1: 0.00 |  Val. precision: 0.00 | Val. recall: 0.00\n",
            "[wo] Epoch: 06 | Epoch Time: 0m 10s\n",
            "[wo] \tTrain Loss: 1.871 | Train f1: 0.01 | Train precision: 0.02 | Train recall: 0.00\n",
            "[wo] \t Val. Loss: 1.773 |  Val. f1: 0.00 |  Val. precision: 0.00 | Val. recall: 0.00\n",
            "[wo] Epoch: 07 | Epoch Time: 0m 10s\n",
            "[wo] \tTrain Loss: 1.693 | Train f1: 0.00 | Train precision: 0.02 | Train recall: 0.00\n",
            "[wo] \t Val. Loss: 1.612 |  Val. f1: 0.00 |  Val. precision: 0.00 | Val. recall: 0.00\n",
            "[wo] Epoch: 08 | Epoch Time: 0m 10s\n",
            "[wo] \tTrain Loss: 1.540 | Train f1: 0.00 | Train precision: 0.01 | Train recall: 0.00\n",
            "[wo] \t Val. Loss: 1.491 |  Val. f1: 0.00 |  Val. precision: 0.00 | Val. recall: 0.00\n",
            "[wo] Epoch: 09 | Epoch Time: 0m 10s\n",
            "[wo] \tTrain Loss: 1.451 | Train f1: 0.00 | Train precision: 0.03 | Train recall: 0.00\n",
            "[wo] \t Val. Loss: 1.424 |  Val. f1: 0.00 |  Val. precision: 0.00 | Val. recall: 0.00\n",
            "[wo] Epoch: 10 | Epoch Time: 0m 10s\n",
            "[wo] \tTrain Loss: 1.422 | Train f1: 0.00 | Train precision: 0.05 | Train recall: 0.00\n",
            "[wo] \t Val. Loss: 1.388 |  Val. f1: 0.00 |  Val. precision: 0.00 | Val. recall: 0.00\n",
            "[wo] Val. Loss: 1.388 |  Val. f1: 0.00 | Val. precision: 0.00 | Val. recall: 0.00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Normal:\n",
        "\n",
        "[baseline] Val. Loss: 0.394 |  Val. f1: 0.74 | Val. precision: 0.76 | Val. recall: 0.72\n",
        "\n",
        "Masked:\n",
        "\n",
        "[masked] Val. Loss: 0.408 |  Val. f1: 0.73 | Val. precision: 0.72 | Val. recall: 0.74\n",
        "\n",
        "Without:\n",
        "\n",
        "[wo] Val. Loss: 1.351 |  Val. f1: 0.00 | Val. precision: 0.00 | Val. recall: 0.00"
      ],
      "metadata": {
        "id": "rIcCARtWzuE6"
      }
    }
  ]
}