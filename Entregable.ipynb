{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3559fead",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Competencia 2 NLP : Reconocimiento de entidades (NER) en Espa√±ol\n",
    "\n",
    "**Integrantes:** Alexander Cuevas - Jorge Guti√©rrez - Benjam√≠n Mellado - Felipe Urrutia\n",
    "\n",
    "**Usuario del equipo en CodaLab:** *teamGalactico üåå*\n",
    "\n",
    "**Fecha l√≠mite de entrega üìÜ:** 29 de Junio a las 23:59 hrs.\n",
    "\n",
    "**Tiempo estimado de dedicaci√≥n:** En curso... ‚è≤Ô∏è\n",
    "\n",
    "**Link competencia:** [CC6205 Assignment 2 - Named Entity Recognition (NER) in Spanish](https://codalab.lisn.upsaclay.fr/competitions/5098?secret_key=09955d45-6210-4a35-a171-8050aa050855#learn_the_details)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2842ee35",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Introducci√≥n üè∑Ô∏è\n",
    "\n",
    "**REQUIREMENTS: Presentar brevemente el contexto, problema a resolver, incluyendo la formalizaci√≥n de la task (c√≥mo son los inputs y outputs del problema) y los desaf√≠os que ven al analizar el corpus entregado. (0.5 puntos)**\n",
    "\n",
    "El an√°lisis de documentos cl√≠nicos es una labor que permite identificar enfermedades, procedimientos, medicamentos, y partes del cuerpo, entre otras entidades relevantes. Adem√°s, se puede usar para determinar las relaciones entre cada una de estas. Con estos datos es posible elaborar estad√≠sticas para una poblaci√≥n y sus individuos. En particular, se pueden determinar factores de riesgo, priorizar pacientes de acuerdo a sus necesidades, establecer casos solucionables sin intervenci√≥n presencial, conocer antecedentes familiares, saber cuantas personas sufren m√°s de una enfermedad, cuantas consumen m√°s de un medicamento, etc.\n",
    "\n",
    "En Chile, el sistema p√∫blico de salud cubre al 75% de la poblaci√≥n. Bajo este sistema, las visitas a especialistas, que requieren la remisi√≥n de un m√©dico general, son manejadas a trav√©s de una lista de espera dividida para casos GES y NO GES. En 2017, hubo 1.661.826 personas esperando una cita con un especialista, con un promedio de espera por sobre los 400 d√≠as. Por este motivo, se hace necesario desarrollar sistemas automatizados que analicen la lista NO GES, para mejorar el manejo de pacientes. En particular, existe una falta de trabajos realizados en el √°rea de NER en Espa√±ol y a√∫n m√°s en el contexto cl√≠nico.\n",
    "\n",
    "Bajo tal contexto, en este trabajo utilizaremos el Corpus de Listas de Espera propuesto por [1]. Este corpus est√° constituido por derivaciones de diversas consultas de la lista de espera NO GES de los hospitales p√∫blicos chilenos. Por su naturaleza, cada una de estas derivaciones es un dato no-estructurado escrito en lenguaje natural. Entre los resultados principales de [1] est√° otorgar un Corpus anotado usando texto cl√≠nico en espa√±ol.\n",
    "\n",
    "La tarea principal propuesta por [1] es un reconocimiento de entidades nombradas. Tambi√©n conocida como NER, por sus siglas en ingles. Dicha tarea es una de las m√°s populares en NLP. La cual consiste en identificar autom√°ticamente piezas esenciales de information (entidades) en el texto. Tradicionalmente, la categor√≠a de entidades a identificar eran nombres de personas, lugares y organizaciones. Actualmente, los tipos de categor√≠as se han expandido a diversos dominios del conocimiento. En este trabajo, las entidades que estudiaremos viven en el contexto de texto cl√≠nico.\n",
    "\n",
    "[1] utiliza el esquema Unified Medical Language System (UMLS) para las categor√≠as de entidades. Originalmente, [1] genera un Corpus con siete tipos de entidades:\n",
    "\n",
    "    (1) Finding, (2) Procedure, (3) Family Member, (4) Disease, (5) Body Part, (6) Medication y (7) Abbreviation. \n",
    "    \n",
    "Sin embargo, en este trabajo solo estudiaremos cinco de ellas. Todas excepto (1) y (7). Las anotaciones siguen el esquema IOB2, estandar en NER. Esquema definido como sigue: \n",
    "\n",
    ">*Esquema IOB2*. Consiste en identificar cada token con una etiqueta que indica si pertenece o no a una entidad. Si un token **no** es parte de una entidad, se indican con $\\texttt{O}$ (por la inicial en Ingles de *Fuera*). Sino, este necesariamente es un token de una entidad. Para indicar si el token es el inicio de una entidad, se utiliza el prefijo $\\texttt{B-}$ (por la inicial en Ingles de *Comienzo*). En cambio, si el token es la continuacion de una entidad, se utiliza el prefijo $\\texttt{I-}$ (por la inicial en Ingles de *Dentro*). Un ejemplo de este esquema, es el siguiente: \n",
    "\n",
    "\\begin{align*}\n",
    "&\\texttt{PRESENTA}& &\\texttt{FRACTURA}& &\\texttt{CORONARIA}& &\\texttt{COMPLICADA}& &\\texttt{EN}& &\\quad\\text{(Texto)}\\\\\n",
    "&\\texttt{O}& &\\texttt{B-Disease}& &\\texttt{I-Disease}& &\\texttt{I-Disease}& &\\texttt{O}& &\\quad\\text{(IOB2)}\n",
    "\\end{align*}\n",
    "\n",
    "Tal como se puede observar del ejemplo anterior, si se sigue el esquema de anotamiento IOB2 para la tarea de NER, el problema de reconocimiento de entidades nombradas se transforma en un problema de Sequence-Labeling. Tipo de problema que hemos discutido durante las √∫ltimas unidades del curso. Entre los modelos estudiados estan HMM, CRF, MEMMS, CNN y RNN. En este trabajo, nos focalizaremos en solo modelos neuronales dentro de la familia de las Redes Neuronales Recurrentes (RNN, por sus siglas en Ingles).\n",
    "\n",
    "Ahora bien, el conjunto de datos vienen entregados en una forma est√°ndar para la tarea de NER, denominado ConLL. Este consiste en un archivo de texto con dos columnas y saltos de l√≠nea que separan cada secuencia. La primera columna contiene los tokens de la secuencia. Mientras que la segunda columna, contiene las etiquetas IOB2 asociadas a cada token de la secuencia. Por otro lado, los datos vienen previamente separados en conjuntos de Entrenamiento, Desarrollo y Prueba. Estos conjuntos tienen tama√±os X, Y y Z, resp. Se espera que el output siga el mismo formato, y resulte de modificar el conjunto de Prueba que contiene tokens con todas las entidades inicializadas en $\\texttt{O}$.\n",
    "\n",
    "Adem√°s, la proporci√≥n de etiquetas IOB2 son mayoritariamente $\\texttt{O}$. Con una proporci√≥n total de A, B y C, en cada conjunto resp. El segundo tipo de etiqueta IOB2 m√°s frecuente es D, con proporciones E, F y G. Mientras que la etiqueta IOB2 menos frecuente es H, con proporciones I, J, K. Esta diferencia considerable de proporciones de las etiquetas, se denomina desbalance de datos.\n",
    "\n",
    "Entre las dificultades encontradas para esta tarea est√°n: (1) Desbalance de etiquetas IOB2, (2) [alguna otra encontrada del an√°lisis exploratorio de datos] y (3) NER en contexto de texto cl√≠nico. Consideramos que dichas dificultades son desafiantes de resolver. Es por esto que es de nuestro inter√©s lograr resolver satisfactoriamente esta tarea. En las secciones que siguen se encuentra detalladamente nuestra soluci√≥n.\n",
    "\n",
    "**Referencias**\n",
    "\n",
    "[1] B√°ez, P., Villena, F., Rojas, M., Dur√°n, M., & Dunstan, J. (2020, November). The Chilean Waiting List Corpus: a new resource for clinical named entity recognition in Spanish. In Proceedings of the 3rd clinical natural language processing workshop (pp. 291-300). https://aclanthology.org/2020.clinicalnlp-1.32/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a2812ce",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Modelos ü§ñ\n",
    "\n",
    "**REQUIREMENTS: Describir brevemente los modelos, m√©todos e hiperpar√°metros utilizados. (1.0 puntos)**\n",
    "### Baseline\n",
    "\n",
    "El primer modelo probado fue el entregado como baseline para la competencia. Este consiste en una red neuronal no bidireccional con capa de embedding, tres capas LSTM, y una capa de salida. Utiliza dropout en el entrenamiento. Como hiperpar√°metros, se fija la dimensi√≥n de los embeddings a 200, y la de las tres capas ocultas a 128. Adem√°s, la cantidad de √©pocas de entrenamiento se fija en 10. Finalmente, se define la funci√≥n de p√©rdida como la entrop√≠a cruzada entre el input y los datos objetivo.\n",
    "\n",
    "### Sin stopwords\n",
    "\n",
    "### Con embeddings cl√≠nicos del lenguaje espa√±ol\n",
    "\n",
    "### Con embeddings generales del lenguaje espa√±ol\n",
    "En adici√≥n a los embeddings cl√≠nicos, se realizaron pruebas con embeddings generales del lenguaje espa√±ol, en particular, cinco conjuntos de embeddings disponibles en la p√°gina [Spanish Word Embeddings](https://github.com/dccuchile/spanish-word-embeddings) (consultado el 11 de junio de 2022). Estos provienen de tres corpus de origen: Spanish Unannotated Corpora (SUC), Spanish Billion Word Corpus (SWBC), y Spanish Wikipedia. Y fueron procesados usando tres algoritmos: FastText, Glove, y Word2Vec.\n",
    "\n",
    "Para cada conjunto anterior se cre√≥ un modelo usando el baseline como base. Para todos estos se cambi√≥ el hiperpar√°metro de dimensi√≥n de embeddings a 300, y se mantuvo el resto igual a lo reportado anteriormente. Adem√°s, se modific√≥ la capa de embeddings seg√∫n correspondiera para utilizar los embeddings de la p√°gina en formato .vec.\n",
    "\n",
    "## Baseline mejorado\n",
    "Luego de probar los embeddings generales, se procedi√≥ a ajustar los par√°metros del modelo baseline. En particular, se ajust√≥ la dimensi√≥n de entrada del embedding usado, para que corresponda al que dio mejor resultado: Glove aplicado a SWBC (855380).\n",
    "\n",
    "Adicionalmente, se usaron capas LSTM de 256 dimensiones. Dropout igual a 0.5, y con bidireccionalidad. Para la predicci√≥n, se entren√≥ el modelo 3 veces.\n",
    "\n",
    "### Con datos balanceados"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "372b0939",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## M√©tricas de evaluaci√≥n üìä\n",
    "\n",
    "**REQUIREMENTS: Describir las m√©tricas utilizadas en la evaluaci√≥n indicando qu√© miden y cu√°l es su interpretaci√≥n en este problema en particular. (0.5 puntos)**\n",
    "\n",
    "\n",
    "En un problema de Sequence-labeling, la variable predicha por un modelo predictivo es un vector de etiquetas, donde cada etiqueta est√° asociada a un token en la secuencia del input. Una metrica usual para medir el desempe√±o de un modelo de este tipo determina TP, TN, FP y FN con cada etiqueta independientemente. Sin embargo, en la tarea de NER, el objetivo es reconocer entidades nombradas en el texto. Siendo las entidades, las predicciones importantes del modelo y no las etiquetas individuales en formato IOB2. Esto porque en el contexto cl√≠nico, varias condiciones y medicamentos tienen asociados una serie de adjetivos, biol√≥gicos y qu√≠micos, que afectan su significado e intensidad. Por su parte, los detalles de un paciente, y la especificaci√≥n de partes del cuerpo son similarmente relevantes. Por esto, una metrica usual no cuantifica el desempe√±o del modelo para el problema original. Para ser m√°s preciso, existen dos motivos, (1) las etiquetas importantes son aquellas asociadas a las entidades y (2) el modelo debe de reconocer exactamente tanto la ubicaci√≥n de la entidad como el tipo de entidad. Una metrica que si logra estas dos cosas, se denomina **metrica estricta**.\n",
    "\n",
    "<!-- \\begin{align*}\n",
    "&\\texttt{PRESENTA}& &\\texttt{FRACTURA}& &\\texttt{CORONARIA}& &\\texttt{COMPLICADA}& &\\texttt{EN}& &\\quad\\text{(Texto)}\\\\\n",
    "&\\texttt{O}& &\\texttt{B-Disease}& &\\texttt{I-Disease}& &\\texttt{I-Disease}& &\\texttt{O}& &\\quad\\text{(IOB2)}\\\\\n",
    "&0& &1& &2& &3& &4& &\\quad\\text{(Indice)}\n",
    "\\end{align*} -->\n",
    "\n",
    "En lo que sigue detallaremos las m√©tricas estrictas utilizadas. Para formalizar su definici√≥n, consideremos $D = (S, T)$ un conjunto de $n$ datos. Donde para cada secuencia $s \\in S$, $t^s \\in T$ corresponden a las etiquetas de $s$. Sin perdida de generalidad, supondremos que toda secuencia es de largo $m$. En la pr√°ctica, $m$ corresponde al largo m√°ximo de una secuencia dentro del conjunto de datos. Si una secuencia (y sus etiquetas) es de largo menor que $m$, se rellena con $\\texttt{<pad>}$. Consideremos $P$ a las etiquetas predichas, i.e., $p^s \\in P$ es la predicci√≥n de la secuencia $s \\in S$, comparable con las etiquetas reales $t^s \\in T$. Naturalmente, $s_i$ es el i-esimo token de la secuencia $s$. An√°logamente, $t^s_i$ para su etiqueta real y $p^s_i$ para la predicha. Se dir√° que $(l, i, j, c)$ es una entidad v√°lida de tipo $c$ en las etiquetas $l$, si $i\\leq j$, $l_i = \\texttt{B-}c$, para cada $k \\in \\{i+1, ..., j\\}$, $l_k = \\texttt{I-}c$ y, si $j<m$, entonces $l_{j+1} \\neq \\texttt{I-}c$. Denotaremos por $E(l, c)$ al conjunto de las entidades v√°lidas de tipo $c$ en las etiquetas $l$. Adicionalmente, denotaremos por $E(L)$ al conjunto de todas las entidades v√°lidas seg√∫n $L$, i.e.,  $$E(L) = \\bigcup \\{E(l,  c): l \\in L \\text{ y $c$ es un tipo de entidad}\\}$$\n",
    "  \n",
    "* *Precision*. Para un tipo de entidad fija (p.ej. Disease), la precision mide la proporci√≥n de veces que una entidad predicha de tal tipo coincide con la entidad real sobre el total de veces que el modelo predice una entidad con el tipo deseado. En efecto, para una entidad de tipo $c$,\n",
    "\n",
    "$$ \\texttt{Precision}(c) = \\frac{|\\{ (l, i, j, d) \\in E(P): d = c, \\exists s \\in S, l = p^s, (p^s, i, j, c) \\in E(t^s, c)\\}|}{|(l, i, j, d) \\in E(P): d=c |}$$\n",
    "\n",
    "* *Recall*. En el contexto cl√≠nico, el recall tambi√©n se conoce como sensibilidad, y es importante dado que frente a diagn√≥sticos y medicamentos sensibles, es preferible descartar un caso FN a no haberlo considerado en primer lugar. Para un tipo de entidad fija, el recall mide la proporci√≥n de veces que una entidad predicha de tal tipo coincide con la entidad real sobre el total de entidades del tipo deseado. En efecto, para una entidad de tipo $c$,\n",
    "\n",
    "$$ \\texttt{Recall}(c) = \\frac{|\\{ (l, i, j, d) \\in E(P): d = c, \\exists s\\in S, l=p^s, (p^s, i, j, c) \\in E(t^s, c)\\}|}{|(l, i, j, d) \\in E(T): d=c |}$$\n",
    "\n",
    "* *Micro F1 score*. **Recuerde hacer la distinci√≥n entre lo que ser√≠a una m√©trica de micro f1-score vs macro f1-score.** Para un tipo de entidad fija, la Precision y el Recall, entregan informaci√≥n distinta sobre las predicciones para dicho tipo de entidad. El primero considera falsos-positivos, mientras que el segundo considera falsos-negativos. Una forma de condensar esta informaci√≥n en una √∫nica m√©trica es con el promedio arm√≥nico entre Precision y Recall, denominado F1-score. Dicha metrica es condicional al tipo de entidad elegida. Para obtener un F1-score global (que considere todos los tipos de entidades), existen dos aproximaciones: (1) Micro F1-score y (2) Macro F1-score. En palabras simples, (1) es el Accuracy del modelo y (2) Un promedio de F1-score por tipo de entidad. Descartamos Macro F1-score dado que nuestros datos son desbalancedados. Por lo tanto, cada F1-score por clase no son estad√≠sticamente comparables. Nos quedaremos con Micro F1-score, ya que traduce globalmente el desempe√±o del modelo. Para computar esta metrica, se utiliza la t√©cnica estricta definida anteriormente. En efecto,\n",
    "\n",
    "$$ \\texttt{Micro-F1-score} = \\frac{|\\{ (l, i, j, d) \\in E(P): \\exists s\\in S, l=p^s, (p^s, i, j, d) \\in E(t^s, d)\\}|}{|S|}$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b571663",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Dise√±o experimental ‚úèÔ∏è\n",
    "\n",
    "**REQUIREMENTS: Esta es una de las secciones m√°s importantes del reporte. Deben describir minuciosamente los experimentos que realizar√°n en la siguiente secci√≥n. Describir las variables de control que manejar√°n, algunos ejemplos pueden ser: Los hiperpar√°metros de los modelos, tipo de embeddings utilizados, tipos de arquitecturas. Ser claros con el conjunto de hiperpar√°metros que probar√°n, la decisi√≥n en las funciones de optimizaci√≥n, funci√≥n de p√©rdida, regulaci√≥n, etc. B√°sicamente explicar qu√© es lo que veremos en la siguiente secci√≥n. (1 punto)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e029db2",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Con embeddings generales del lenguaje espa√±ol\n",
    "En adici√≥n a los embeddings cl√≠nicos, se realizaron pruebas con embeddings generales del lenguaje espa√±ol, en particular, los cinco conjuntos de embeddings disponibles en la p√°gina: [Spanish Word Embeddings](https://github.com/dccuchile/spanish-word-embeddings)\n",
    "### Hiperpar√°metros\n",
    "Estos son comunes para los cinco conjuntos.\n",
    "\n",
    "    Arquitectura: NER RNN\n",
    "    Dimensi√≥n de entrada: Largo del vocabulario de entrenamiento (17591)\n",
    "    Dimensi√≥n de salida: N√∫mero de clases (12)\n",
    "    Tipo de capas intermedias: LSTM\n",
    "    N√∫mero de capas intermedias: 3\n",
    "    Dimensi√≥n de capas intermedias: 256\n",
    "    Tama√±o de los batches: 16\n",
    "    Funci√≥n de p√©rdida: CrossEntropyLoss\n",
    "    Optimizador: Algoritmo ADAM\n",
    "    Bidireccional: S√≠\n",
    "    Dropout: 0.5\n",
    "    N√∫mero de epochs: 10\n",
    "\n",
    "#### Fast SWBC\n",
    "    Capa de embedding: Pre-entrenada (855380x300)\n",
    "\n",
    "#### Glove SWBC\n",
    "    Capa de embedding: Pre-entrenada (855380x300)\n",
    "\n",
    "#### Fast SUC\n",
    "    Capa de embedding: Pre-entrenada (1313423x300)\n",
    "\n",
    "#### W2V SWBC\n",
    "    Capa de embedding: Pre-entrenada (1000653x300)\n",
    "\n",
    "#### Wikipedia (espa√±ol)\n",
    "    Capa de embedding: Pre-entrenada (985667x300)\n",
    "\n",
    "## Baseline mejorado\n",
    "Luego de probar los embeddings generales y ver que no exist√≠a una mejora notable, se procedi√≥ a probar con los par√°metros ajustados del modelo baseline, pero sin los embeddings.\n",
    "### Hiperpar√°metros\n",
    "    Arquitectura: NER RNN\n",
    "    Dimensi√≥n de entrada: Largo del vocabulario de entrenamiento (17591)\n",
    "    Dimensi√≥n de salida: N√∫mero de clases (12)\n",
    "    Tipo de capas intermedias: LSTM\n",
    "    N√∫mero de capas intermedias: 3\n",
    "    Dimensi√≥n de capas intermedias: 256\n",
    "    Tama√±o de los batches: 16\n",
    "    Capa de embedding: Sin pre-entrenar (855380x300)\n",
    "    Funci√≥n de p√©rdida: CrossEntropyLoss\n",
    "    Optimizador: Algoritmo ADAM\n",
    "    Bidireccional: S√≠\n",
    "    Dropout: 0.5\n",
    "    N√∫mero de epochs: 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dbb5989",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Experimentos üß™\n",
    "\n",
    "**REQUIREMENTS: Reportar todos sus experimentos y c√≥digo en esta secci√≥n. Comparar los resultados obtenidos utilizando diferentes modelos. ¬°Es vital haber realizado varios experimentos para sacar una buena nota! (2.0 puntos)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eec00b05",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Configuraci√≥n com√∫n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c12991b8",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%pip install -U torchtext==0.10.0\n",
    "%pip install seqeval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a997bb74",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from seqeval.metrics import f1_score, precision_score, recall_score\n",
    "from torchtext import data, datasets, legacy\n",
    "\n",
    "# Garantizar reproducibilidad de los experimentos\n",
    "SEED = 1234\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "\n",
    "TEXT = legacy.data.Field(lower=False) \n",
    "NER_TAGS = legacy.data.Field(unk_token=None)\n",
    "fields = ((\"text\", TEXT), (\"nertags\", NER_TAGS))\n",
    "\n",
    "\n",
    "train_data, valid_data, test_data = legacy.datasets.SequenceTaggingDataset.splits(\n",
    "    path=\"./\",\n",
    "    train=\"train.txt\",\n",
    "    validation=\"dev.txt\",\n",
    "    test=\"test.txt\",\n",
    "    fields=fields,\n",
    "    encoding=\"utf-8\",\n",
    "    separator=\" \"\n",
    ")\n",
    "\n",
    "\n",
    "print(f\"Numero de ejemplos de entrenamiento: {len(train_data)}\")\n",
    "print(f\"N√∫mero de ejemplos de validaci√≥n: {len(valid_data)}\")\n",
    "print(f\"N√∫mero de ejemplos de test (competencia): {len(test_data)}\")\n",
    "\n",
    "\n",
    "TEXT.build_vocab(train_data)\n",
    "NER_TAGS.build_vocab(train_data)\n",
    "UNK_IDX = TEXT.vocab.stoi[TEXT.unk_token]\n",
    "PAD_IDX = TEXT.vocab.stoi[TEXT.pad_token]\n",
    "PAD_TAG_IDX = NER_TAGS.vocab.stoi[NER_TAGS.pad_token]\n",
    "O_TAG_IDX = NER_TAGS.vocab.stoi['O']\n",
    "\n",
    "\n",
    "def tag_percentage(tag_counts):\n",
    "    \n",
    "    total_count = sum([count for tag, count in tag_counts])\n",
    "    tag_counts_percentages = [(tag, count, count/total_count) for tag, count in tag_counts]\n",
    "  \n",
    "    return tag_counts_percentages\n",
    "\n",
    "\n",
    "print(\"Tag Ocurrencia Porcentaje\\n\")\n",
    "\n",
    "for tag, count, percent in tag_percentage(NER_TAGS.vocab.freqs.most_common()):\n",
    "    print(f\"{tag}\\t{count}\\t{percent*100:4.1f}%\")\n",
    "\n",
    "BATCH_SIZE = 16  # disminuir si hay problemas de ram.\n",
    "\n",
    "# Usar cuda si es que est√° disponible.\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using', device)\n",
    "\n",
    "# Dividir datos entre entrenamiento y test. Si van a hacer alg√∫n sort no puede ser sobre\n",
    "# el conjunto de testing ya que al hacer sus predicciones sobre el conjunto de test sin etiquetas\n",
    "# debe conservar el orden original para ser comparado con los golden_labels. \n",
    "\n",
    "train_iterator, valid_iterator, test_iterator = legacy.data.BucketIterator.splits(\n",
    "    (train_data, valid_data, test_data),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    device=device,\n",
    "    sort=False,\n",
    ")\n",
    "\n",
    "\n",
    "def calculate_metrics(preds, y_true, pad_idx=PAD_TAG_IDX, o_idx=O_TAG_IDX):\n",
    "    \"\"\"\n",
    "    Calcula precision, recall y f1 de cada batch.\n",
    "    \"\"\"\n",
    "\n",
    "    # Obtener el indice de la clase con probabilidad mayor. (clases)\n",
    "    y_pred = preds.argmax(dim=1, keepdim=True)\n",
    "\n",
    "    # filtramos <pad> para calcular los scores.\n",
    "    mask = [(y_true != pad_idx)]\n",
    "    y_pred = y_pred[mask]\n",
    "    y_true = y_true[mask]\n",
    "\n",
    "    # traemos a la cpu\n",
    "    y_pred = y_pred.view(-1).to('cpu').numpy()\n",
    "    y_true = y_true.to('cpu').numpy()\n",
    "    y_pred = [[NER_TAGS.vocab.itos[v] for v in y_pred]]\n",
    "    y_true = [[NER_TAGS.vocab.itos[v] for v in y_true]]\n",
    "    \n",
    "    # calcular scores\n",
    "    f1 = f1_score(y_true, y_pred, mode='strict')\n",
    "    precision = precision_score(y_true, y_pred, mode='strict')\n",
    "    recall = recall_score(y_true, y_pred, mode='strict')\n",
    "\n",
    "    return precision, recall, f1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbf7b169",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Embeddings generales del espa√±ol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c88d8833",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import gensim\n",
    "\n",
    "# Cargar data de los embeddings manualmente\n",
    "embedding_glove_swbc_dict = dict()\n",
    "embedding_fast_swbc_dict = dict()\n",
    "embedding_fast_suc_dict = dict()\n",
    "embedding_w2v_dict = dict()\n",
    "embedding_wiki_dict = dict()\n",
    "\n",
    "for par in [\n",
    "            [embedding_glove_swbc_dict, 'glove-sbwc.i25.vec'],\n",
    "            [embedding_fast_swbc_dict, 'fasttext-sbwc.3.6.e20.vec'],\n",
    "            [embedding_fast_suc_dict, 'embedding-l-model.vec'],\n",
    "            [embedding_w2v_dict, 'SBW-vectors-300-min5.vec']\n",
    "            [embedding_wiki_dict, 'wiki.es.vec']\n",
    "            ]:\n",
    "  with open(par[1], mode='rb') as f:\n",
    "    first = True\n",
    "    for line in f:\n",
    "      if first:\n",
    "        first = False\n",
    "        continue\n",
    "      values = line.split()\n",
    "      word = values[0]\n",
    "      coefs = np.asarray(values[1:], dtype='float32')\n",
    "      par[0][word] = coefs\n",
    "  print('Loaded %s word vectors.' % len(par[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "912d7df5",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Cargar embeddings con gensim\n",
    "temp = gensim.models.KeyedVectors.load_word2vec_format('glove-sbwc.i25.vec')\n",
    "embedding_glove_swbc = nn.Embedding.from_pretrained(torch.FloatTensor(temp.vectors))\n",
    "\n",
    "temp = gensim.models.KeyedVectors.load_word2vec_format('fasttext-sbwc.3.6.e20.vec')\n",
    "embedding_fast_swbc = nn.Embedding.from_pretrained(torch.FloatTensor(temp.vectors))\n",
    "\n",
    "temp = gensim.models.KeyedVectors.load_word2vec_format('embedding-l-model.vec')\n",
    "embedding_fast_suc = nn.Embedding.from_pretrained(torch.FloatTensor(temp.vectors))\n",
    "\n",
    "temp = gensim.models.KeyedVectors.load_word2vec_format('SBW-vectors-300-min5.vec')\n",
    "embedding_w2v = nn.Embedding.from_pretrained(torch.FloatTensor(temp.vectors))\n",
    "\n",
    "temp = gensim.models.KeyedVectors.load_word2vec_format('wiki.es.vec')\n",
    "embedding_wiki = nn.Embedding.from_pretrained(torch.FloatTensor(temp.vectors))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eead28f3",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Fast SUC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d603b4a2",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Definir la red\n",
    "class NER_RNN(nn.Module):\n",
    "    def __init__(self, \n",
    "                 input_dim, \n",
    "                 embedding_dim, \n",
    "                 hidden_dim, \n",
    "                 output_dim,\n",
    "                 n_layers, \n",
    "                 bidirectional, \n",
    "                 dropout):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        # Capa de embedding\n",
    "        self.embedding = embedding_fast_suc\n",
    "\n",
    "        # Capa LSTM\n",
    "        self.lstm = nn.LSTM(embedding_dim,\n",
    "                           hidden_dim,\n",
    "                           num_layers=n_layers,\n",
    "                           bidirectional=bidirectional, \n",
    "                           dropout = dropout if n_layers > 1 else 0)\n",
    "\n",
    "        # Capa de salida\n",
    "        self.fc = nn.Linear(hidden_dim * 2 if bidirectional else hidden_dim,\n",
    "                            output_dim)\n",
    "\n",
    "        # Dropout\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, text):\n",
    "        embedded = self.dropout(self.embedding(text))\n",
    "        outputs, (hidden, cell) = self.lstm(embedded)\n",
    "        predictions = self.fc(self.dropout(outputs))\n",
    "        return predictions\n",
    "\n",
    "\n",
    "# Tama√±o del vocabulario\n",
    "INPUT_DIM = len(TEXT.vocab)\n",
    "EMBEDDING_DIM = 300  # dimensi√≥n de los embeddings.\n",
    "HIDDEN_DIM = 256  # dimensi√≥n de la capas LSTM\n",
    "OUTPUT_DIM = len(NER_TAGS.vocab)  # n√∫mero de clases\n",
    "\n",
    "N_LAYERS = 3  # n√∫mero de capas.\n",
    "DROPOUT = 0.5\n",
    "BIDIRECTIONAL = True\n",
    "\n",
    "baseline_model = NER_RNN(INPUT_DIM, EMBEDDING_DIM, HIDDEN_DIM, OUTPUT_DIM,\n",
    "                         N_LAYERS, BIDIRECTIONAL, DROPOUT)\n",
    "\n",
    "baseline_model_name = 'baseline'  # nombre que tendr√° el modelo guardado...\n",
    "baseline_n_epochs = 10\n",
    "\n",
    "\n",
    "# Loss: Cross Entropy\n",
    "TAG_PAD_IDX = NER_TAGS.vocab.stoi[NER_TAGS.pad_token]\n",
    "baseline_criterion = nn.CrossEntropyLoss(ignore_index = TAG_PAD_IDX)\n",
    "\n",
    "\n",
    "model = baseline_model\n",
    "model_name = baseline_model_name\n",
    "criterion = baseline_criterion\n",
    "n_epochs = baseline_n_epochs\n",
    "\n",
    "\n",
    "def init_weights(m):\n",
    "    # Inicializamos los pesos como aleatorios\n",
    "    for name, param in m.named_parameters():\n",
    "        nn.init.normal_(param.data, mean=0, std=0.1) \n",
    "        \n",
    "    # Seteamos como 0 los embeddings de UNK y PAD.\n",
    "    model.embedding.weight.data[UNK_IDX] = torch.zeros(EMBEDDING_DIM)\n",
    "    model.embedding.weight.data[PAD_IDX] = torch.zeros(EMBEDDING_DIM)\n",
    "\n",
    "\n",
    "model.apply(init_weights)\n",
    "\n",
    "\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f'El modelo actual tiene {count_parameters(model):,} par√°metros entrenables.')\n",
    "\n",
    "\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "model = model.to(device)\n",
    "criterion = criterion.to(device)\n",
    "\n",
    "def train(model, iterator, optimizer, criterion):\n",
    "\n",
    "    epoch_loss = 0\n",
    "    epoch_precision = 0\n",
    "    epoch_recall = 0\n",
    "    epoch_f1 = 0\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    # Por cada batch del iterador de la √©poca:\n",
    "    for batch in iterator:\n",
    "\n",
    "        # Extraemos el texto y los tags del batch que estamos procesado\n",
    "        text = batch.text\n",
    "        tags = batch.nertags\n",
    "\n",
    "        # Reiniciamos los gradientes calculados en la iteraci√≥n anterior\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        #text = [sent len, batch size]\n",
    "\n",
    "        # Predecimos los tags del texto del batch.\n",
    "        predictions = model(text)\n",
    "\n",
    "        #predictions = [sent len, batch size, output dim]\n",
    "        #tags = [sent len, batch size]\n",
    "\n",
    "        # Reordenamos los datos para calcular la loss\n",
    "        predictions = predictions.view(-1, predictions.shape[-1])\n",
    "        tags = tags.view(-1)\n",
    "\n",
    "        #predictions = [sent len * batch size, output dim]\n",
    "\n",
    "\n",
    "\n",
    "        # Calculamos el Cross Entropy de las predicciones con respecto a las etiquetas reales\n",
    "        loss = criterion(predictions, tags)\n",
    "        \n",
    "        # Calculamos el accuracy\n",
    "        precision, recall, f1 = calculate_metrics(predictions, tags)\n",
    "\n",
    "        # Calculamos los gradientes\n",
    "        loss.backward()\n",
    "\n",
    "        # Actualizamos los par√°metros de la red\n",
    "        optimizer.step()\n",
    "\n",
    "        # Actualizamos el loss y las m√©tricas\n",
    "        epoch_loss += loss.item()\n",
    "        epoch_precision += precision\n",
    "        epoch_recall += recall\n",
    "        epoch_f1 += f1\n",
    "\n",
    "    return epoch_loss / len(iterator), epoch_precision / len(iterator), epoch_recall / len(iterator), epoch_f1 / len(iterator)\n",
    "\n",
    "\n",
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs\n",
    "\n",
    "\n",
    "def evaluate(model, iterator, criterion):\n",
    "\n",
    "    epoch_loss = 0\n",
    "    epoch_precision = 0\n",
    "    epoch_recall = 0\n",
    "    epoch_f1 = 0\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    # Indicamos que ahora no guardaremos los gradientes\n",
    "    with torch.no_grad():\n",
    "        # Por cada batch\n",
    "        for batch in iterator:\n",
    "\n",
    "            text = batch.text\n",
    "            tags = batch.nertags\n",
    "\n",
    "            # Predecimos\n",
    "            predictions = model(text)\n",
    "\n",
    "            predictions = predictions.view(-1, predictions.shape[-1])\n",
    "            tags = tags.view(-1)\n",
    "\n",
    "            # Calculamos el Cross Entropy de las predicciones con respecto a las etiquetas reales\n",
    "            loss = criterion(predictions, tags)\n",
    "\n",
    "            # Calculamos las m√©tricas\n",
    "            precision, recall, f1 = calculate_metrics(predictions, tags)\n",
    "\n",
    "            # Actualizamos el loss y las m√©tricas\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_precision += precision\n",
    "            epoch_recall += recall\n",
    "            epoch_f1 += f1\n",
    "\n",
    "    return epoch_loss / len(iterator), epoch_precision / len(\n",
    "        iterator), epoch_recall / len(iterator), epoch_f1 / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b369056b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "iK5lQqpviicf",
    "outputId": "0880b2a1-7386-4277-f863-afd343c8cd13",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01 | Epoch Time: 4m 1s\n",
      "\tTrain Loss: 0.935 | Train f1: 0.30 | Train precision: 0.53 | Train recall: 0.21\n",
      "\t Val. Loss: 0.759 |  Val. f1: 0.45 |  Val. precision: 0.70 | Val. recall: 0.34\n",
      "Epoch: 02 | Epoch Time: 4m 7s\n",
      "\tTrain Loss: 0.746 | Train f1: 0.46 | Train precision: 0.65 | Train recall: 0.36\n",
      "\t Val. Loss: 0.639 |  Val. f1: 0.51 |  Val. precision: 0.72 | Val. recall: 0.40\n",
      "Epoch: 03 | Epoch Time: 11m 35s\n",
      "\tTrain Loss: 0.662 | Train f1: 0.52 | Train precision: 0.69 | Train recall: 0.42\n",
      "\t Val. Loss: 0.586 |  Val. f1: 0.55 |  Val. precision: 0.74 | Val. recall: 0.45\n",
      "Epoch: 04 | Epoch Time: 3m 56s\n",
      "\tTrain Loss: 0.604 | Train f1: 0.56 | Train precision: 0.72 | Train recall: 0.47\n",
      "\t Val. Loss: 0.564 |  Val. f1: 0.59 |  Val. precision: 0.75 | Val. recall: 0.50\n",
      "Epoch: 05 | Epoch Time: 3m 57s\n",
      "\tTrain Loss: 0.566 | Train f1: 0.59 | Train precision: 0.73 | Train recall: 0.50\n",
      "\t Val. Loss: 0.546 |  Val. f1: 0.59 |  Val. precision: 0.76 | Val. recall: 0.49\n",
      "Epoch: 06 | Epoch Time: 3m 58s\n",
      "\tTrain Loss: 0.538 | Train f1: 0.61 | Train precision: 0.74 | Train recall: 0.52\n",
      "\t Val. Loss: 0.538 |  Val. f1: 0.60 |  Val. precision: 0.73 | Val. recall: 0.51\n",
      "Epoch: 07 | Epoch Time: 4m 2s\n",
      "\tTrain Loss: 0.502 | Train f1: 0.63 | Train precision: 0.75 | Train recall: 0.55\n",
      "\t Val. Loss: 0.520 |  Val. f1: 0.61 |  Val. precision: 0.70 | Val. recall: 0.55\n",
      "Epoch: 08 | Epoch Time: 4m 2s\n",
      "\tTrain Loss: 0.473 | Train f1: 0.66 | Train precision: 0.76 | Train recall: 0.58\n",
      "\t Val. Loss: 0.483 |  Val. f1: 0.64 |  Val. precision: 0.75 | Val. recall: 0.56\n",
      "Epoch: 09 | Epoch Time: 3m 59s\n",
      "\tTrain Loss: 0.451 | Train f1: 0.67 | Train precision: 0.77 | Train recall: 0.60\n",
      "\t Val. Loss: 0.484 |  Val. f1: 0.64 |  Val. precision: 0.73 | Val. recall: 0.59\n",
      "Epoch: 10 | Epoch Time: 4m 26s\n",
      "\tTrain Loss: 0.428 | Train f1: 0.69 | Train precision: 0.79 | Train recall: 0.62\n",
      "\t Val. Loss: 0.487 |  Val. f1: 0.64 |  Val. precision: 0.71 | Val. recall: 0.59\n"
     ]
    }
   ],
   "source": [
    "best_valid_loss = float('inf')\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Recuerdo: train_iterator y valid_iterator contienen el dataset dividido en batches.\n",
    "\n",
    "    # Entrenar\n",
    "    train_loss, train_precision, train_recall, train_f1 = train(\n",
    "        model, train_iterator, optimizer, criterion)\n",
    "\n",
    "    # Evaluar (valid = validaci√≥n)\n",
    "    valid_loss, valid_precision, valid_recall, valid_f1 = evaluate(\n",
    "        model, valid_iterator, criterion)\n",
    "\n",
    "    end_time = time.time()\n",
    "\n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "\n",
    "    # Si obtuvimos mejores resultados, guardamos este modelo en el almacenamiento (para poder cargarlo luego)\n",
    "    # Si detienen el entrenamiento prematuramente, pueden cargar el modelo en el siguiente recuadro de c√≥digo.\n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), '{}.pt'.format(model_name))\n",
    "    # Si ya no mejoramos el loss de validaci√≥n, terminamos de entrenar.\n",
    "\n",
    "    print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(\n",
    "        f'\\tTrain Loss: {train_loss:.3f} | Train f1: {train_f1:.2f} | Train precision: {train_precision:.2f} | Train recall: {train_recall:.2f}'\n",
    "    )\n",
    "    print(\n",
    "        f'\\t Val. Loss: {valid_loss:.3f} |  Val. f1: {valid_f1:.2f} |  Val. precision: {valid_precision:.2f} | Val. recall: {valid_recall:.2f}'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18496b3e",
   "metadata": {
    "id": "s0gVbP8yiicj",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val. Loss: 0.483 |  Val. f1: 0.64 | Val. precision: 0.75 | Val. recall: 0.56\n"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('{}.pt'.format(model_name)))\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "valid_loss, valid_precision, valid_recall, valid_f1 = evaluate(\n",
    "    model, valid_iterator, criterion)\n",
    "\n",
    "print(\n",
    "    f'Val. Loss: {valid_loss:.3f} |  Val. f1: {valid_f1:.2f} | Val. precision: {valid_precision:.2f} | Val. recall: {valid_recall:.2f}'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b69d8924",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Fast SWBC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d1e61f0",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Definir la red\n",
    "class NER_RNN(nn.Module):\n",
    "    def __init__(self, \n",
    "                 input_dim, \n",
    "                 embedding_dim, \n",
    "                 hidden_dim, \n",
    "                 output_dim,\n",
    "                 n_layers, \n",
    "                 bidirectional, \n",
    "                 dropout):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        # Capa de embedding\n",
    "        self.embedding = embedding_fast_swbc\n",
    "\n",
    "        # Capa LSTM\n",
    "        self.lstm = nn.LSTM(embedding_dim,\n",
    "                           hidden_dim,\n",
    "                           num_layers=n_layers,\n",
    "                           bidirectional=bidirectional, \n",
    "                           dropout = dropout if n_layers > 1 else 0)\n",
    "\n",
    "        # Capa de salida\n",
    "        self.fc = nn.Linear(hidden_dim * 2 if bidirectional else hidden_dim,\n",
    "                            output_dim)\n",
    "\n",
    "        # Dropout\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, text):\n",
    "        embedded = self.dropout(self.embedding(text))\n",
    "        outputs, (hidden, cell) = self.lstm(embedded)\n",
    "        predictions = self.fc(self.dropout(outputs))\n",
    "        return predictions\n",
    "\n",
    "\n",
    "# Tama√±o del vocabulario\n",
    "INPUT_DIM = len(TEXT.vocab)\n",
    "EMBEDDING_DIM = 300  # dimensi√≥n de los embeddings.\n",
    "HIDDEN_DIM = 256  # dimensi√≥n de la capas LSTM\n",
    "OUTPUT_DIM = len(NER_TAGS.vocab)  # n√∫mero de clases\n",
    "\n",
    "N_LAYERS = 3  # n√∫mero de capas.\n",
    "DROPOUT = 0.5\n",
    "BIDIRECTIONAL = True\n",
    "\n",
    "baseline_model = NER_RNN(INPUT_DIM, EMBEDDING_DIM, HIDDEN_DIM, OUTPUT_DIM,\n",
    "                         N_LAYERS, BIDIRECTIONAL, DROPOUT)\n",
    "\n",
    "baseline_model_name = 'baseline'  # nombre que tendr√° el modelo guardado...\n",
    "baseline_n_epochs = 10\n",
    "\n",
    "\n",
    "# Loss: Cross Entropy\n",
    "TAG_PAD_IDX = NER_TAGS.vocab.stoi[NER_TAGS.pad_token]\n",
    "baseline_criterion = nn.CrossEntropyLoss(ignore_index = TAG_PAD_IDX)\n",
    "\n",
    "\n",
    "model = baseline_model\n",
    "model_name = baseline_model_name\n",
    "criterion = baseline_criterion\n",
    "n_epochs = baseline_n_epochs\n",
    "\n",
    "\n",
    "def init_weights(m):\n",
    "    # Inicializamos los pesos como aleatorios\n",
    "    for name, param in m.named_parameters():\n",
    "        nn.init.normal_(param.data, mean=0, std=0.1) \n",
    "        \n",
    "    # Seteamos como 0 los embeddings de UNK y PAD.\n",
    "    model.embedding.weight.data[UNK_IDX] = torch.zeros(EMBEDDING_DIM)\n",
    "    model.embedding.weight.data[PAD_IDX] = torch.zeros(EMBEDDING_DIM)\n",
    "\n",
    "\n",
    "model.apply(init_weights)\n",
    "\n",
    "\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f'El modelo actual tiene {count_parameters(model):,} par√°metros entrenables.')\n",
    "\n",
    "\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "model = model.to(device)\n",
    "criterion = criterion.to(device)\n",
    "\n",
    "def train(model, iterator, optimizer, criterion):\n",
    "\n",
    "    epoch_loss = 0\n",
    "    epoch_precision = 0\n",
    "    epoch_recall = 0\n",
    "    epoch_f1 = 0\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    # Por cada batch del iterador de la √©poca:\n",
    "    for batch in iterator:\n",
    "\n",
    "        # Extraemos el texto y los tags del batch que estamos procesado\n",
    "        text = batch.text\n",
    "        tags = batch.nertags\n",
    "\n",
    "        # Reiniciamos los gradientes calculados en la iteraci√≥n anterior\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        #text = [sent len, batch size]\n",
    "\n",
    "        # Predecimos los tags del texto del batch.\n",
    "        predictions = model(text)\n",
    "\n",
    "        #predictions = [sent len, batch size, output dim]\n",
    "        #tags = [sent len, batch size]\n",
    "\n",
    "        # Reordenamos los datos para calcular la loss\n",
    "        predictions = predictions.view(-1, predictions.shape[-1])\n",
    "        tags = tags.view(-1)\n",
    "\n",
    "        #predictions = [sent len * batch size, output dim]\n",
    "\n",
    "\n",
    "\n",
    "        # Calculamos el Cross Entropy de las predicciones con respecto a las etiquetas reales\n",
    "        loss = criterion(predictions, tags)\n",
    "        \n",
    "        # Calculamos el accuracy\n",
    "        precision, recall, f1 = calculate_metrics(predictions, tags)\n",
    "\n",
    "        # Calculamos los gradientes\n",
    "        loss.backward()\n",
    "\n",
    "        # Actualizamos los par√°metros de la red\n",
    "        optimizer.step()\n",
    "\n",
    "        # Actualizamos el loss y las m√©tricas\n",
    "        epoch_loss += loss.item()\n",
    "        epoch_precision += precision\n",
    "        epoch_recall += recall\n",
    "        epoch_f1 += f1\n",
    "\n",
    "    return epoch_loss / len(iterator), epoch_precision / len(iterator), epoch_recall / len(iterator), epoch_f1 / len(iterator)\n",
    "\n",
    "\n",
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs\n",
    "\n",
    "\n",
    "def evaluate(model, iterator, criterion):\n",
    "\n",
    "    epoch_loss = 0\n",
    "    epoch_precision = 0\n",
    "    epoch_recall = 0\n",
    "    epoch_f1 = 0\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    # Indicamos que ahora no guardaremos los gradientes\n",
    "    with torch.no_grad():\n",
    "        # Por cada batch\n",
    "        for batch in iterator:\n",
    "\n",
    "            text = batch.text\n",
    "            tags = batch.nertags\n",
    "\n",
    "            # Predecimos\n",
    "            predictions = model(text)\n",
    "\n",
    "            predictions = predictions.view(-1, predictions.shape[-1])\n",
    "            tags = tags.view(-1)\n",
    "\n",
    "            # Calculamos el Cross Entropy de las predicciones con respecto a las etiquetas reales\n",
    "            loss = criterion(predictions, tags)\n",
    "\n",
    "            # Calculamos las m√©tricas\n",
    "            precision, recall, f1 = calculate_metrics(predictions, tags)\n",
    "\n",
    "            # Actualizamos el loss y las m√©tricas\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_precision += precision\n",
    "            epoch_recall += recall\n",
    "            epoch_f1 += f1\n",
    "\n",
    "    return epoch_loss / len(iterator), epoch_precision / len(\n",
    "        iterator), epoch_recall / len(iterator), epoch_f1 / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1f496fb",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "iK5lQqpviicf",
    "outputId": "0880b2a1-7386-4277-f863-afd343c8cd13",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\autofact\\Documents\\NLP-competition-2\\venv\\lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: <pad> seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "c:\\Users\\autofact\\Documents\\NLP-competition-2\\venv\\lib\\site-packages\\seqeval\\metrics\\v1.py:57: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01 | Epoch Time: 3m 59s\n",
      "\tTrain Loss: 0.944 | Train f1: 0.28 | Train precision: 0.50 | Train recall: 0.20\n",
      "\t Val. Loss: 0.767 |  Val. f1: 0.43 |  Val. precision: 0.69 | Val. recall: 0.32\n",
      "Epoch: 02 | Epoch Time: 3m 56s\n",
      "\tTrain Loss: 0.751 | Train f1: 0.46 | Train precision: 0.66 | Train recall: 0.36\n",
      "\t Val. Loss: 0.639 |  Val. f1: 0.50 |  Val. precision: 0.74 | Val. recall: 0.39\n",
      "Epoch: 03 | Epoch Time: 3m 57s\n",
      "\tTrain Loss: 0.674 | Train f1: 0.51 | Train precision: 0.69 | Train recall: 0.41\n",
      "\t Val. Loss: 0.605 |  Val. f1: 0.55 |  Val. precision: 0.70 | Val. recall: 0.45\n",
      "Epoch: 04 | Epoch Time: 4m 12s\n",
      "\tTrain Loss: 0.612 | Train f1: 0.56 | Train precision: 0.72 | Train recall: 0.46\n",
      "\t Val. Loss: 0.563 |  Val. f1: 0.58 |  Val. precision: 0.74 | Val. recall: 0.48\n",
      "Epoch: 05 | Epoch Time: 4m 13s\n",
      "\tTrain Loss: 0.566 | Train f1: 0.59 | Train precision: 0.73 | Train recall: 0.50\n",
      "\t Val. Loss: 0.532 |  Val. f1: 0.61 |  Val. precision: 0.74 | Val. recall: 0.54\n",
      "Epoch: 06 | Epoch Time: 4m 5s\n",
      "\tTrain Loss: 0.536 | Train f1: 0.61 | Train precision: 0.74 | Train recall: 0.52\n",
      "\t Val. Loss: 0.515 |  Val. f1: 0.62 |  Val. precision: 0.71 | Val. recall: 0.55\n",
      "Epoch: 07 | Epoch Time: 4m 16s\n",
      "\tTrain Loss: 0.506 | Train f1: 0.63 | Train precision: 0.75 | Train recall: 0.55\n",
      "\t Val. Loss: 0.500 |  Val. f1: 0.63 |  Val. precision: 0.72 | Val. recall: 0.56\n",
      "Epoch: 08 | Epoch Time: 4m 11s\n",
      "\tTrain Loss: 0.481 | Train f1: 0.65 | Train precision: 0.76 | Train recall: 0.57\n",
      "\t Val. Loss: 0.468 |  Val. f1: 0.65 |  Val. precision: 0.76 | Val. recall: 0.58\n",
      "Epoch: 09 | Epoch Time: 4m 14s\n",
      "\tTrain Loss: 0.452 | Train f1: 0.67 | Train precision: 0.77 | Train recall: 0.60\n",
      "\t Val. Loss: 0.460 |  Val. f1: 0.66 |  Val. precision: 0.73 | Val. recall: 0.61\n",
      "Epoch: 10 | Epoch Time: 4m 39s\n",
      "\tTrain Loss: 0.435 | Train f1: 0.69 | Train precision: 0.78 | Train recall: 0.61\n",
      "\t Val. Loss: 0.475 |  Val. f1: 0.66 |  Val. precision: 0.73 | Val. recall: 0.60\n"
     ]
    }
   ],
   "source": [
    "best_valid_loss = float('inf')\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Recuerdo: train_iterator y valid_iterator contienen el dataset dividido en batches.\n",
    "\n",
    "    # Entrenar\n",
    "    train_loss, train_precision, train_recall, train_f1 = train(\n",
    "        model, train_iterator, optimizer, criterion)\n",
    "\n",
    "    # Evaluar (valid = validaci√≥n)\n",
    "    valid_loss, valid_precision, valid_recall, valid_f1 = evaluate(\n",
    "        model, valid_iterator, criterion)\n",
    "\n",
    "    end_time = time.time()\n",
    "\n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "\n",
    "    # Si obtuvimos mejores resultados, guardamos este modelo en el almacenamiento (para poder cargarlo luego)\n",
    "    # Si detienen el entrenamiento prematuramente, pueden cargar el modelo en el siguiente recuadro de c√≥digo.\n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), '{}.pt'.format(model_name))\n",
    "    # Si ya no mejoramos el loss de validaci√≥n, terminamos de entrenar.\n",
    "\n",
    "    print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(\n",
    "        f'\\tTrain Loss: {train_loss:.3f} | Train f1: {train_f1:.2f} | Train precision: {train_precision:.2f} | Train recall: {train_recall:.2f}'\n",
    "    )\n",
    "    print(\n",
    "        f'\\t Val. Loss: {valid_loss:.3f} |  Val. f1: {valid_f1:.2f} |  Val. precision: {valid_precision:.2f} | Val. recall: {valid_recall:.2f}'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb748ef8",
   "metadata": {
    "id": "s0gVbP8yiicj",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val. Loss: 0.460 |  Val. f1: 0.66 | Val. precision: 0.73 | Val. recall: 0.61\n"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('{}.pt'.format(model_name)))\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "valid_loss, valid_precision, valid_recall, valid_f1 = evaluate(\n",
    "    model, valid_iterator, criterion)\n",
    "\n",
    "print(\n",
    "    f'Val. Loss: {valid_loss:.3f} |  Val. f1: {valid_f1:.2f} | Val. precision: {valid_precision:.2f} | Val. recall: {valid_recall:.2f}'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebbabd9f",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Glove SWBC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "062ff0b1",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Definir la red\n",
    "class NER_RNN(nn.Module):\n",
    "    def __init__(self, \n",
    "                 input_dim, \n",
    "                 embedding_dim, \n",
    "                 hidden_dim, \n",
    "                 output_dim,\n",
    "                 n_layers, \n",
    "                 bidirectional, \n",
    "                 dropout):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        # Capa de embedding\n",
    "        self.embedding = embedding_glove_swbc\n",
    "\n",
    "        # Capa LSTM\n",
    "        self.lstm = nn.LSTM(embedding_dim,\n",
    "                           hidden_dim,\n",
    "                           num_layers=n_layers,\n",
    "                           bidirectional=bidirectional, \n",
    "                           dropout = dropout if n_layers > 1 else 0)\n",
    "\n",
    "        # Capa de salida\n",
    "        self.fc = nn.Linear(hidden_dim * 2 if bidirectional else hidden_dim,\n",
    "                            output_dim)\n",
    "\n",
    "        # Dropout\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, text):\n",
    "        embedded = self.dropout(self.embedding(text))\n",
    "        outputs, (hidden, cell) = self.lstm(embedded)\n",
    "        predictions = self.fc(self.dropout(outputs))\n",
    "        return predictions\n",
    "\n",
    "\n",
    "# Tama√±o del vocabulario\n",
    "INPUT_DIM = len(TEXT.vocab)\n",
    "EMBEDDING_DIM = 300  # dimensi√≥n de los embeddings.\n",
    "HIDDEN_DIM = 256  # dimensi√≥n de la capas LSTM\n",
    "OUTPUT_DIM = len(NER_TAGS.vocab)  # n√∫mero de clases\n",
    "\n",
    "N_LAYERS = 3  # n√∫mero de capas.\n",
    "DROPOUT = 0.5\n",
    "BIDIRECTIONAL = True\n",
    "\n",
    "baseline_model = NER_RNN(INPUT_DIM, EMBEDDING_DIM, HIDDEN_DIM, OUTPUT_DIM,\n",
    "                         N_LAYERS, BIDIRECTIONAL, DROPOUT)\n",
    "\n",
    "baseline_model_name = 'baseline'  # nombre que tendr√° el modelo guardado...\n",
    "baseline_n_epochs = 10\n",
    "\n",
    "\n",
    "# Loss: Cross Entropy\n",
    "TAG_PAD_IDX = NER_TAGS.vocab.stoi[NER_TAGS.pad_token]\n",
    "baseline_criterion = nn.CrossEntropyLoss(ignore_index = TAG_PAD_IDX)\n",
    "\n",
    "\n",
    "model = baseline_model\n",
    "model_name = baseline_model_name\n",
    "criterion = baseline_criterion\n",
    "n_epochs = baseline_n_epochs\n",
    "\n",
    "\n",
    "def init_weights(m):\n",
    "    # Inicializamos los pesos como aleatorios\n",
    "    for name, param in m.named_parameters():\n",
    "        nn.init.normal_(param.data, mean=0, std=0.1) \n",
    "        \n",
    "    # Seteamos como 0 los embeddings de UNK y PAD.\n",
    "    model.embedding.weight.data[UNK_IDX] = torch.zeros(EMBEDDING_DIM)\n",
    "    model.embedding.weight.data[PAD_IDX] = torch.zeros(EMBEDDING_DIM)\n",
    "\n",
    "\n",
    "model.apply(init_weights)\n",
    "\n",
    "\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f'El modelo actual tiene {count_parameters(model):,} par√°metros entrenables.')\n",
    "\n",
    "\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "model = model.to(device)\n",
    "criterion = criterion.to(device)\n",
    "\n",
    "def train(model, iterator, optimizer, criterion):\n",
    "\n",
    "    epoch_loss = 0\n",
    "    epoch_precision = 0\n",
    "    epoch_recall = 0\n",
    "    epoch_f1 = 0\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    # Por cada batch del iterador de la √©poca:\n",
    "    for batch in iterator:\n",
    "\n",
    "        # Extraemos el texto y los tags del batch que estamos procesado\n",
    "        text = batch.text\n",
    "        tags = batch.nertags\n",
    "\n",
    "        # Reiniciamos los gradientes calculados en la iteraci√≥n anterior\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        #text = [sent len, batch size]\n",
    "\n",
    "        # Predecimos los tags del texto del batch.\n",
    "        predictions = model(text)\n",
    "\n",
    "        #predictions = [sent len, batch size, output dim]\n",
    "        #tags = [sent len, batch size]\n",
    "\n",
    "        # Reordenamos los datos para calcular la loss\n",
    "        predictions = predictions.view(-1, predictions.shape[-1])\n",
    "        tags = tags.view(-1)\n",
    "\n",
    "        #predictions = [sent len * batch size, output dim]\n",
    "\n",
    "\n",
    "\n",
    "        # Calculamos el Cross Entropy de las predicciones con respecto a las etiquetas reales\n",
    "        loss = criterion(predictions, tags)\n",
    "        \n",
    "        # Calculamos el accuracy\n",
    "        precision, recall, f1 = calculate_metrics(predictions, tags)\n",
    "\n",
    "        # Calculamos los gradientes\n",
    "        loss.backward()\n",
    "\n",
    "        # Actualizamos los par√°metros de la red\n",
    "        optimizer.step()\n",
    "\n",
    "        # Actualizamos el loss y las m√©tricas\n",
    "        epoch_loss += loss.item()\n",
    "        epoch_precision += precision\n",
    "        epoch_recall += recall\n",
    "        epoch_f1 += f1\n",
    "\n",
    "    return epoch_loss / len(iterator), epoch_precision / len(iterator), epoch_recall / len(iterator), epoch_f1 / len(iterator)\n",
    "\n",
    "\n",
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs\n",
    "\n",
    "\n",
    "def evaluate(model, iterator, criterion):\n",
    "\n",
    "    epoch_loss = 0\n",
    "    epoch_precision = 0\n",
    "    epoch_recall = 0\n",
    "    epoch_f1 = 0\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    # Indicamos que ahora no guardaremos los gradientes\n",
    "    with torch.no_grad():\n",
    "        # Por cada batch\n",
    "        for batch in iterator:\n",
    "\n",
    "            text = batch.text\n",
    "            tags = batch.nertags\n",
    "\n",
    "            # Predecimos\n",
    "            predictions = model(text)\n",
    "\n",
    "            predictions = predictions.view(-1, predictions.shape[-1])\n",
    "            tags = tags.view(-1)\n",
    "\n",
    "            # Calculamos el Cross Entropy de las predicciones con respecto a las etiquetas reales\n",
    "            loss = criterion(predictions, tags)\n",
    "\n",
    "            # Calculamos las m√©tricas\n",
    "            precision, recall, f1 = calculate_metrics(predictions, tags)\n",
    "\n",
    "            # Actualizamos el loss y las m√©tricas\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_precision += precision\n",
    "            epoch_recall += recall\n",
    "            epoch_f1 += f1\n",
    "\n",
    "    return epoch_loss / len(iterator), epoch_precision / len(\n",
    "        iterator), epoch_recall / len(iterator), epoch_f1 / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea48422c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "iK5lQqpviicf",
    "outputId": "0880b2a1-7386-4277-f863-afd343c8cd13",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\autofact\\Documents\\NLP-competition-2\\venv\\lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: <pad> seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "c:\\Users\\autofact\\Documents\\NLP-competition-2\\venv\\lib\\site-packages\\seqeval\\metrics\\v1.py:57: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01 | Epoch Time: 3m 59s\n",
      "\tTrain Loss: 0.944 | Train f1: 0.28 | Train precision: 0.50 | Train recall: 0.20\n",
      "\t Val. Loss: 0.767 |  Val. f1: 0.43 |  Val. precision: 0.69 | Val. recall: 0.32\n",
      "Epoch: 02 | Epoch Time: 3m 56s\n",
      "\tTrain Loss: 0.751 | Train f1: 0.46 | Train precision: 0.66 | Train recall: 0.36\n",
      "\t Val. Loss: 0.639 |  Val. f1: 0.50 |  Val. precision: 0.74 | Val. recall: 0.39\n",
      "Epoch: 03 | Epoch Time: 3m 57s\n",
      "\tTrain Loss: 0.674 | Train f1: 0.51 | Train precision: 0.69 | Train recall: 0.41\n",
      "\t Val. Loss: 0.605 |  Val. f1: 0.55 |  Val. precision: 0.70 | Val. recall: 0.45\n",
      "Epoch: 04 | Epoch Time: 4m 12s\n",
      "\tTrain Loss: 0.612 | Train f1: 0.56 | Train precision: 0.72 | Train recall: 0.46\n",
      "\t Val. Loss: 0.563 |  Val. f1: 0.58 |  Val. precision: 0.74 | Val. recall: 0.48\n",
      "Epoch: 05 | Epoch Time: 4m 13s\n",
      "\tTrain Loss: 0.566 | Train f1: 0.59 | Train precision: 0.73 | Train recall: 0.50\n",
      "\t Val. Loss: 0.532 |  Val. f1: 0.61 |  Val. precision: 0.74 | Val. recall: 0.54\n",
      "Epoch: 06 | Epoch Time: 4m 5s\n",
      "\tTrain Loss: 0.536 | Train f1: 0.61 | Train precision: 0.74 | Train recall: 0.52\n",
      "\t Val. Loss: 0.515 |  Val. f1: 0.62 |  Val. precision: 0.71 | Val. recall: 0.55\n",
      "Epoch: 07 | Epoch Time: 4m 16s\n",
      "\tTrain Loss: 0.506 | Train f1: 0.63 | Train precision: 0.75 | Train recall: 0.55\n",
      "\t Val. Loss: 0.500 |  Val. f1: 0.63 |  Val. precision: 0.72 | Val. recall: 0.56\n",
      "Epoch: 08 | Epoch Time: 4m 11s\n",
      "\tTrain Loss: 0.481 | Train f1: 0.65 | Train precision: 0.76 | Train recall: 0.57\n",
      "\t Val. Loss: 0.468 |  Val. f1: 0.65 |  Val. precision: 0.76 | Val. recall: 0.58\n",
      "Epoch: 09 | Epoch Time: 4m 14s\n",
      "\tTrain Loss: 0.452 | Train f1: 0.67 | Train precision: 0.77 | Train recall: 0.60\n",
      "\t Val. Loss: 0.460 |  Val. f1: 0.66 |  Val. precision: 0.73 | Val. recall: 0.61\n",
      "Epoch: 10 | Epoch Time: 4m 39s\n",
      "\tTrain Loss: 0.435 | Train f1: 0.69 | Train precision: 0.78 | Train recall: 0.61\n",
      "\t Val. Loss: 0.475 |  Val. f1: 0.66 |  Val. precision: 0.73 | Val. recall: 0.60\n"
     ]
    }
   ],
   "source": [
    "best_valid_loss = float('inf')\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Recuerdo: train_iterator y valid_iterator contienen el dataset dividido en batches.\n",
    "\n",
    "    # Entrenar\n",
    "    train_loss, train_precision, train_recall, train_f1 = train(\n",
    "        model, train_iterator, optimizer, criterion)\n",
    "\n",
    "    # Evaluar (valid = validaci√≥n)\n",
    "    valid_loss, valid_precision, valid_recall, valid_f1 = evaluate(\n",
    "        model, valid_iterator, criterion)\n",
    "\n",
    "    end_time = time.time()\n",
    "\n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "\n",
    "    # Si obtuvimos mejores resultados, guardamos este modelo en el almacenamiento (para poder cargarlo luego)\n",
    "    # Si detienen el entrenamiento prematuramente, pueden cargar el modelo en el siguiente recuadro de c√≥digo.\n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), '{}.pt'.format(model_name))\n",
    "    # Si ya no mejoramos el loss de validaci√≥n, terminamos de entrenar.\n",
    "\n",
    "    print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(\n",
    "        f'\\tTrain Loss: {train_loss:.3f} | Train f1: {train_f1:.2f} | Train precision: {train_precision:.2f} | Train recall: {train_recall:.2f}'\n",
    "    )\n",
    "    print(\n",
    "        f'\\t Val. Loss: {valid_loss:.3f} |  Val. f1: {valid_f1:.2f} |  Val. precision: {valid_precision:.2f} | Val. recall: {valid_recall:.2f}'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdf54b51",
   "metadata": {
    "id": "s0gVbP8yiicj",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val. Loss: 0.460 |  Val. f1: 0.66 | Val. precision: 0.73 | Val. recall: 0.61\n"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('{}.pt'.format(model_name)))\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "valid_loss, valid_precision, valid_recall, valid_f1 = evaluate(\n",
    "    model, valid_iterator, criterion)\n",
    "\n",
    "print(\n",
    "    f'Val. Loss: {valid_loss:.3f} |  Val. f1: {valid_f1:.2f} | Val. precision: {valid_precision:.2f} | Val. recall: {valid_recall:.2f}'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9d6a661",
   "metadata": {},
   "source": [
    "Wiki es"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d603b4a2",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Definir la red\n",
    "class NER_RNN(nn.Module):\n",
    "    def __init__(self, \n",
    "                 input_dim, \n",
    "                 embedding_dim, \n",
    "                 hidden_dim, \n",
    "                 output_dim,\n",
    "                 n_layers, \n",
    "                 bidirectional, \n",
    "                 dropout):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        # Capa de embedding\n",
    "        self.embedding = embedding_wiki\n",
    "\n",
    "        # Capa LSTM\n",
    "        self.lstm = nn.LSTM(embedding_dim,\n",
    "                           hidden_dim,\n",
    "                           num_layers=n_layers,\n",
    "                           bidirectional=bidirectional, \n",
    "                           dropout = dropout if n_layers > 1 else 0)\n",
    "\n",
    "        # Capa de salida\n",
    "        self.fc = nn.Linear(hidden_dim * 2 if bidirectional else hidden_dim,\n",
    "                            output_dim)\n",
    "\n",
    "        # Dropout\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, text):\n",
    "        embedded = self.dropout(self.embedding(text))\n",
    "        outputs, (hidden, cell) = self.lstm(embedded)\n",
    "        predictions = self.fc(self.dropout(outputs))\n",
    "        return predictions\n",
    "\n",
    "\n",
    "# Tama√±o del vocabulario\n",
    "INPUT_DIM = len(TEXT.vocab)\n",
    "EMBEDDING_DIM = 300  # dimensi√≥n de los embeddings.\n",
    "HIDDEN_DIM = 256  # dimensi√≥n de la capas LSTM\n",
    "OUTPUT_DIM = len(NER_TAGS.vocab)  # n√∫mero de clases\n",
    "\n",
    "N_LAYERS = 3  # n√∫mero de capas.\n",
    "DROPOUT = 0.5\n",
    "BIDIRECTIONAL = True\n",
    "\n",
    "baseline_model = NER_RNN(INPUT_DIM, EMBEDDING_DIM, HIDDEN_DIM, OUTPUT_DIM,\n",
    "                         N_LAYERS, BIDIRECTIONAL, DROPOUT)\n",
    "\n",
    "baseline_model_name = 'baseline'  # nombre que tendr√° el modelo guardado...\n",
    "baseline_n_epochs = 10\n",
    "\n",
    "\n",
    "# Loss: Cross Entropy\n",
    "TAG_PAD_IDX = NER_TAGS.vocab.stoi[NER_TAGS.pad_token]\n",
    "baseline_criterion = nn.CrossEntropyLoss(ignore_index = TAG_PAD_IDX)\n",
    "\n",
    "\n",
    "model = baseline_model\n",
    "model_name = baseline_model_name\n",
    "criterion = baseline_criterion\n",
    "n_epochs = baseline_n_epochs\n",
    "\n",
    "\n",
    "def init_weights(m):\n",
    "    # Inicializamos los pesos como aleatorios\n",
    "    for name, param in m.named_parameters():\n",
    "        nn.init.normal_(param.data, mean=0, std=0.1) \n",
    "        \n",
    "    # Seteamos como 0 los embeddings de UNK y PAD.\n",
    "    model.embedding.weight.data[UNK_IDX] = torch.zeros(EMBEDDING_DIM)\n",
    "    model.embedding.weight.data[PAD_IDX] = torch.zeros(EMBEDDING_DIM)\n",
    "\n",
    "\n",
    "model.apply(init_weights)\n",
    "\n",
    "\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f'El modelo actual tiene {count_parameters(model):,} par√°metros entrenables.')\n",
    "\n",
    "\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "model = model.to(device)\n",
    "criterion = criterion.to(device)\n",
    "\n",
    "def train(model, iterator, optimizer, criterion):\n",
    "\n",
    "    epoch_loss = 0\n",
    "    epoch_precision = 0\n",
    "    epoch_recall = 0\n",
    "    epoch_f1 = 0\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    # Por cada batch del iterador de la √©poca:\n",
    "    for batch in iterator:\n",
    "\n",
    "        # Extraemos el texto y los tags del batch que estamos procesado\n",
    "        text = batch.text\n",
    "        tags = batch.nertags\n",
    "\n",
    "        # Reiniciamos los gradientes calculados en la iteraci√≥n anterior\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        #text = [sent len, batch size]\n",
    "\n",
    "        # Predecimos los tags del texto del batch.\n",
    "        predictions = model(text)\n",
    "\n",
    "        #predictions = [sent len, batch size, output dim]\n",
    "        #tags = [sent len, batch size]\n",
    "\n",
    "        # Reordenamos los datos para calcular la loss\n",
    "        predictions = predictions.view(-1, predictions.shape[-1])\n",
    "        tags = tags.view(-1)\n",
    "\n",
    "        #predictions = [sent len * batch size, output dim]\n",
    "\n",
    "\n",
    "\n",
    "        # Calculamos el Cross Entropy de las predicciones con respecto a las etiquetas reales\n",
    "        loss = criterion(predictions, tags)\n",
    "        \n",
    "        # Calculamos el accuracy\n",
    "        precision, recall, f1 = calculate_metrics(predictions, tags)\n",
    "\n",
    "        # Calculamos los gradientes\n",
    "        loss.backward()\n",
    "\n",
    "        # Actualizamos los par√°metros de la red\n",
    "        optimizer.step()\n",
    "\n",
    "        # Actualizamos el loss y las m√©tricas\n",
    "        epoch_loss += loss.item()\n",
    "        epoch_precision += precision\n",
    "        epoch_recall += recall\n",
    "        epoch_f1 += f1\n",
    "\n",
    "    return epoch_loss / len(iterator), epoch_precision / len(iterator), epoch_recall / len(iterator), epoch_f1 / len(iterator)\n",
    "\n",
    "\n",
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs\n",
    "\n",
    "\n",
    "def evaluate(model, iterator, criterion):\n",
    "\n",
    "    epoch_loss = 0\n",
    "    epoch_precision = 0\n",
    "    epoch_recall = 0\n",
    "    epoch_f1 = 0\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    # Indicamos que ahora no guardaremos los gradientes\n",
    "    with torch.no_grad():\n",
    "        # Por cada batch\n",
    "        for batch in iterator:\n",
    "\n",
    "            text = batch.text\n",
    "            tags = batch.nertags\n",
    "\n",
    "            # Predecimos\n",
    "            predictions = model(text)\n",
    "\n",
    "            predictions = predictions.view(-1, predictions.shape[-1])\n",
    "            tags = tags.view(-1)\n",
    "\n",
    "            # Calculamos el Cross Entropy de las predicciones con respecto a las etiquetas reales\n",
    "            loss = criterion(predictions, tags)\n",
    "\n",
    "            # Calculamos las m√©tricas\n",
    "            precision, recall, f1 = calculate_metrics(predictions, tags)\n",
    "\n",
    "            # Actualizamos el loss y las m√©tricas\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_precision += precision\n",
    "            epoch_recall += recall\n",
    "            epoch_f1 += f1\n",
    "\n",
    "    return epoch_loss / len(iterator), epoch_precision / len(\n",
    "        iterator), epoch_recall / len(iterator), epoch_f1 / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79fc041e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "iK5lQqpviicf",
    "outputId": "0880b2a1-7386-4277-f863-afd343c8cd13",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\autofact\\Documents\\NLP-competition-2\\venv\\lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: <pad> seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "c:\\Users\\autofact\\Documents\\NLP-competition-2\\venv\\lib\\site-packages\\seqeval\\metrics\\v1.py:57: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01 | Epoch Time: 3m 55s\n",
      "\tTrain Loss: 0.937 | Train f1: 0.30 | Train precision: 0.53 | Train recall: 0.22\n",
      "\t Val. Loss: 0.748 |  Val. f1: 0.44 |  Val. precision: 0.68 | Val. recall: 0.33\n",
      "Epoch: 02 | Epoch Time: 4m 5s\n",
      "\tTrain Loss: 0.750 | Train f1: 0.46 | Train precision: 0.66 | Train recall: 0.36\n",
      "\t Val. Loss: 0.645 |  Val. f1: 0.49 |  Val. precision: 0.73 | Val. recall: 0.37\n",
      "Epoch: 03 | Epoch Time: 3m 56s\n",
      "\tTrain Loss: 0.666 | Train f1: 0.51 | Train precision: 0.68 | Train recall: 0.41\n",
      "\t Val. Loss: 0.589 |  Val. f1: 0.56 |  Val. precision: 0.72 | Val. recall: 0.47\n",
      "Epoch: 04 | Epoch Time: 4m 3s\n",
      "\tTrain Loss: 0.612 | Train f1: 0.56 | Train precision: 0.72 | Train recall: 0.46\n",
      "\t Val. Loss: 0.570 |  Val. f1: 0.56 |  Val. precision: 0.74 | Val. recall: 0.47\n",
      "Epoch: 05 | Epoch Time: 6m 41s\n",
      "\tTrain Loss: 0.568 | Train f1: 0.59 | Train precision: 0.74 | Train recall: 0.50\n",
      "\t Val. Loss: 0.538 |  Val. f1: 0.59 |  Val. precision: 0.75 | Val. recall: 0.50\n",
      "Epoch: 06 | Epoch Time: 4m 2s\n",
      "\tTrain Loss: 0.537 | Train f1: 0.61 | Train precision: 0.74 | Train recall: 0.52\n",
      "\t Val. Loss: 0.516 |  Val. f1: 0.60 |  Val. precision: 0.74 | Val. recall: 0.52\n",
      "Epoch: 07 | Epoch Time: 7m 3s\n",
      "\tTrain Loss: 0.506 | Train f1: 0.63 | Train precision: 0.75 | Train recall: 0.55\n",
      "\t Val. Loss: 0.497 |  Val. f1: 0.63 |  Val. precision: 0.75 | Val. recall: 0.55\n",
      "Epoch: 08 | Epoch Time: 4m 15s\n",
      "\tTrain Loss: 0.483 | Train f1: 0.65 | Train precision: 0.76 | Train recall: 0.57\n",
      "\t Val. Loss: 0.478 |  Val. f1: 0.64 |  Val. precision: 0.75 | Val. recall: 0.57\n",
      "Epoch: 09 | Epoch Time: 4m 19s\n",
      "\tTrain Loss: 0.452 | Train f1: 0.67 | Train precision: 0.78 | Train recall: 0.60\n",
      "\t Val. Loss: 0.471 |  Val. f1: 0.66 |  Val. precision: 0.75 | Val. recall: 0.59\n",
      "Epoch: 10 | Epoch Time: 4m 22s\n",
      "\tTrain Loss: 0.434 | Train f1: 0.69 | Train precision: 0.78 | Train recall: 0.62\n",
      "\t Val. Loss: 0.469 |  Val. f1: 0.66 |  Val. precision: 0.74 | Val. recall: 0.60\n"
     ]
    }
   ],
   "source": [
    "best_valid_loss = float('inf')\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Recuerdo: train_iterator y valid_iterator contienen el dataset dividido en batches.\n",
    "\n",
    "    # Entrenar\n",
    "    train_loss, train_precision, train_recall, train_f1 = train(\n",
    "        model, train_iterator, optimizer, criterion)\n",
    "\n",
    "    # Evaluar (valid = validaci√≥n)\n",
    "    valid_loss, valid_precision, valid_recall, valid_f1 = evaluate(\n",
    "        model, valid_iterator, criterion)\n",
    "\n",
    "    end_time = time.time()\n",
    "\n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "\n",
    "    # Si obtuvimos mejores resultados, guardamos este modelo en el almacenamiento (para poder cargarlo luego)\n",
    "    # Si detienen el entrenamiento prematuramente, pueden cargar el modelo en el siguiente recuadro de c√≥digo.\n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), '{}.pt'.format(model_name))\n",
    "    # Si ya no mejoramos el loss de validaci√≥n, terminamos de entrenar.\n",
    "\n",
    "    print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(\n",
    "        f'\\tTrain Loss: {train_loss:.3f} | Train f1: {train_f1:.2f} | Train precision: {train_precision:.2f} | Train recall: {train_recall:.2f}'\n",
    "    )\n",
    "    print(\n",
    "        f'\\t Val. Loss: {valid_loss:.3f} |  Val. f1: {valid_f1:.2f} |  Val. precision: {valid_precision:.2f} | Val. recall: {valid_recall:.2f}'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34462344",
   "metadata": {
    "id": "s0gVbP8yiicj",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val. Loss: 0.469 |  Val. f1: 0.66 | Val. precision: 0.74 | Val. recall: 0.60\n"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('{}.pt'.format(model_name)))\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "valid_loss, valid_precision, valid_recall, valid_f1 = evaluate(\n",
    "    model, valid_iterator, criterion)\n",
    "\n",
    "print(\n",
    "    f'Val. Loss: {valid_loss:.3f} |  Val. f1: {valid_f1:.2f} | Val. precision: {valid_precision:.2f} | Val. recall: {valid_recall:.2f}'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "805d01b2",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Baseline mejorado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a22820f",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Definir la red\n",
    "class NER_RNN(nn.Module):\n",
    "    def __init__(self, \n",
    "                 input_dim, \n",
    "                 embedding_dim, \n",
    "                 hidden_dim, \n",
    "                 output_dim,\n",
    "                 n_layers, \n",
    "                 bidirectional, \n",
    "                 dropout):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        # Capa de embedding\n",
    "        self.embedding = nn.Embedding(855380, embedding_dim)\n",
    "\n",
    "        # Capa LSTM\n",
    "        self.lstm = nn.LSTM(embedding_dim,\n",
    "                           hidden_dim,\n",
    "                           num_layers=n_layers,\n",
    "                           bidirectional=bidirectional, \n",
    "                           dropout = dropout if n_layers > 1 else 0)\n",
    "\n",
    "        # Capa de salida\n",
    "        self.fc = nn.Linear(hidden_dim * 2 if bidirectional else hidden_dim,\n",
    "                            output_dim)\n",
    "\n",
    "        # Dropout\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, text):\n",
    "\n",
    "        #text = [sent len, batch size]\n",
    "\n",
    "        # Convertir lo enviado a embedding\n",
    "        embedded = self.dropout(self.embedding(text))\n",
    "        \n",
    "        outputs, (hidden, cell) = self.lstm(embedded)\n",
    "        #embedded = [sent len, batch size, emb dim]\n",
    "\n",
    "        # Pasar los embeddings por la rnn (LSTM)\n",
    "\n",
    "        #output = [sent len, batch size, hid dim * n directions]\n",
    "        #hidden/cell = [n layers * n directions, batch size, hid dim]\n",
    "\n",
    "        # Predecir usando la capa de salida.\n",
    "        predictions = self.fc(self.dropout(outputs))\n",
    "        #predictions = [sent len, batch size, output dim]\n",
    "\n",
    "        return predictions\n",
    "\n",
    "\n",
    "# Tama√±o del vocabulario\n",
    "INPUT_DIM = len(TEXT.vocab)\n",
    "EMBEDDING_DIM = 300  # dimensi√≥n de los embeddings.\n",
    "HIDDEN_DIM = 256  # dimensi√≥n de la capas LSTM\n",
    "OUTPUT_DIM = len(NER_TAGS.vocab)  # n√∫mero de clases\n",
    "\n",
    "N_LAYERS = 3  # n√∫mero de capas.\n",
    "DROPOUT = 0.5\n",
    "BIDIRECTIONAL = True\n",
    "\n",
    "baseline_model = NER_RNN(INPUT_DIM, EMBEDDING_DIM, HIDDEN_DIM, OUTPUT_DIM,\n",
    "                         N_LAYERS, BIDIRECTIONAL, DROPOUT)\n",
    "\n",
    "baseline_model_name = 'baseline'  # nombre que tendr√° el modelo guardado...\n",
    "baseline_n_epochs = 10\n",
    "\n",
    "\n",
    "# Loss: Cross Entropy\n",
    "TAG_PAD_IDX = NER_TAGS.vocab.stoi[NER_TAGS.pad_token]\n",
    "baseline_criterion = nn.CrossEntropyLoss(ignore_index = TAG_PAD_IDX)\n",
    "\n",
    "\n",
    "model = baseline_model\n",
    "model_name = baseline_model_name\n",
    "criterion = baseline_criterion\n",
    "n_epochs = baseline_n_epochs\n",
    "\n",
    "\n",
    "def init_weights(m):\n",
    "    # Inicializamos los pesos como aleatorios\n",
    "    for name, param in m.named_parameters():\n",
    "        nn.init.normal_(param.data, mean=0, std=0.1) \n",
    "        \n",
    "    # Seteamos como 0 los embeddings de UNK y PAD.\n",
    "    model.embedding.weight.data[UNK_IDX] = torch.zeros(EMBEDDING_DIM)\n",
    "    model.embedding.weight.data[PAD_IDX] = torch.zeros(EMBEDDING_DIM)\n",
    "\n",
    "\n",
    "model.apply(init_weights)\n",
    "\n",
    "\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f'El modelo actual tiene {count_parameters(model):,} par√°metros entrenables.')\n",
    "\n",
    "\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "model = model.to(device)\n",
    "criterion = criterion.to(device)\n",
    "\n",
    "def train(model, iterator, optimizer, criterion):\n",
    "\n",
    "    epoch_loss = 0\n",
    "    epoch_precision = 0\n",
    "    epoch_recall = 0\n",
    "    epoch_f1 = 0\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    # Por cada batch del iterador de la √©poca:\n",
    "    for batch in iterator:\n",
    "\n",
    "        # Extraemos el texto y los tags del batch que estamos procesado\n",
    "        text = batch.text\n",
    "        tags = batch.nertags\n",
    "\n",
    "        # Reiniciamos los gradientes calculados en la iteraci√≥n anterior\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        #text = [sent len, batch size]\n",
    "\n",
    "        # Predecimos los tags del texto del batch.\n",
    "        predictions = model(text)\n",
    "\n",
    "        #predictions = [sent len, batch size, output dim]\n",
    "        #tags = [sent len, batch size]\n",
    "\n",
    "        # Reordenamos los datos para calcular la loss\n",
    "        predictions = predictions.view(-1, predictions.shape[-1])\n",
    "        tags = tags.view(-1)\n",
    "\n",
    "        #predictions = [sent len * batch size, output dim]\n",
    "\n",
    "\n",
    "\n",
    "        # Calculamos el Cross Entropy de las predicciones con respecto a las etiquetas reales\n",
    "        loss = criterion(predictions, tags)\n",
    "        \n",
    "        # Calculamos el accuracy\n",
    "        precision, recall, f1 = calculate_metrics(predictions, tags)\n",
    "\n",
    "        # Calculamos los gradientes\n",
    "        loss.backward()\n",
    "\n",
    "        # Actualizamos los par√°metros de la red\n",
    "        optimizer.step()\n",
    "\n",
    "        # Actualizamos el loss y las m√©tricas\n",
    "        epoch_loss += loss.item()\n",
    "        epoch_precision += precision\n",
    "        epoch_recall += recall\n",
    "        epoch_f1 += f1\n",
    "\n",
    "    return epoch_loss / len(iterator), epoch_precision / len(iterator), epoch_recall / len(iterator), epoch_f1 / len(iterator)\n",
    "\n",
    "\n",
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs\n",
    "\n",
    "\n",
    "def evaluate(model, iterator, criterion):\n",
    "\n",
    "    epoch_loss = 0\n",
    "    epoch_precision = 0\n",
    "    epoch_recall = 0\n",
    "    epoch_f1 = 0\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    # Indicamos que ahora no guardaremos los gradientes\n",
    "    with torch.no_grad():\n",
    "        # Por cada batch\n",
    "        for batch in iterator:\n",
    "\n",
    "            text = batch.text\n",
    "            tags = batch.nertags\n",
    "\n",
    "            # Predecimos\n",
    "            predictions = model(text)\n",
    "\n",
    "            predictions = predictions.view(-1, predictions.shape[-1])\n",
    "            tags = tags.view(-1)\n",
    "\n",
    "            # Calculamos el Cross Entropy de las predicciones con respecto a las etiquetas reales\n",
    "            loss = criterion(predictions, tags)\n",
    "\n",
    "            # Calculamos las m√©tricas\n",
    "            precision, recall, f1 = calculate_metrics(predictions, tags)\n",
    "\n",
    "            # Actualizamos el loss y las m√©tricas\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_precision += precision\n",
    "            epoch_recall += recall\n",
    "            epoch_f1 += f1\n",
    "\n",
    "    return epoch_loss / len(iterator), epoch_precision / len(\n",
    "        iterator), epoch_recall / len(iterator), epoch_f1 / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "932711a8",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iK5lQqpviicf",
    "outputId": "fab7900b-9cac-4f99-8c71-172a10d73d31",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: <pad> seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/usr/local/lib/python3.7/dist-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01 | Epoch Time: 1m 5s\n",
      "\tTrain Loss: 0.758 | Train f1: 0.45 | Train precision: 0.61 | Train recall: 0.38\n",
      "\t Val. Loss: 0.465 |  Val. f1: 0.65 |  Val. precision: 0.77 | Val. recall: 0.57\n",
      "Epoch: 02 | Epoch Time: 1m 6s\n",
      "\tTrain Loss: 0.393 | Train f1: 0.72 | Train precision: 0.77 | Train recall: 0.68\n",
      "\t Val. Loss: 0.397 |  Val. f1: 0.71 |  Val. precision: 0.80 | Val. recall: 0.65\n",
      "Epoch: 03 | Epoch Time: 1m 6s\n",
      "\tTrain Loss: 0.265 | Train f1: 0.81 | Train precision: 0.83 | Train recall: 0.79\n",
      "\t Val. Loss: 0.365 |  Val. f1: 0.76 |  Val. precision: 0.78 | Val. recall: 0.75\n",
      "Epoch: 04 | Epoch Time: 1m 7s\n",
      "\tTrain Loss: 0.192 | Train f1: 0.86 | Train precision: 0.87 | Train recall: 0.86\n",
      "\t Val. Loss: 0.343 |  Val. f1: 0.77 |  Val. precision: 0.79 | Val. recall: 0.75\n",
      "Epoch: 05 | Epoch Time: 1m 6s\n",
      "\tTrain Loss: 0.151 | Train f1: 0.89 | Train precision: 0.89 | Train recall: 0.89\n",
      "\t Val. Loss: 0.357 |  Val. f1: 0.77 |  Val. precision: 0.79 | Val. recall: 0.76\n",
      "Epoch: 06 | Epoch Time: 1m 6s\n",
      "\tTrain Loss: 0.120 | Train f1: 0.91 | Train precision: 0.91 | Train recall: 0.91\n",
      "\t Val. Loss: 0.393 |  Val. f1: 0.78 |  Val. precision: 0.79 | Val. recall: 0.76\n",
      "Epoch: 07 | Epoch Time: 1m 6s\n",
      "\tTrain Loss: 0.099 | Train f1: 0.93 | Train precision: 0.93 | Train recall: 0.93\n",
      "\t Val. Loss: 0.434 |  Val. f1: 0.78 |  Val. precision: 0.79 | Val. recall: 0.78\n",
      "Epoch: 08 | Epoch Time: 1m 6s\n",
      "\tTrain Loss: 0.088 | Train f1: 0.93 | Train precision: 0.93 | Train recall: 0.93\n",
      "\t Val. Loss: 0.432 |  Val. f1: 0.77 |  Val. precision: 0.79 | Val. recall: 0.76\n",
      "Epoch: 09 | Epoch Time: 1m 6s\n",
      "\tTrain Loss: 0.073 | Train f1: 0.94 | Train precision: 0.94 | Train recall: 0.94\n",
      "\t Val. Loss: 0.472 |  Val. f1: 0.77 |  Val. precision: 0.79 | Val. recall: 0.75\n",
      "Epoch: 10 | Epoch Time: 1m 6s\n",
      "\tTrain Loss: 0.063 | Train f1: 0.95 | Train precision: 0.95 | Train recall: 0.95\n",
      "\t Val. Loss: 0.468 |  Val. f1: 0.77 |  Val. precision: 0.78 | Val. recall: 0.77\n"
     ]
    }
   ],
   "source": [
    "best_valid_loss = float('inf')\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Recuerdo: train_iterator y valid_iterator contienen el dataset dividido en batches.\n",
    "\n",
    "    # Entrenar\n",
    "    train_loss, train_precision, train_recall, train_f1 = train(\n",
    "        model, train_iterator, optimizer, criterion)\n",
    "\n",
    "    # Evaluar (valid = validaci√≥n)\n",
    "    valid_loss, valid_precision, valid_recall, valid_f1 = evaluate(\n",
    "        model, valid_iterator, criterion)\n",
    "\n",
    "    end_time = time.time()\n",
    "\n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "\n",
    "    # Si obtuvimos mejores resultados, guardamos este modelo en el almacenamiento (para poder cargarlo luego)\n",
    "    # Si detienen el entrenamiento prematuramente, pueden cargar el modelo en el siguiente recuadro de c√≥digo.\n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), '{}.pt'.format(model_name))\n",
    "    # Si ya no mejoramos el loss de validaci√≥n, terminamos de entrenar.\n",
    "\n",
    "    print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(\n",
    "        f'\\tTrain Loss: {train_loss:.3f} | Train f1: {train_f1:.2f} | Train precision: {train_precision:.2f} | Train recall: {train_recall:.2f}'\n",
    "    )\n",
    "    print(\n",
    "        f'\\t Val. Loss: {valid_loss:.3f} |  Val. f1: {valid_f1:.2f} |  Val. precision: {valid_precision:.2f} | Val. recall: {valid_recall:.2f}'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94e86b0b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "s0gVbP8yiicj",
    "outputId": "2c9cad02-679b-4b4b-c33e-07145690b90b",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val. Loss: 0.331 |  Val. f1: 0.77 | Val. precision: 0.80 | Val. recall: 0.75\n"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('{}.pt'.format(model_name)))\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "valid_loss, valid_precision, valid_recall, valid_f1 = evaluate(model, valid_iterator, criterion)\n",
    "\n",
    "print(\n",
    "    f'Val. Loss: {valid_loss:.3f} |  Val. f1: {valid_f1:.2f} | Val. precision: {valid_precision:.2f} | Val. recall: {valid_recall:.2f}'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1867bf5",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Conclusiones üí°\n",
    "\n",
    "**REQUIREMENTS: Discutir resultados, proponer trabajo futuro. (1 punto)**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
