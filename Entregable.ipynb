{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3559fead",
   "metadata": {},
   "source": [
    "# Competencia 2 NLP : Reconocimiento de entidades (NER) en Espa√±ol\n",
    "\n",
    "**Integrantes:** Alexander Cuevas - Jorge Guti√©rrez - Benjam√≠n Mellado - Felipe Urrutia\n",
    "\n",
    "**Usuario del equipo en CodaLab:** *teamGalactico üåå*\n",
    "\n",
    "**Fecha l√≠mite de entrega üìÜ:** 29 de Junio a las 23:59 hrs.\n",
    "\n",
    "**Tiempo estimado de dedicaci√≥n:** En curso... ‚è≤Ô∏è\n",
    "\n",
    "**Link competencia:** [CC6205 Assignment 2 - Named Entity Recognition (NER) in Spanish](https://codalab.lisn.upsaclay.fr/competitions/5098?secret_key=09955d45-6210-4a35-a171-8050aa050855#learn_the_details)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2842ee35",
   "metadata": {},
   "source": [
    "## Introducci√≥n üè∑Ô∏è\n",
    "\n",
    "**REQUIREMENTS: Presentar brevemente el contexto, problema a resolver, incluyendo la formalizaci√≥n de la task (c√≥mo son los inputs y outputs del problema) y los desaf√≠os que ven al analizar el corpus entregado. (0.5 puntos)**\n",
    "\n",
    "El an√°lisis de documentos cl√≠nicos es una labor que permite identificar enfermedades, procedimientos, medicamentos, y partes del cuerpo, entre otras entidades relevantes. Adem√°s, se puede usar para determinar las relaciones entre cada una de estas. Con estos datos es posible elaborar estad√≠sticas para una poblaci√≥n y sus individuos. En particular, se pueden determinar factores de riesgo, priorizar pacientes de acuerdo a sus necesidades, establecer casos solucionables sin intervenci√≥n presencial, conocer antecedentes familiares, saber cuantas personas sufren m√°s de una enfermedad, cuantas consumen m√°s de un medicamento, etc.\n",
    "\n",
    "En Chile, el sistema p√∫blico de salud cubre al 75% de la poblaci√≥n. Bajo este sistema, las visitas a especialistas, que requieren la remisi√≥n de un m√©dico general, son manejadas a trav√©s de una lista de espera dividida para casos GES y NO GES. En 2017, hubo 1.661.826 personas esperando una cita con un especialista, con un promedio de espera por sobre los 400 d√≠as. Por este motivo, se hace necesario desarrollar sistemas automatizados que analicen la lista NO GES, para mejorar el manejo de pacientes. En particular, existe una falta de trabajos realizados en el √°rea de NER en Espa√±ol y a√∫n m√°s en el contexto cl√≠nico.\n",
    "\n",
    "Bajo tal contexto, en este trabajo utilizaremos el Corpus de Listas de Espera propuesto por [1]. Este corpus est√° constituido por derivaciones de diversas consultas de la lista de espera NO GES de los hospitales p√∫blicos chilenos. Por su naturaleza, cada una de estas derivaciones es un dato no-estructurado escrito en lenguaje natural. Entre los resultados principales de [1] est√° otorgar un Corpus anotado usando texto cl√≠nico en espa√±ol.\n",
    "\n",
    "La tarea principal propuesta por [1] es un reconocimiento de entidades nombradas. Tambi√©n conocida como NER, por sus siglas en ingles. Dicha tarea es una de las m√°s populares en NLP. La cual consiste en identificar autom√°ticamente piezas esenciales de information (entidades) en el texto. Tradicionalmente, la categor√≠a de entidades a identificar eran nombres de personas, lugares y organizaciones. Actualmente, los tipos de categor√≠as se han expandido a diversos dominios del conocimiento. En este trabajo, las entidades que estudiaremos viven en el contexto de texto cl√≠nico.\n",
    "\n",
    "[1] utiliza el esquema Unified Medical Language System (UMLS) para las categor√≠as de entidades. Originalmente, [1] genera un Corpus con siete tipos de entidades:\n",
    "\n",
    "    (1) Finding, (2) Procedure, (3) Family Member, (4) Disease, (5) Body Part, (6) Medication y (7) Abbreviation. \n",
    "    \n",
    "Sin embargo, en este trabajo solo estudiaremos cinco de ellas. Todas excepto (1) y (7). Las anotaciones siguen el esquema IOB2, estandar en NER. Esquema definido como sigue: \n",
    "\n",
    ">*Esquema IOB2*. Consiste en identificar cada token con una etiqueta que indica si pertenece o no a una entidad. Si un token **no** es parte de una entidad, se indican con $\\texttt{O}$ (por la inicial en Ingles de *Fuera*). Sino, este necesariamente es un token de una entidad. Para indicar si el token es el inicio de una entidad, se utiliza el prefijo $\\texttt{B-}$ (por la inicial en Ingles de *Comienzo*). En cambio, si el token es la continuacion de una entidad, se utiliza el prefijo $\\texttt{I-}$ (por la inicial en Ingles de *Dentro*). Un ejemplo de este esquema, es el siguiente: \n",
    "\n",
    "\\begin{align*}\n",
    "&\\texttt{PRESENTA}& &\\texttt{FRACTURA}& &\\texttt{CORONARIA}& &\\texttt{COMPLICADA}& &\\texttt{EN}& &\\quad\\text{(Texto)}\\\\\n",
    "&\\texttt{O}& &\\texttt{B-Disease}& &\\texttt{I-Disease}& &\\texttt{I-Disease}& &\\texttt{O}& &\\quad\\text{(IOB2)}\n",
    "\\end{align*}\n",
    "\n",
    "Tal como se puede observar del ejemplo anterior, si se sigue el esquema de anotamiento IOB2 para la tarea de NER, el problema de reconocimiento de entidades nombradas se transforma en un problema de Sequence-Labeling. Tipo de problema que hemos discutido durante las √∫ltimas unidades del curso. Entre los modelos estudiados estan HMM, CRF, MEMMS, CNN y RNN. En este trabajo, nos focalizaremos en solo modelos neuronales dentro de la familia de las Redes Neuronales Recurrentes (RNN, por sus siglas en Ingles).\n",
    "\n",
    "Ahora bien, el conjunto de datos vienen entregados en una forma est√°ndar para la tarea de NER, denominado ConLL. Este consiste en un archivo de texto con dos columnas y saltos de l√≠nea que separan cada secuencia. La primera columna contiene los tokens de la secuencia. Mientras que la segunda columna, contiene las etiquetas IOB2 asociadas a cada token de la secuencia. Por otro lado, los datos vienen previamente separados en conjuntos de Entrenamiento, Desarrollo y Prueba. Estos conjuntos tienen tama√±os X, Y y Z, resp. Se espera que el output siga el mismo formato, y resulte de modificar el conjunto de Prueba que contiene tokens con todas las entidades inicializadas en $\\texttt{O}$.\n",
    "\n",
    "Adem√°s, la proporci√≥n de etiquetas IOB2 son mayoritariamente $\\texttt{O}$. Con una proporci√≥n total de A, B y C, en cada conjunto resp. El segundo tipo de etiqueta IOB2 m√°s frecuente es D, con proporciones E, F y G. Mientras que la etiqueta IOB2 menos frecuente es H, con proporciones I, J, K. Esta diferencia considerable de proporciones de las etiquetas, se denomina desbalance de datos.\n",
    "\n",
    "Entre las dificultades encontradas para esta tarea est√°n: (1) Desbalance de etiquetas IOB2, (2) [alguna otra encontrada del an√°lisis exploratorio de datos] y (3) NER en contexto de texto cl√≠nico. Consideramos que dichas dificultades son desafiantes de resolver. Es por esto que es de nuestro inter√©s lograr resolver satisfactoriamente esta tarea. En las secciones que siguen se encuentra detalladamente nuestra soluci√≥n.\n",
    "\n",
    "**Referencias**\n",
    "\n",
    "[1] B√°ez, P., Villena, F., Rojas, M., Dur√°n, M., & Dunstan, J. (2020, November). The Chilean Waiting List Corpus: a new resource for clinical named entity recognition in Spanish. In Proceedings of the 3rd clinical natural language processing workshop (pp. 291-300). https://aclanthology.org/2020.clinicalnlp-1.32/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a2812ce",
   "metadata": {},
   "source": [
    "## Modelos ü§ñ\n",
    "\n",
    "**REQUIREMENTS: Describir brevemente los modelos, m√©todos e hiperpar√°metros utilizados. (1.0 puntos)**\n",
    "### Baseline\n",
    "\n",
    "El primer modelo probado fue el entregado como baseline para la competencia. Este consiste en una red neuronal no bidireccional con capa de embedding, tres capas LSTM, y una capa de salida. Utiliza dropout en el entrenamiento. Como hiperpar√°metros, se fija la dimensi√≥n de los embeddings a 200, y la de las tres capas ocultas a 128. Adem√°s, la cantidad de √©pocas de entrenamiento se fija en 10. Finalmente, se define la funci√≥n de p√©rdida como la entrop√≠a cruzada entre el input y los datos objetivo.\n",
    "\n",
    "### Sin stopwords\n",
    "\n",
    "### Con embeddings cl√≠nicos del lenguaje espa√±ol\n",
    "\n",
    "### Con embeddings generales del lenguaje espa√±ol\n",
    "En adici√≥n a los embeddings cl√≠nicos, se realizaron pruebas con embeddings generales del lenguaje espa√±ol, en particular, cinco conjuntos de embeddings disponibles en la p√°gina [Spanish Word Embeddings](https://github.com/dccuchile/spanish-word-embeddings) (consultado el 11 de junio de 2022). Estos provienen de tres corpus de origen: Spanish Unannotated Corpora (SUC), Spanish Billion Word Corpus (SWBC), y Spanish Wikipedia. Y fueron procesados usando tres algoritmos: FastText, Glove, y Word2Vec.\n",
    "\n",
    "Para cada conjunto anterior se cre√≥ un modelo usando el baseline como base. Para todos estos se cambi√≥ el hiperpar√°metro de dimensi√≥n de embeddings a 300, y se mantuvo el resto igual a lo reportado anteriormente. Adem√°s, se modific√≥ la capa de embeddings seg√∫n correspondiera para utilizar los embeddings de la p√°gina en formato .vec.\n",
    "\n",
    "## Baseline mejorado\n",
    "Luego de probar los embeddings generales, se procedi√≥ a ajustar los par√°metros del modelo baseline. En particular, se ajust√≥ la dimensi√≥n de entrada del embedding usado, para que corresponda al que dio mejor resultado: Glove aplicado a SWBC (855380).\n",
    "\n",
    "Adicionalmente, se usaron capas LSTM de 256 dimensiones. Dropout igual a 0.5, y con bidireccionalidad. Para la predicci√≥n, se entren√≥ el modelo 3 veces.\n",
    "\n",
    "### Con datos balanceados en Baseline\n",
    "\n",
    "Al analizar el dataset se observo que los tipos de entidades estaban desbalanceadas, con una gran diferencia entre las cantidades entre tipos etiquetados. El √°tomo que se considero para balancear los tipos de entidades fueron las frases, ya que se consider√≥ que modificar frases agregando o quitando conjuntos de tokens podr√≠a alterar el sentido de la oraci√≥n. Por lo tanto se ejecut√≥ un algoritmo que eliminaba y agraga frases existentes en el dataset. El criterio para decidir esto en base al promedio de los tipos de entidades, en el fondo se quiere lograr que cada tipo de entidad se acerce al promedio para que queden todos balanceados. Se considera los tipos que comienzan como I o B como uno solo. Es decir B-Desease y I-Desease se considerar√≠an solo como Desease. En base a este promedio se decidir√≠a si una frase es agregada nuevamente o quitada del dataset si es que aporta a que el vector con la cantidad de ocurrencia de cada tipo ayuda que el corpus se acerque al promedio.  \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "372b0939",
   "metadata": {},
   "source": [
    "## M√©tricas de evaluaci√≥n üìä\n",
    "\n",
    "**REQUIREMENTS: Describir las m√©tricas utilizadas en la evaluaci√≥n indicando qu√© miden y cu√°l es su interpretaci√≥n en este problema en particular. (0.5 puntos)**\n",
    "\n",
    "\n",
    "En un problema de Sequence-labeling, la variable predicha por un modelo predictivo es un vector de etiquetas, donde cada etiqueta est√° asociada a un token en la secuencia del input. Una metrica usual para medir el desempe√±o de un modelo de este tipo determina TP, TN, FP y FN con cada etiqueta independientemente. Sin embargo, en la tarea de NER, el objetivo es reconocer entidades nombradas en el texto. Siendo las entidades, las predicciones importantes del modelo y no las etiquetas individuales en formato IOB2. Esto porque en el contexto cl√≠nico, varias condiciones y medicamentos tienen asociados una serie de adjetivos, biol√≥gicos y qu√≠micos, que afectan su significado e intensidad. Por su parte, los detalles de un paciente, y la especificaci√≥n de partes del cuerpo son similarmente relevantes. Por esto, una metrica usual no cuantifica el desempe√±o del modelo para el problema original. Para ser m√°s preciso, existen dos motivos, (1) las etiquetas importantes son aquellas asociadas a las entidades y (2) el modelo debe de reconocer exactamente tanto la ubicaci√≥n de la entidad como el tipo de entidad. Una metrica que si logra estas dos cosas, se denomina **metrica estricta**.\n",
    "\n",
    "<!-- \\begin{align*}\n",
    "&\\texttt{PRESENTA}& &\\texttt{FRACTURA}& &\\texttt{CORONARIA}& &\\texttt{COMPLICADA}& &\\texttt{EN}& &\\quad\\text{(Texto)}\\\\\n",
    "&\\texttt{O}& &\\texttt{B-Disease}& &\\texttt{I-Disease}& &\\texttt{I-Disease}& &\\texttt{O}& &\\quad\\text{(IOB2)}\\\\\n",
    "&0& &1& &2& &3& &4& &\\quad\\text{(Indice)}\n",
    "\\end{align*} -->\n",
    "\n",
    "En lo que sigue detallaremos las m√©tricas estrictas utilizadas. Para formalizar su definici√≥n, consideremos $D = (S, T)$ un conjunto de $n$ datos. Donde para cada secuencia $s \\in S$, $t^s \\in T$ corresponden a las etiquetas de $s$. Sin perdida de generalidad, supondremos que toda secuencia es de largo $m$. En la pr√°ctica, $m$ corresponde al largo m√°ximo de una secuencia dentro del conjunto de datos. Si una secuencia (y sus etiquetas) es de largo menor que $m$, se rellena con $\\texttt{<pad>}$. Consideremos $P$ a las etiquetas predichas, i.e., $p^s \\in P$ es la predicci√≥n de la secuencia $s \\in S$, comparable con las etiquetas reales $t^s \\in T$. Naturalmente, $s_i$ es el i-esimo token de la secuencia $s$. An√°logamente, $t^s_i$ para su etiqueta real y $p^s_i$ para la predicha. Se dir√° que $(l, i, j, c)$ es una entidad v√°lida de tipo $c$ en las etiquetas $l$, si $i\\leq j$, $l_i = \\texttt{B-}c$, para cada $k \\in \\{i+1, ..., j\\}$, $l_k = \\texttt{I-}c$ y, si $j<m$, entonces $l_{j+1} \\neq \\texttt{I-}c$. Denotaremos por $E(l, c)$ al conjunto de las entidades v√°lidas de tipo $c$ en las etiquetas $l$. Adicionalmente, denotaremos por $E(L)$ al conjunto de todas las entidades v√°lidas seg√∫n $L$, i.e.,  $$E(L) = \\bigcup \\{E(l,  c): l \\in L \\text{ y $c$ es un tipo de entidad}\\}$$\n",
    "  \n",
    "* *Precision*. Para un tipo de entidad fija (p.ej. Disease), la precision mide la proporci√≥n de veces que una entidad predicha de tal tipo coincide con la entidad real sobre el total de veces que el modelo predice una entidad con el tipo deseado. En efecto, para una entidad de tipo $c$,\n",
    "\n",
    "$$ \\texttt{Precision}(c) = \\frac{|\\{ (l, i, j, d) \\in E(P): d = c, \\exists s \\in S, l = p^s, (p^s, i, j, c) \\in E(t^s, c)\\}|}{|(l, i, j, d) \\in E(P): d=c |}$$\n",
    "\n",
    "* *Recall*. En el contexto cl√≠nico, el recall tambi√©n se conoce como sensibilidad, y es importante dado que frente a diagn√≥sticos y medicamentos sensibles, es preferible descartar un caso FN a no haberlo considerado en primer lugar. Para un tipo de entidad fija, el recall mide la proporci√≥n de veces que una entidad predicha de tal tipo coincide con la entidad real sobre el total de entidades del tipo deseado. En efecto, para una entidad de tipo $c$,\n",
    "\n",
    "$$ \\texttt{Recall}(c) = \\frac{|\\{ (l, i, j, d) \\in E(P): d = c, \\exists s\\in S, l=p^s, (p^s, i, j, c) \\in E(t^s, c)\\}|}{|(l, i, j, d) \\in E(T): d=c |}$$\n",
    "\n",
    "* *Micro F1 score*. **Recuerde hacer la distinci√≥n entre lo que ser√≠a una m√©trica de micro f1-score vs macro f1-score.** Para un tipo de entidad fija, la Precision y el Recall, entregan informaci√≥n distinta sobre las predicciones para dicho tipo de entidad. El primero considera falsos-positivos, mientras que el segundo considera falsos-negativos. Una forma de condensar esta informaci√≥n en una √∫nica m√©trica es con el promedio arm√≥nico entre Precision y Recall, denominado F1-score. Dicha metrica es condicional al tipo de entidad elegida. Para obtener un F1-score global (que considere todos los tipos de entidades), existen dos aproximaciones: (1) Micro F1-score y (2) Macro F1-score. En palabras simples, (1) es el Accuracy del modelo y (2) Un promedio de F1-score por tipo de entidad. Descartamos Macro F1-score dado que nuestros datos son desbalancedados. Por lo tanto, cada F1-score por clase no son estad√≠sticamente comparables. Nos quedaremos con Micro F1-score, ya que traduce globalmente el desempe√±o del modelo. Para computar esta metrica, se utiliza la t√©cnica estricta definida anteriormente. En efecto,\n",
    "\n",
    "$$ \\texttt{Micro-F1-score} = \\frac{|\\{ (l, i, j, d) \\in E(P): \\exists s\\in S, l=p^s, (p^s, i, j, d) \\in E(t^s, d)\\}|}{|S|}$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b571663",
   "metadata": {},
   "source": [
    "## Dise√±o experimental ‚úèÔ∏è\n",
    "\n",
    "**REQUIREMENTS: Esta es una de las secciones m√°s importantes del reporte. Deben describir minuciosamente los experimentos que realizar√°n en la siguiente secci√≥n. Describir las variables de control que manejar√°n, algunos ejemplos pueden ser: Los hiperpar√°metros de los modelos, tipo de embeddings utilizados, tipos de arquitecturas. Ser claros con el conjunto de hiperpar√°metros que probar√°n, la decisi√≥n en las funciones de optimizaci√≥n, funci√≥n de p√©rdida, regulaci√≥n, etc. B√°sicamente explicar qu√© es lo que veremos en la siguiente secci√≥n. (1 punto)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fa3f9b5",
   "metadata": {},
   "source": [
    "## Experimentos üß™\n",
    "\n",
    "**REQUIREMENTS: Reportar todos sus experimentos y c√≥digo en esta secci√≥n. Comparar los resultados obtenidos utilizando diferentes modelos. ¬°Es vital haber realizado varios experimentos para sacar una buena nota! (2.0 puntos)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a64e6188",
   "metadata": {},
   "source": [
    "#### **Obtener datos**\n",
    "\n",
    "Descargamos los datos de entrenamiento, validaci√≥n y prueba en nuestro directorio de trabajo\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eb4279a3",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'wget' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n",
      "'wget' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n",
      "'wget' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    }
   ],
   "source": [
    "!wget https://github.com/dccuchile/CC6205/releases/download/v1.0/train.txt -nc # Dataset de Entrenamiento\n",
    "!wget https://github.com/dccuchile/CC6205/releases/download/v1.0/dev.txt -nc    # Dataset de Validaci√≥n (Para probar y ajustar el modelo)\n",
    "!wget https://github.com/dccuchile/CC6205/releases/download/v1.0/test.txt -nc  # Dataset de la Competencia. Estos datos solo contienen los tokens. ¬°¬°SON LOS QUE DEBEN SER PREDICHOS!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3e0b9201",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (2233562756.py, line 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Input \u001b[1;32mIn [2]\u001b[1;36m\u001b[0m\n\u001b[1;33m    Un `field`:\u001b[0m\n\u001b[1;37m       ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# Primer Field: TEXT. Representan los tokens de la secuencia\n",
    "TEXT = legacy.data.Field(lower=False) \n",
    "\n",
    "# Segundo Field: NER_TAGS. Representan los Tags asociados a cada palabra.\n",
    "NER_TAGS = legacy.data.Field(unk_token=None)\n",
    "fields = ((\"text\", TEXT), (\"nertags\", NER_TAGS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ad627f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "fields"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd95b650",
   "metadata": {},
   "source": [
    "### Configuraci√≥n com√∫n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e5998c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -U torchtext==0.10.0\n",
    "%pip install seqeval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b9cac9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from seqeval.metrics import f1_score, precision_score, recall_score\n",
    "from torchtext import data, datasets, legacy\n",
    "\n",
    "# Garantizar reproducibilidad de los experimentos\n",
    "SEED = 1234\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "\n",
    "TEXT = legacy.data.Field(lower=False) \n",
    "NER_TAGS = legacy.data.Field(unk_token=None)\n",
    "fields = ((\"text\", TEXT), (\"nertags\", NER_TAGS))\n",
    "\n",
    "\n",
    "train_data, valid_data, test_data = legacy.datasets.SequenceTaggingDataset.splits(\n",
    "    path=\"./\",\n",
    "    train=\"train.txt\",\n",
    "    validation=\"dev.txt\",\n",
    "    test=\"test.txt\",\n",
    "    fields=fields,\n",
    "    encoding=\"utf-8\",\n",
    "    separator=\" \"\n",
    ")\n",
    "\n",
    "\n",
    "print(f\"Numero de ejemplos de entrenamiento: {len(train_data)}\")\n",
    "print(f\"N√∫mero de ejemplos de validaci√≥n: {len(valid_data)}\")\n",
    "print(f\"N√∫mero de ejemplos de test (competencia): {len(test_data)}\")\n",
    "\n",
    "\n",
    "TEXT.build_vocab(train_data)\n",
    "NER_TAGS.build_vocab(train_data)\n",
    "UNK_IDX = TEXT.vocab.stoi[TEXT.unk_token]\n",
    "PAD_IDX = TEXT.vocab.stoi[TEXT.pad_token]\n",
    "PAD_TAG_IDX = NER_TAGS.vocab.stoi[NER_TAGS.pad_token]\n",
    "O_TAG_IDX = NER_TAGS.vocab.stoi['O']\n",
    "\n",
    "\n",
    "def tag_percentage(tag_counts):\n",
    "    \n",
    "    total_count = sum([count for tag, count in tag_counts])\n",
    "    tag_counts_percentages = [(tag, count, count/total_count) for tag, count in tag_counts]\n",
    "  \n",
    "    return tag_counts_percentages\n",
    "\n",
    "\n",
    "print(\"Tag Ocurrencia Porcentaje\\n\")\n",
    "\n",
    "for tag, count, percent in tag_percentage(NER_TAGS.vocab.freqs.most_common()):\n",
    "    print(f\"{tag}\\t{count}\\t{percent*100:4.1f}%\")\n",
    "\n",
    "BATCH_SIZE = 16  # disminuir si hay problemas de ram.\n",
    "\n",
    "# Usar cuda si es que est√° disponible.\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using', device)\n",
    "\n",
    "# Dividir datos entre entrenamiento y test. Si van a hacer alg√∫n sort no puede ser sobre\n",
    "# el conjunto de testing ya que al hacer sus predicciones sobre el conjunto de test sin etiquetas\n",
    "# debe conservar el orden original para ser comparado con los golden_labels. \n",
    "\n",
    "train_iterator, valid_iterator, test_iterator = legacy.data.BucketIterator.splits(\n",
    "    (train_data, valid_data, test_data),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    device=device,\n",
    "    sort=False,\n",
    ")\n",
    "\n",
    "\n",
    "def calculate_metrics(preds, y_true, pad_idx=PAD_TAG_IDX, o_idx=O_TAG_IDX):\n",
    "    \"\"\"\n",
    "    Calcula precision, recall y f1 de cada batch.\n",
    "    \"\"\"\n",
    "\n",
    "    # Obtener el indice de la clase con probabilidad mayor. (clases)\n",
    "    y_pred = preds.argmax(dim=1, keepdim=True)\n",
    "\n",
    "    # filtramos <pad> para calcular los scores.\n",
    "    mask = [(y_true != pad_idx)]\n",
    "    y_pred = y_pred[mask]\n",
    "    y_true = y_true[mask]\n",
    "\n",
    "    # traemos a la cpu\n",
    "    y_pred = y_pred.view(-1).to('cpu').numpy()\n",
    "    y_true = y_true.to('cpu').numpy()\n",
    "    y_pred = [[NER_TAGS.vocab.itos[v] for v in y_pred]]\n",
    "    y_true = [[NER_TAGS.vocab.itos[v] for v in y_true]]\n",
    "    \n",
    "    # calcular scores\n",
    "    f1 = f1_score(y_true, y_pred, mode='strict')\n",
    "    precision = precision_score(y_true, y_pred, mode='strict')\n",
    "    recall = recall_score(y_true, y_pred, mode='strict')\n",
    "\n",
    "    return precision, recall, f1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "805d01b2",
   "metadata": {},
   "source": [
    "### Baseline mejorado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a22820f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir la red\n",
    "class NER_RNN(nn.Module):\n",
    "    def __init__(self, \n",
    "                 input_dim, \n",
    "                 embedding_dim, \n",
    "                 hidden_dim, \n",
    "                 output_dim,\n",
    "                 n_layers, \n",
    "                 bidirectional, \n",
    "                 dropout):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        # Capa de embedding\n",
    "        self.embedding = nn.Embedding(855380, embedding_dim)\n",
    "\n",
    "        # Capa LSTM\n",
    "        self.lstm = nn.LSTM(embedding_dim,\n",
    "                           hidden_dim,\n",
    "                           num_layers=n_layers,\n",
    "                           bidirectional=bidirectional, \n",
    "                           dropout = dropout if n_layers > 1 else 0)\n",
    "\n",
    "        # Capa de salida\n",
    "        self.fc = nn.Linear(hidden_dim * 2 if bidirectional else hidden_dim,\n",
    "                            output_dim)\n",
    "\n",
    "        # Dropout\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, text):\n",
    "\n",
    "        #text = [sent len, batch size]\n",
    "\n",
    "        # Convertir lo enviado a embedding\n",
    "        embedded = self.dropout(self.embedding(text))\n",
    "        \n",
    "        outputs, (hidden, cell) = self.lstm(embedded)\n",
    "        #embedded = [sent len, batch size, emb dim]\n",
    "\n",
    "        # Pasar los embeddings por la rnn (LSTM)\n",
    "\n",
    "        #output = [sent len, batch size, hid dim * n directions]\n",
    "        #hidden/cell = [n layers * n directions, batch size, hid dim]\n",
    "\n",
    "        # Predecir usando la capa de salida.\n",
    "        predictions = self.fc(self.dropout(outputs))\n",
    "        #predictions = [sent len, batch size, output dim]\n",
    "\n",
    "        return predictions\n",
    "\n",
    "\n",
    "# Tama√±o del vocabulario\n",
    "INPUT_DIM = len(TEXT.vocab)\n",
    "EMBEDDING_DIM = 300  # dimensi√≥n de los embeddings.\n",
    "HIDDEN_DIM = 256  # dimensi√≥n de la capas LSTM\n",
    "OUTPUT_DIM = len(NER_TAGS.vocab)  # n√∫mero de clases\n",
    "\n",
    "N_LAYERS = 3  # n√∫mero de capas.\n",
    "DROPOUT = 0.5\n",
    "BIDIRECTIONAL = True\n",
    "\n",
    "baseline_model = NER_RNN(INPUT_DIM, EMBEDDING_DIM, HIDDEN_DIM, OUTPUT_DIM,\n",
    "                         N_LAYERS, BIDIRECTIONAL, DROPOUT)\n",
    "\n",
    "baseline_model_name = 'baseline'  # nombre que tendr√° el modelo guardado...\n",
    "baseline_n_epochs = 10\n",
    "\n",
    "\n",
    "# Loss: Cross Entropy\n",
    "TAG_PAD_IDX = NER_TAGS.vocab.stoi[NER_TAGS.pad_token]\n",
    "baseline_criterion = nn.CrossEntropyLoss(ignore_index = TAG_PAD_IDX)\n",
    "\n",
    "\n",
    "model = baseline_model\n",
    "model_name = baseline_model_name\n",
    "criterion = baseline_criterion\n",
    "n_epochs = baseline_n_epochs\n",
    "\n",
    "\n",
    "def init_weights(m):\n",
    "    # Inicializamos los pesos como aleatorios\n",
    "    for name, param in m.named_parameters():\n",
    "        nn.init.normal_(param.data, mean=0, std=0.1) \n",
    "        \n",
    "    # Seteamos como 0 los embeddings de UNK y PAD.\n",
    "    model.embedding.weight.data[UNK_IDX] = torch.zeros(EMBEDDING_DIM)\n",
    "    model.embedding.weight.data[PAD_IDX] = torch.zeros(EMBEDDING_DIM)\n",
    "\n",
    "\n",
    "model.apply(init_weights)\n",
    "\n",
    "\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f'El modelo actual tiene {count_parameters(model):,} par√°metros entrenables.')\n",
    "\n",
    "\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "model = model.to(device)\n",
    "criterion = criterion.to(device)\n",
    "\n",
    "def train(model, iterator, optimizer, criterion):\n",
    "\n",
    "    epoch_loss = 0\n",
    "    epoch_precision = 0\n",
    "    epoch_recall = 0\n",
    "    epoch_f1 = 0\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    # Por cada batch del iterador de la √©poca:\n",
    "    for batch in iterator:\n",
    "\n",
    "        # Extraemos el texto y los tags del batch que estamos procesado\n",
    "        text = batch.text\n",
    "        tags = batch.nertags\n",
    "\n",
    "        # Reiniciamos los gradientes calculados en la iteraci√≥n anterior\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        #text = [sent len, batch size]\n",
    "\n",
    "        # Predecimos los tags del texto del batch.\n",
    "        predictions = model(text)\n",
    "\n",
    "        #predictions = [sent len, batch size, output dim]\n",
    "        #tags = [sent len, batch size]\n",
    "\n",
    "        # Reordenamos los datos para calcular la loss\n",
    "        predictions = predictions.view(-1, predictions.shape[-1])\n",
    "        tags = tags.view(-1)\n",
    "\n",
    "        #predictions = [sent len * batch size, output dim]\n",
    "\n",
    "\n",
    "\n",
    "        # Calculamos el Cross Entropy de las predicciones con respecto a las etiquetas reales\n",
    "        loss = criterion(predictions, tags)\n",
    "        \n",
    "        # Calculamos el accuracy\n",
    "        precision, recall, f1 = calculate_metrics(predictions, tags)\n",
    "\n",
    "        # Calculamos los gradientes\n",
    "        loss.backward()\n",
    "\n",
    "        # Actualizamos los par√°metros de la red\n",
    "        optimizer.step()\n",
    "\n",
    "        # Actualizamos el loss y las m√©tricas\n",
    "        epoch_loss += loss.item()\n",
    "        epoch_precision += precision\n",
    "        epoch_recall += recall\n",
    "        epoch_f1 += f1\n",
    "\n",
    "    return epoch_loss / len(iterator), epoch_precision / len(iterator), epoch_recall / len(iterator), epoch_f1 / len(iterator)\n",
    "\n",
    "\n",
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs\n",
    "\n",
    "\n",
    "def evaluate(model, iterator, criterion):\n",
    "\n",
    "    epoch_loss = 0\n",
    "    epoch_precision = 0\n",
    "    epoch_recall = 0\n",
    "    epoch_f1 = 0\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    # Indicamos que ahora no guardaremos los gradientes\n",
    "    with torch.no_grad():\n",
    "        # Por cada batch\n",
    "        for batch in iterator:\n",
    "\n",
    "            text = batch.text\n",
    "            tags = batch.nertags\n",
    "\n",
    "            # Predecimos\n",
    "            predictions = model(text)\n",
    "\n",
    "            predictions = predictions.view(-1, predictions.shape[-1])\n",
    "            tags = tags.view(-1)\n",
    "\n",
    "            # Calculamos el Cross Entropy de las predicciones con respecto a las etiquetas reales\n",
    "            loss = criterion(predictions, tags)\n",
    "\n",
    "            # Calculamos las m√©tricas\n",
    "            precision, recall, f1 = calculate_metrics(predictions, tags)\n",
    "\n",
    "            # Actualizamos el loss y las m√©tricas\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_precision += precision\n",
    "            epoch_recall += recall\n",
    "            epoch_f1 += f1\n",
    "\n",
    "    return epoch_loss / len(iterator), epoch_precision / len(\n",
    "        iterator), epoch_recall / len(iterator), epoch_f1 / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "932711a8",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iK5lQqpviicf",
    "outputId": "fab7900b-9cac-4f99-8c71-172a10d73d31"
   },
   "outputs": [],
   "source": [
    "best_valid_loss = float('inf')\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Recuerdo: train_iterator y valid_iterator contienen el dataset dividido en batches.\n",
    "\n",
    "    # Entrenar\n",
    "    train_loss, train_precision, train_recall, train_f1 = train(\n",
    "        model, train_iterator, optimizer, criterion)\n",
    "\n",
    "    # Evaluar (valid = validaci√≥n)\n",
    "    valid_loss, valid_precision, valid_recall, valid_f1 = evaluate(\n",
    "        model, valid_iterator, criterion)\n",
    "\n",
    "    end_time = time.time()\n",
    "\n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "\n",
    "    # Si obtuvimos mejores resultados, guardamos este modelo en el almacenamiento (para poder cargarlo luego)\n",
    "    # Si detienen el entrenamiento prematuramente, pueden cargar el modelo en el siguiente recuadro de c√≥digo.\n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), '{}.pt'.format(model_name))\n",
    "    # Si ya no mejoramos el loss de validaci√≥n, terminamos de entrenar.\n",
    "\n",
    "    print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(\n",
    "        f'\\tTrain Loss: {train_loss:.3f} | Train f1: {train_f1:.2f} | Train precision: {train_precision:.2f} | Train recall: {train_recall:.2f}'\n",
    "    )\n",
    "    print(\n",
    "        f'\\t Val. Loss: {valid_loss:.3f} |  Val. f1: {valid_f1:.2f} |  Val. precision: {valid_precision:.2f} | Val. recall: {valid_recall:.2f}'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94e86b0b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "s0gVbP8yiicj",
    "outputId": "2c9cad02-679b-4b4b-c33e-07145690b90b"
   },
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load('{}.pt'.format(model_name)))\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "valid_loss, valid_precision, valid_recall, valid_f1 = evaluate(model, valid_iterator, criterion)\n",
    "\n",
    "print(\n",
    "    f'Val. Loss: {valid_loss:.3f} |  Val. f1: {valid_f1:.2f} | Val. precision: {valid_precision:.2f} | Val. recall: {valid_recall:.2f}'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f53c5428",
   "metadata": {},
   "source": [
    "### Data balanceada con modelo Baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "143e8da1",
   "metadata": {},
   "source": [
    "A continuaci√≥n se presenta el c√≥digo de como se gener√≥ el dataset balanceado sin embargo el archivo se adjuntara en la entrega ya que este c√≥digo demora en ejecutarse, podr√≠a optimizarse para generear el dataset m√°s r√°pido."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40f0710d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Procedemos a generar los datos balanceados\n",
    "\n",
    "train_data, valid_data, test_data = legacy.datasets.SequenceTaggingDataset.splits(\n",
    "    path=\"./\",\n",
    "    train=\"train.txt\",\n",
    "    validation=\"dev.txt\",\n",
    "    test=\"test.txt\",\n",
    "    fields=fields,\n",
    "    encoding=\"utf-8\",\n",
    "    separator=\" \"\n",
    ")\n",
    "\n",
    "columns=['text', 'nertags', 'phrase']\n",
    "text_datum = []\n",
    "nertag_datum = []\n",
    "phrase_datum = []\n",
    "\n",
    "count = 0\n",
    "for i in train_data:\n",
    "  text_datum.append(i.text)\n",
    "  nertag_datum.append(i.nertags)\n",
    "  phrase_datum.append([count]*len(i.text))\n",
    "  count += 1\n",
    "\n",
    "\n",
    "# Convertimos a Dataframe para manejar y analizar los datos usando Pandas\n",
    "df = pd.DataFrame({\n",
    "    'text': np.concatenate(text_datum),\n",
    "    'nertags': np.concatenate(nertag_datum),\n",
    "    'phrase':np.concatenate(phrase_datum)\n",
    "})\n",
    "\n",
    "#Creamos para una columna para los ner tags sin distincion de beggining o inside\n",
    "df['simpleNertag']= df.apply(lambda x :  x.nertags[2:] if len(x.nertags) > 1 else x.nertags , axis=1)\n",
    "\n",
    "\n",
    "#Creamos un dataframe que agrupe cada frase y cuente los simple nertag\n",
    "df_vectors = pd.DataFrame(df.groupby(\"phrase\").simpleNertag.value_counts()).rename(columns={\"simpleNertag\": \"value\"}).reset_index(level=[0,1])\n",
    "\n",
    "#Hacemos un pivote \n",
    "df_vectors_p = pd.DataFrame(df_vectors).pivot(index=\"phrase\", columns=\"simpleNertag\", values=\"value\").fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e76230c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Definimos algunas funcione que nos ayudaran a sumar y restar valores de lista\n",
    "def add_vector(A, B):\n",
    "  assert(len(A) == len(B))\n",
    "  C = []\n",
    "  for i in range(len(A)):\n",
    "    C.append(A[i] + B[i])\n",
    "  return C\n",
    "  \n",
    "def substract_vector(A, B):\n",
    "  assert(len(A) == len(B))\n",
    "  C = []\n",
    "  for i in range(len(A)):\n",
    "    C.append(A[i] - B[i])\n",
    "  return C\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5f5aca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "random.seed(0)\n",
    "\n",
    "mean = 100000\n",
    "\n",
    "\n",
    "def interp(mean, sumlist):\n",
    "  ans = 0\n",
    "  ans2 = 0\n",
    "  for i in range(len(sumlist)):\n",
    "    ans += abs(sumlist[i] - mean)\n",
    "  \n",
    "  return ans\n",
    "\n",
    "phrases_vector = [1]*phrase_count #8024 frases\n",
    "interp_v = interp(mean, total_sum)\n",
    "\n",
    "\n",
    "while(abs(interp_v[1] - mean)  > 30000):\n",
    "    phrase_idx = random.randint(0, phrase_count-1)\n",
    "    vector_i = list(vectors.iloc[phrase_idx])\n",
    "    add_interp = interp(mean, add_vector(total_sum, vector_i))\n",
    "    substract_interp = interp(mean, substract_vector(total_sum, vector_i))\n",
    "\n",
    "    if(phrases_vector[phrase_idx] != 0):\n",
    "      if(add_interp < interp_v):\n",
    "        phrases_vector[phrase_idx] +=1\n",
    "        total_sum = add_vector(total_sum, vector_i)\n",
    "        interp_v = add_interp\n",
    "\n",
    "      elif(substract_interp < interp_v):\n",
    "        phrases_vector[phrase_idx] -=1\n",
    "        interp_v = substract_interp\n",
    "        total_sum = substract_vector(total_sum, vector_i)\n",
    "\n",
    "      else: \n",
    "        if(substract_interp < add_interp):\n",
    "          phrases_vector[phrase_idx] -=1\n",
    "          interp_v = substract_interp\n",
    "          total_sum = substract_vector(total_sum, vector_i)\n",
    "\n",
    "        else:\n",
    "          phrases_vector[phrase_idx] +=1\n",
    "          interp_v = add_interp\n",
    "          total_sum = add_vector(total_sum, vector_i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de9a9396",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generamos nuevo dataframe\n",
    "new_df = df\n",
    "c = 8024\n",
    "new_df\n",
    "for i in range(len(phrases_vector)):\n",
    "  if phrases_vector[i] > 0:\n",
    "    for j in range(phrases_vector[i] - 1):\n",
    "      new_section = new_df.loc[new_df[\"phrase\"] == i]\n",
    "      new_section[\"phrase\"] = c + 1\n",
    "      c += 1\n",
    "      new_df = new_df.append(new_section)\n",
    "  else: \n",
    "    new_section = new_df.loc[new_df[\"phrase\"] == i]\n",
    "    new_df = new_df.drop(new_section.index)\n",
    "    \n",
    "new_df\n",
    "\n",
    "new2 = new_df.drop(columns='simpleNertag')\n",
    "new3 = new2.groupby('phrase').aggregate({'text': list, 'nertags': list})\n",
    "new3.text.to_list()[:2]\n",
    "new3 = new3.reset_index()\n",
    "\n",
    "\n",
    "f = open(\"balanced.txt\", \"w\")\n",
    "for index, row in new3.iterrows():\n",
    "  textList = row.text\n",
    "  tagsList = row.nertags\n",
    "  for i in range(len(textList)):\n",
    "    newString = textList[i] + \"\\s\" + tagsList[i] + \"\\n\"\n",
    "    f.write(newString)\n",
    "  f.write('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70a2b09b",
   "metadata": {},
   "source": [
    "Ahora probaremos el modelo del baseline entrenando a partir del dataset balanceado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9094bd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from seqeval.metrics import f1_score, precision_score, recall_score\n",
    "from torchtext import data, datasets, legacy\n",
    "\n",
    "# Garantizar reproducibilidad de los experimentos\n",
    "SEED = 1234\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "\n",
    "TEXT = legacy.data.Field(lower=False) \n",
    "NER_TAGS = legacy.data.Field(unk_token=None)\n",
    "fields = ((\"text\", TEXT), (\"nertags\", NER_TAGS))\n",
    "\n",
    "\n",
    "train_data, valid_data, test_data = legacy.datasets.SequenceTaggingDataset.splits(\n",
    "    path=\"./\",\n",
    "    train=\"balanced.txt\",\n",
    "    validation=\"dev.txt\",\n",
    "    test=\"test.txt\",\n",
    "    fields=fields,\n",
    "    encoding=\"utf-8\",\n",
    "    separator=\" \"\n",
    ")\n",
    "\n",
    "\n",
    "print(f\"Numero de ejemplos de entrenamiento: {len(train_data)}\")\n",
    "print(f\"N√∫mero de ejemplos de validaci√≥n: {len(valid_data)}\")\n",
    "print(f\"N√∫mero de ejemplos de test (competencia): {len(test_data)}\")\n",
    "\n",
    "\n",
    "TEXT.build_vocab(train_data)\n",
    "NER_TAGS.build_vocab(train_data)\n",
    "UNK_IDX = TEXT.vocab.stoi[TEXT.unk_token]\n",
    "PAD_IDX = TEXT.vocab.stoi[TEXT.pad_token]\n",
    "PAD_TAG_IDX = NER_TAGS.vocab.stoi[NER_TAGS.pad_token]\n",
    "O_TAG_IDX = NER_TAGS.vocab.stoi['O']\n",
    "\n",
    "\n",
    "def tag_percentage(tag_counts):\n",
    "    \n",
    "    total_count = sum([count for tag, count in tag_counts])\n",
    "    tag_counts_percentages = [(tag, count, count/total_count) for tag, count in tag_counts]\n",
    "  \n",
    "    return tag_counts_percentages\n",
    "\n",
    "\n",
    "print(\"Tag Ocurrencia Porcentaje\\n\")\n",
    "\n",
    "for tag, count, percent in tag_percentage(NER_TAGS.vocab.freqs.most_common()):\n",
    "    print(f\"{tag}\\t{count}\\t{percent*100:4.1f}%\")\n",
    "\n",
    "BATCH_SIZE = 16  # disminuir si hay problemas de ram.\n",
    "\n",
    "# Usar cuda si es que est√° disponible.\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using', device)\n",
    "\n",
    "# Dividir datos entre entrenamiento y test. Si van a hacer alg√∫n sort no puede ser sobre\n",
    "# el conjunto de testing ya que al hacer sus predicciones sobre el conjunto de test sin etiquetas\n",
    "# debe conservar el orden original para ser comparado con los golden_labels. \n",
    "\n",
    "train_iterator, valid_iterator, test_iterator = legacy.data.BucketIterator.splits(\n",
    "    (train_data, valid_data, test_data),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    device=device,\n",
    "    sort=False,\n",
    ")\n",
    "\n",
    "\n",
    "def calculate_metrics(preds, y_true, pad_idx=PAD_TAG_IDX, o_idx=O_TAG_IDX):\n",
    "    \"\"\"\n",
    "    Calcula precision, recall y f1 de cada batch.\n",
    "    \"\"\"\n",
    "\n",
    "    # Obtener el indice de la clase con probabilidad mayor. (clases)\n",
    "    y_pred = preds.argmax(dim=1, keepdim=True)\n",
    "\n",
    "    # filtramos <pad> para calcular los scores.\n",
    "    mask = [(y_true != pad_idx)]\n",
    "    y_pred = y_pred[mask]\n",
    "    y_true = y_true[mask]\n",
    "\n",
    "    # traemos a la cpu\n",
    "    y_pred = y_pred.view(-1).to('cpu').numpy()\n",
    "    y_true = y_true.to('cpu').numpy()\n",
    "    y_pred = [[NER_TAGS.vocab.itos[v] for v in y_pred]]\n",
    "    y_true = [[NER_TAGS.vocab.itos[v] for v in y_true]]\n",
    "    \n",
    "    # calcular scores\n",
    "    f1 = f1_score(y_true, y_pred, mode='strict')\n",
    "    precision = precision_score(y_true, y_pred, mode='strict')\n",
    "    recall = recall_score(y_true, y_pred, mode='strict')\n",
    "\n",
    "    return precision, recall, f1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1867bf5",
   "metadata": {},
   "source": [
    "## Conclusiones üí°\n",
    "\n",
    "**REQUIREMENTS: Discutir resultados, proponer trabajo futuro. (1 punto)**\n",
    "\n",
    "Para el modelo baseline con datos balanceados se obtuvieron resultados muy bajos, esto se puede explicar a que en el resultado del dataset se borraron casi 7000 de las 8000 frases. Esto tiene sentido ya que casi el informaci√≥n se perdi√≥ al rededor de un 87,5% de la informaci√≥n original, haciendo que el modelo se entrenara con muy poca informaci√≥n y por lo tanto no siendo capaz de predecir √≥ptimamente los datos de validaci√≥n y training. Una mejora que se puede hacer es balancear los datos hacia arriba generando un oversampling de los datos, pero a pesar de que esto se intent√≥ con un m√©todo similar no se logr√≥ que convergiera el algoritmo. Un trabajo futuro es probar que pasar√≠a si se probara no agregar frases enteras nuevas si no que editar existentes para que incluyan m√°s variedad de etiquetas, y se pueda rebatir o no la hipotesis de que esto generer√≠a malos resultados debido a que las frases podr√≠an alterar su sem√°ntica."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cdc11c3",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b30e707",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
